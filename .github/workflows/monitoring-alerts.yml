name: CI/CD Monitoring & Alerting for Self-hosted Runners

on:
  workflow_run:
    workflows: ["Auto Deploy to Registry"]
    types:
      - completed
  schedule:
    # Reduced frequency - Monitor every 4 hours during business hours only
    - cron: '0 8,12,16 * * 1-5'
    # Single health check per day outside business hours
    - cron: '0 20 * * *'
  workflow_dispatch:
    inputs:
      monitoring_scope:
        description: 'Monitoring scope'
        required: false
        default: 'full'
        type: choice
        options:
          - 'full'
          - 'pipeline-only'
          - 'infrastructure-only'
          - 'performance-only'
      alert_level:
        description: 'Alert sensitivity level'
        required: false
        default: 'normal'
        type: choice
        options:
          - 'low'
          - 'normal'
          - 'high'
          - 'critical'

env:
  REGISTRY: registry.jclee.me
  MONITORING_INTERVAL: 300  # 5 minutes
  ALERT_THRESHOLD_CPU: 80
  ALERT_THRESHOLD_MEMORY: 80
  ALERT_THRESHOLD_DISK: 85

jobs:
  # Infrastructure health monitoring
  infrastructure-monitoring:
    name: üñ•Ô∏è Infrastructure Health Monitoring
    runs-on: self-hosted
    outputs:
      runner-health: ${{ steps.health-check.outputs.status }}
      disk-usage: ${{ steps.health-check.outputs.disk-usage }}
      memory-usage: ${{ steps.health-check.outputs.memory-usage }}
      cpu-usage: ${{ steps.health-check.outputs.cpu-usage }}
    steps:
      - name: System health assessment
        id: health-check
        run: |
          echo "üîç Performing comprehensive system health assessment..."
          
          # CPU usage
          CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')
          echo "cpu-usage=$CPU_USAGE" >> $GITHUB_OUTPUT
          
          # Memory usage
          MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')
          echo "memory-usage=$MEMORY_USAGE" >> $GITHUB_OUTPUT
          
          # Disk usage
          DISK_USAGE=$(df / | awk 'NR==2{printf("%.1f", $3/$2*100)}')
          echo "disk-usage=$DISK_USAGE" >> $GITHUB_OUTPUT
          
          # Overall health status
          HEALTH_STATUS="healthy"
          
          if (( $(echo "$CPU_USAGE > ${{ env.ALERT_THRESHOLD_CPU }}" | bc -l) )); then
            HEALTH_STATUS="warning"
            echo "‚ö†Ô∏è High CPU usage: $CPU_USAGE%"
          fi
          
          if (( $(echo "$MEMORY_USAGE > ${{ env.ALERT_THRESHOLD_MEMORY }}" | bc -l) )); then
            HEALTH_STATUS="warning"
            echo "‚ö†Ô∏è High memory usage: $MEMORY_USAGE%"
          fi
          
          if (( $(echo "$DISK_USAGE > ${{ env.ALERT_THRESHOLD_DISK }}" | bc -l) )); then
            HEALTH_STATUS="critical"
            echo "üö® High disk usage: $DISK_USAGE%"
          fi
          
          echo "status=$HEALTH_STATUS" >> $GITHUB_OUTPUT
          
          echo "üìä System metrics:"
          echo "  CPU: $CPU_USAGE%"
          echo "  Memory: $MEMORY_USAGE%"
          echo "  Disk: $DISK_USAGE%"
          echo "  Status: $HEALTH_STATUS"

      - name: Docker daemon health check
        run: |
          echo "üê≥ Checking Docker daemon health..."
          
          # Docker daemon status
          if ! docker info > /dev/null 2>&1; then
            echo "üö® Docker daemon is not responding"
            echo "docker-status=critical" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Docker disk usage
          DOCKER_DISK=$(docker system df --format "table {{.Type}}\t{{.TotalCount}}\t{{.Size}}\t{{.Reclaimable}}")
          echo "Docker disk usage:"
          echo "$DOCKER_DISK"
          
          # Check for stuck containers
          STUCK_CONTAINERS=$(docker ps --filter "status=exited" --filter "since=24h" -q | wc -l)
          if [ "$STUCK_CONTAINERS" -gt 10 ]; then
            echo "‚ö†Ô∏è Found $STUCK_CONTAINERS stuck containers from last 24h"
          fi
          
          # Check for image bloat
          TOTAL_IMAGES=$(docker images -q | wc -l)
          if [ "$TOTAL_IMAGES" -gt 100 ]; then
            echo "‚ö†Ô∏è High number of Docker images: $TOTAL_IMAGES"
          fi

      - name: Build cache health assessment
        run: |
          echo "üóÇÔ∏è Assessing build cache health..."
          
          # Buildx cache size
          if [ -d /tmp/.buildx-cache ]; then
            CACHE_SIZE=$(du -sh /tmp/.buildx-cache | cut -f1)
            CACHE_FILES=$(find /tmp/.buildx-cache -type f | wc -l)
            echo "Build cache: $CACHE_SIZE ($CACHE_FILES files)"
            
            # Check cache age
            OLDEST_FILE=$(find /tmp/.buildx-cache -type f -printf '%T+ %p\n' | sort | head -1)
            if [ -n "$OLDEST_FILE" ]; then
              echo "Oldest cache file: $OLDEST_FILE"
            fi
          else
            echo "‚ö†Ô∏è Build cache directory not found"
          fi
          
          # Registry cache
          if [ -d ~/.docker/buildx ]; then
            REGISTRY_CACHE_SIZE=$(du -sh ~/.docker/buildx | cut -f1)
            echo "Registry cache: $REGISTRY_CACHE_SIZE"
          fi

      - name: Network connectivity verification
        run: |
          echo "üåê Verifying network connectivity..."
          
          # GitHub connectivity
          if curl -sf https://api.github.com/zen > /dev/null; then
            echo "‚úÖ GitHub API connectivity: OK"
          else
            echo "‚ùå GitHub API connectivity: FAILED"
            echo "github-connectivity=failed" >> $GITHUB_OUTPUT
          fi
          
          # Registry connectivity
          if ping -c 3 ${{ env.REGISTRY }} > /dev/null 2>&1; then
            echo "‚úÖ Registry connectivity: OK"
          else
            echo "‚ùå Registry connectivity: FAILED"
            echo "registry-connectivity=failed" >> $GITHUB_OUTPUT
          fi
          
          # DNS resolution test
          if nslookup google.com > /dev/null 2>&1; then
            echo "‚úÖ DNS resolution: OK"
          else
            echo "‚ùå DNS resolution: FAILED"
            echo "dns-resolution=failed" >> $GITHUB_OUTPUT
          fi

  # Pipeline performance monitoring
  pipeline-monitoring:
    name: üöÄ Pipeline Performance Monitoring
    runs-on: self-hosted
    needs: infrastructure-monitoring
    outputs:
      pipeline-health: ${{ steps.pipeline-check.outputs.status }}
      avg-build-time: ${{ steps.pipeline-check.outputs.avg-build-time }}
      success-rate: ${{ steps.pipeline-check.outputs.success-rate }}
    steps:
      - name: Analyze recent pipeline performance
        id: pipeline-check
        run: |
          echo "üìä Analyzing recent pipeline performance..."
          
          # Get recent workflow runs using gh CLI
          if command -v gh &> /dev/null; then
            echo "Fetching recent workflow runs..."
            
            # Get last 20 workflow runs
            gh run list --limit 20 --json status,conclusion,createdAt,updatedAt,name > recent-runs.json || echo "[]" > recent-runs.json
            
            # Calculate success rate
            TOTAL_RUNS=$(jq length recent-runs.json)
            SUCCESS_RUNS=$(jq '[.[] | select(.conclusion == "success")] | length' recent-runs.json)
            
            if [ "$TOTAL_RUNS" -gt 0 ]; then
              SUCCESS_RATE=$(echo "scale=1; $SUCCESS_RUNS * 100 / $TOTAL_RUNS" | bc)
            else
              SUCCESS_RATE="0"
            fi
            
            echo "success-rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT
            echo "üìà Success rate: $SUCCESS_RATE% ($SUCCESS_RUNS/$TOTAL_RUNS)"
            
            # Analyze build times (simplified calculation)
            AVG_BUILD_TIME="N/A"
            if [ "$TOTAL_RUNS" -gt 0 ]; then
              # Estimate build time based on workflow duration
              AVG_BUILD_TIME=$(jq -r '[.[] | select(.conclusion == "success")] | length' recent-runs.json)
              echo "avg-build-time=$AVG_BUILD_TIME" >> $GITHUB_OUTPUT
            fi
            
            # Determine pipeline health
            PIPELINE_STATUS="healthy"
            if (( $(echo "$SUCCESS_RATE < 80" | bc -l) )); then
              PIPELINE_STATUS="warning"
            fi
            if (( $(echo "$SUCCESS_RATE < 60" | bc -l) )); then
              PIPELINE_STATUS="critical"
            fi
            
            echo "status=$PIPELINE_STATUS" >> $GITHUB_OUTPUT
            echo "üéØ Pipeline status: $PIPELINE_STATUS"
            
          else
            echo "‚ö†Ô∏è GitHub CLI not available, using fallback monitoring"
            echo "status=unknown" >> $GITHUB_OUTPUT
            echo "success-rate=unknown" >> $GITHUB_OUTPUT
            echo "avg-build-time=unknown" >> $GITHUB_OUTPUT
          fi

      - name: Check for common pipeline issues
        run: |
          echo "üîç Checking for common pipeline issues..."
          
          # Check for disk space issues in logs
          echo "Checking for disk space warnings..."
          if [ -f /var/log/syslog ]; then
            DISK_WARNINGS=$(grep -i "no space left" /var/log/syslog | tail -5)
            if [ -n "$DISK_WARNINGS" ]; then
              echo "‚ö†Ô∏è Recent disk space warnings found:"
              echo "$DISK_WARNINGS"
            fi
          fi
          
          # Check for Docker issues
          echo "Checking for Docker daemon issues..."
          if journalctl -u docker --since "1 hour ago" | grep -i error > /dev/null; then
            echo "‚ö†Ô∏è Docker errors found in recent logs"
            journalctl -u docker --since "1 hour ago" | grep -i error | tail -5
          fi
          
          # Check for memory pressure
          echo "Checking for memory pressure..."
          if dmesg | grep -i "killed process" | tail -1 | grep -q "$(date +%b)"; then
            echo "‚ö†Ô∏è Recent OOM kills detected"
            dmesg | grep -i "killed process" | tail -3
          fi

      - name: Analyze build performance trends
        run: |
          echo "üìà Analyzing build performance trends..."
          
          # Create performance metrics
          cat > performance-metrics.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "infrastructure": {
              "cpu_usage": "${{ needs.infrastructure-monitoring.outputs.cpu-usage }}",
              "memory_usage": "${{ needs.infrastructure-monitoring.outputs.memory-usage }}",
              "disk_usage": "${{ needs.infrastructure-monitoring.outputs.disk-usage }}",
              "status": "${{ needs.infrastructure-monitoring.outputs.runner-health }}"
            },
            "pipeline": {
              "success_rate": "${{ steps.pipeline-check.outputs.success-rate }}",
              "avg_build_time": "${{ steps.pipeline-check.outputs.avg-build-time }}",
              "status": "${{ steps.pipeline-check.outputs.status }}"
            },
            "docker": {
              "total_images": "$(docker images -q | wc -l)",
              "running_containers": "$(docker ps -q | wc -l)",
              "stopped_containers": "$(docker ps -aq --filter status=exited | wc -l)"
            }
          }
          EOF
          
          cat performance-metrics.json

  # Security monitoring
  security-monitoring:
    name: üîí Security Monitoring
    runs-on: self-hosted
    outputs:
      security-status: ${{ steps.security-check.outputs.status }}
    steps:
      - name: Security health assessment
        id: security-check
        run: |
          echo "üîí Performing security health assessment..."
          
          SECURITY_STATUS="secure"
          
          # Check for failed login attempts
          echo "Checking for security incidents..."
          if [ -f /var/log/auth.log ]; then
            FAILED_LOGINS=$(grep "Failed password" /var/log/auth.log | grep "$(date +%b)" | wc -l)
            if [ "$FAILED_LOGINS" -gt 50 ]; then
              echo "‚ö†Ô∏è High number of failed login attempts: $FAILED_LOGINS"
              SECURITY_STATUS="warning"
            fi
          fi
          
          # Check for unusual network connections
          echo "Checking network connections..."
          EXTERNAL_CONNECTIONS=$(netstat -tn | grep ESTABLISHED | wc -l)
          if [ "$EXTERNAL_CONNECTIONS" -gt 100 ]; then
            echo "‚ö†Ô∏è High number of external connections: $EXTERNAL_CONNECTIONS"
          fi
          
          # Check Docker security
          echo "Checking Docker security..."
          if docker ps --format "table {{.Names}}\t{{.Ports}}" | grep -E "0\.0\.0\.0:(80|443|22|3389)" > /dev/null; then
            echo "‚ö†Ô∏è Containers exposing sensitive ports detected"
            SECURITY_STATUS="warning"
          fi
          
          # Check for privileged containers
          PRIVILEGED_CONTAINERS=$(docker ps --filter "label=privileged=true" -q | wc -l)
          if [ "$PRIVILEGED_CONTAINERS" -gt 0 ]; then
            echo "‚ö†Ô∏è Privileged containers running: $PRIVILEGED_CONTAINERS"
          fi
          
          echo "status=$SECURITY_STATUS" >> $GITHUB_OUTPUT
          echo "üõ°Ô∏è Security status: $SECURITY_STATUS"

      - name: Registry security check
        run: |
          echo "üè™ Checking registry security..."
          
          # Test registry authentication
          if echo "${{ secrets.REGISTRY_PASSWORD }}" | docker login ${{ env.REGISTRY }} \
            -u ${{ secrets.REGISTRY_USERNAME }} --password-stdin > /dev/null 2>&1; then
            echo "‚úÖ Registry authentication: OK"
            docker logout ${{ env.REGISTRY }}
          else
            echo "‚ùå Registry authentication: FAILED"
            echo "registry-auth=failed" >> $GITHUB_OUTPUT
          fi

  # Performance benchmarking
  performance-benchmarking:
    name: ‚ö° Performance Benchmarking
    runs-on: self-hosted
    needs: [infrastructure-monitoring, pipeline-monitoring]
    steps:
      - name: Checkout code for benchmarking
        uses: actions/checkout@v4

      - name: Run infrastructure benchmarks
        run: |
          echo "‚ö° Running infrastructure benchmarks..."
          
          # CPU benchmark
          echo "CPU benchmark (10 seconds)..."
          time $(for i in {1..100000}; do echo "scale=1000; 4*a(1)" | bc -l > /dev/null; done) 2>&1 | grep real
          
          # Memory benchmark
          echo "Memory benchmark..."
          free -h
          
          # Disk I/O benchmark
          echo "Disk I/O benchmark..."
          time dd if=/dev/zero of=/tmp/test-io bs=1M count=100 2>&1 | grep -E "(copied|MB/s)"
          rm -f /tmp/test-io
          
          # Network benchmark (to registry)
          echo "Network benchmark to registry..."
          time curl -sf https://${{ env.REGISTRY }}/v2/ > /dev/null
          echo "Network test completed"

      - name: Docker performance benchmark
        run: |
          echo "üê≥ Running Docker performance benchmarks..."
          
          # Image pull performance
          echo "Testing image pull performance..."
          time docker pull alpine:latest > /dev/null 2>&1
          
          # Container startup performance
          echo "Testing container startup performance..."
          time docker run --rm alpine:latest echo "Hello World" > /dev/null 2>&1
          
          # Build performance test
          echo "Testing build performance..."
          cat > Dockerfile.test << EOF
          FROM alpine:latest
          RUN apk add --no-cache curl
          EOF
          
          time docker build -t test-build:latest -f Dockerfile.test . > /dev/null 2>&1
          docker rmi test-build:latest || true
          rm Dockerfile.test

      - name: Generate performance report
        run: |
          echo "üìä Generating performance report..."
          
          cat > performance-report.md << EOF
          # Performance Monitoring Report
          
          **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **Runner:** self-hosted
          
          ## Infrastructure Metrics
          - **CPU Usage:** ${{ needs.infrastructure-monitoring.outputs.cpu-usage }}%
          - **Memory Usage:** ${{ needs.infrastructure-monitoring.outputs.memory-usage }}%
          - **Disk Usage:** ${{ needs.infrastructure-monitoring.outputs.disk-usage }}%
          - **Health Status:** ${{ needs.infrastructure-monitoring.outputs.runner-health }}
          
          ## Pipeline Metrics
          - **Success Rate:** ${{ needs.pipeline-monitoring.outputs.success-rate }}%
          - **Average Build Time:** ${{ needs.pipeline-monitoring.outputs.avg-build-time }}
          - **Pipeline Health:** ${{ needs.pipeline-monitoring.outputs.pipeline-health }}
          
          ## Resource Utilization
          $(df -h | head -5)
          
          ## Docker Statistics
          - **Total Images:** $(docker images -q | wc -l)
          - **Running Containers:** $(docker ps -q | wc -l)
          - **Total Containers:** $(docker ps -aq | wc -l)
          
          ## Recommendations
          EOF
          
          # Add recommendations based on metrics
          CPU_USAGE=${{ needs.infrastructure-monitoring.outputs.cpu-usage }}
          MEMORY_USAGE=${{ needs.infrastructure-monitoring.outputs.memory-usage }}
          DISK_USAGE=${{ needs.infrastructure-monitoring.outputs.disk-usage }}
          
          if (( $(echo "$CPU_USAGE > 70" | bc -l) )); then
            echo "- ‚ö†Ô∏è Consider optimizing CPU-intensive tasks" >> performance-report.md
          fi
          
          if (( $(echo "$MEMORY_USAGE > 70" | bc -l) )); then
            echo "- ‚ö†Ô∏è Consider increasing memory or optimizing memory usage" >> performance-report.md
          fi
          
          if (( $(echo "$DISK_USAGE > 80" | bc -l) )); then
            echo "- üö® Urgent: Clean up disk space" >> performance-report.md
          fi
          
          cat performance-report.md

  # Alert generation and notification
  alerting:
    name: üö® Alert Generation & Notification
    runs-on: self-hosted
    needs: [infrastructure-monitoring, pipeline-monitoring, security-monitoring, performance-benchmarking]
    if: always()
    steps:
      - name: Analyze alert conditions
        id: alert-analysis
        run: |
          echo "üö® Analyzing alert conditions..."
          
          ALERT_LEVEL="normal"
          ALERTS=()
          
          # Infrastructure alerts
          if [ "${{ needs.infrastructure-monitoring.outputs.runner-health }}" = "critical" ]; then
            ALERTS+=("üö® CRITICAL: Infrastructure health critical")
            ALERT_LEVEL="critical"
          elif [ "${{ needs.infrastructure-monitoring.outputs.runner-health }}" = "warning" ]; then
            ALERTS+=("‚ö†Ô∏è WARNING: Infrastructure health warning")
            ALERT_LEVEL="warning"
          fi
          
          # Pipeline alerts
          if [ "${{ needs.pipeline-monitoring.outputs.pipeline-health }}" = "critical" ]; then
            ALERTS+=("üö® CRITICAL: Pipeline health critical")
            ALERT_LEVEL="critical"
          elif [ "${{ needs.pipeline-monitoring.outputs.pipeline-health }}" = "warning" ]; then
            ALERTS+=("‚ö†Ô∏è WARNING: Pipeline health degraded")
            if [ "$ALERT_LEVEL" != "critical" ]; then
              ALERT_LEVEL="warning"
            fi
          fi
          
          # Security alerts
          if [ "${{ needs.security-monitoring.outputs.security-status }}" = "warning" ]; then
            ALERTS+=("‚ö†Ô∏è WARNING: Security concerns detected")
            if [ "$ALERT_LEVEL" = "normal" ]; then
              ALERT_LEVEL="warning"
            fi
          fi
          
          # Generate alert summary
          if [ ${#ALERTS[@]} -gt 0 ]; then
            echo "alerts-found=true" >> $GITHUB_OUTPUT
            echo "alert-level=$ALERT_LEVEL" >> $GITHUB_OUTPUT
            
            ALERT_MESSAGE=$(IFS=$'\n'; echo "${ALERTS[*]}")
            echo "alert-message<<EOF" >> $GITHUB_OUTPUT
            echo "$ALERT_MESSAGE" >> $GITHUB_OUTPUT
            echo "EOF" >> $GITHUB_OUTPUT
            
            echo "üö® Alerts generated:"
            echo "$ALERT_MESSAGE"
          else
            echo "alerts-found=false" >> $GITHUB_OUTPUT
            echo "‚úÖ No alerts generated - all systems healthy"
          fi

      - name: Generate monitoring summary
        run: |
          echo "üìã Generating comprehensive monitoring summary..."
          
          cat > monitoring-summary.md << EOF
          # CI/CD Monitoring Summary
          
          **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **Monitoring Scope:** ${{ github.event.inputs.monitoring_scope || 'full' }}
          **Alert Level:** ${{ github.event.inputs.alert_level || 'normal' }}
          
          ## System Status Overview
          
          ### Infrastructure
          - **Health:** ${{ needs.infrastructure-monitoring.outputs.runner-health }}
          - **CPU Usage:** ${{ needs.infrastructure-monitoring.outputs.cpu-usage }}%
          - **Memory Usage:** ${{ needs.infrastructure-monitoring.outputs.memory-usage }}%
          - **Disk Usage:** ${{ needs.infrastructure-monitoring.outputs.disk-usage }}%
          
          ### Pipeline Performance
          - **Health:** ${{ needs.pipeline-monitoring.outputs.pipeline-health }}
          - **Success Rate:** ${{ needs.pipeline-monitoring.outputs.success-rate }}%
          - **Avg Build Time:** ${{ needs.pipeline-monitoring.outputs.avg-build-time }}
          
          ### Security
          - **Status:** ${{ needs.security-monitoring.outputs.security-status }}
          
          ## Alert Summary
          EOF
          
          if [ "${{ steps.alert-analysis.outputs.alerts-found }}" = "true" ]; then
            echo "${{ steps.alert-analysis.outputs.alert-message }}" >> monitoring-summary.md
          else
            echo "‚úÖ No alerts - All systems operating normally" >> monitoring-summary.md
          fi
          
          echo "" >> monitoring-summary.md
          echo "## Next Actions" >> monitoring-summary.md
          
          if [ "${{ steps.alert-analysis.outputs.alert-level }}" = "critical" ]; then
            echo "1. üö® Immediate investigation required" >> monitoring-summary.md
            echo "2. Consider emergency maintenance window" >> monitoring-summary.md
            echo "3. Review and apply emergency procedures" >> monitoring-summary.md
          elif [ "${{ steps.alert-analysis.outputs.alert-level }}" = "warning" ]; then
            echo "1. ‚ö†Ô∏è Schedule maintenance window" >> monitoring-summary.md
            echo "2. Review performance optimization opportunities" >> monitoring-summary.md
            echo "3. Monitor trends closely" >> monitoring-summary.md
          else
            echo "1. ‚úÖ Continue normal operations" >> monitoring-summary.md
            echo "2. Review performance trends" >> monitoring-summary.md
            echo "3. Plan proactive optimizations" >> monitoring-summary.md
          fi
          
          cat monitoring-summary.md

      - name: Create monitoring artifacts
        run: |
          echo "üì¶ Creating monitoring artifacts..."
          
          # Create metrics archive
          mkdir -p monitoring-artifacts
          
          # System metrics
          cat > monitoring-artifacts/system-metrics.json << EOF
          {
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "infrastructure": {
              "cpu_usage": "${{ needs.infrastructure-monitoring.outputs.cpu-usage }}",
              "memory_usage": "${{ needs.infrastructure-monitoring.outputs.memory-usage }}",
              "disk_usage": "${{ needs.infrastructure-monitoring.outputs.disk-usage }}",
              "health_status": "${{ needs.infrastructure-monitoring.outputs.runner-health }}"
            },
            "pipeline": {
              "success_rate": "${{ needs.pipeline-monitoring.outputs.success-rate }}",
              "health_status": "${{ needs.pipeline-monitoring.outputs.pipeline-health }}"
            },
            "security": {
              "status": "${{ needs.security-monitoring.outputs.security-status }}"
            },
            "alerts": {
              "found": "${{ steps.alert-analysis.outputs.alerts-found }}",
              "level": "${{ steps.alert-analysis.outputs.alert-level }}"
            }
          }
          EOF
          
          # Copy reports
          cp monitoring-summary.md monitoring-artifacts/
          cp performance-report.md monitoring-artifacts/ || echo "Performance report not available"
          
          # Generate dashboard data for external monitoring
          echo "timestamp,cpu,memory,disk,pipeline_success" > monitoring-artifacts/metrics.csv
          echo "$(date +%s),${{ needs.infrastructure-monitoring.outputs.cpu-usage }},${{ needs.infrastructure-monitoring.outputs.memory-usage }},${{ needs.infrastructure-monitoring.outputs.disk-usage }},${{ needs.pipeline-monitoring.outputs.success-rate }}" >> monitoring-artifacts/metrics.csv

      - name: Upload monitoring artifacts
        uses: actions/upload-artifact@v3
        with:
          name: monitoring-report-$(date +%Y%m%d-%H%M%S)
          path: |
            monitoring-artifacts/
            monitoring-summary.md
          retention-days: 90

      - name: Store metrics for trending
        run: |
          echo "üìà Storing metrics for trending analysis..."
          
          # Create or append to metrics history
          METRICS_FILE="/tmp/ci-cd-metrics-history.csv"
          
          if [ ! -f "$METRICS_FILE" ]; then
            echo "timestamp,cpu,memory,disk,pipeline_success,runner_health,pipeline_health,security_status" > "$METRICS_FILE"
          fi
          
          echo "$(date +%s),${{ needs.infrastructure-monitoring.outputs.cpu-usage }},${{ needs.infrastructure-monitoring.outputs.memory-usage }},${{ needs.infrastructure-monitoring.outputs.disk-usage }},${{ needs.pipeline-monitoring.outputs.success-rate }},${{ needs.infrastructure-monitoring.outputs.runner-health }},${{ needs.pipeline-monitoring.outputs.pipeline-health }},${{ needs.security-monitoring.outputs.security-status }}" >> "$METRICS_FILE"
          
          # Keep only last 1000 entries
          tail -1000 "$METRICS_FILE" > "${METRICS_FILE}.tmp" && mv "${METRICS_FILE}.tmp" "$METRICS_FILE"

  # Automated remediation
  auto-remediation:
    name: üîß Automated Remediation
    runs-on: self-hosted
    needs: [infrastructure-monitoring, pipeline-monitoring, alerting]
    if: always() && needs.alerting.outputs.alerts-found == 'true'
    steps:
      - name: Execute automated remediation
        run: |
          echo "üîß Executing automated remediation procedures..."
          
          ALERT_LEVEL="${{ needs.alerting.outputs.alert-level }}"
          
          case "$ALERT_LEVEL" in
            critical)
              echo "üö® CRITICAL: Executing emergency procedures..."
              
              # Emergency disk cleanup
              if (( $(echo "${{ needs.infrastructure-monitoring.outputs.disk-usage }} > 90" | bc -l) )); then
                echo "Emergency disk cleanup..."
                docker system prune -af --volumes || true
                apt-get clean || true
                journalctl --vacuum-time=1d || true
              fi
              
              # Emergency memory cleanup
              if (( $(echo "${{ needs.infrastructure-monitoring.outputs.memory-usage }} > 95" | bc -l) )); then
                echo "Emergency memory cleanup..."
                echo 3 > /proc/sys/vm/drop_caches || true
              fi
              ;;
              
            warning)
              echo "‚ö†Ô∏è WARNING: Executing preventive procedures..."
              
              # Preventive cleanup
              docker system prune -f || true
              
              # Cache optimization
              if [ -d /tmp/.buildx-cache ]; then
                find /tmp/.buildx-cache -type f -mtime +7 -delete || true
              fi
              ;;
              
            *)
              echo "‚ÑπÔ∏è No automated remediation required"
              ;;
          esac

      - name: Verify remediation effectiveness
        run: |
          echo "‚úÖ Verifying remediation effectiveness..."
          
          # Re-check key metrics
          NEW_DISK_USAGE=$(df / | awk 'NR==2{printf("%.1f", $3/$2*100)}')
          NEW_MEMORY_USAGE=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')
          
          echo "üìä Post-remediation metrics:"
          echo "  Disk usage: ${{ needs.infrastructure-monitoring.outputs.disk-usage }}% ‚Üí $NEW_DISK_USAGE%"
          echo "  Memory usage: ${{ needs.infrastructure-monitoring.outputs.memory-usage }}% ‚Üí $NEW_MEMORY_USAGE%"
          
          # Determine improvement
          DISK_IMPROVEMENT=$(echo "${{ needs.infrastructure-monitoring.outputs.disk-usage }} - $NEW_DISK_USAGE" | bc)
          MEMORY_IMPROVEMENT=$(echo "${{ needs.infrastructure-monitoring.outputs.memory-usage }} - $NEW_MEMORY_USAGE" | bc)
          
          if (( $(echo "$DISK_IMPROVEMENT > 5" | bc -l) )); then
            echo "‚úÖ Significant disk space improvement: $DISK_IMPROVEMENT%"
          fi
          
          if (( $(echo "$MEMORY_IMPROVEMENT > 5" | bc -l) )); then
            echo "‚úÖ Significant memory improvement: $MEMORY_IMPROVEMENT%"
          fi

      - name: Generate remediation report
        run: |
          echo "üìã Generating remediation report..."
          
          cat > remediation-report.md << EOF
          # Automated Remediation Report
          
          **Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')
          **Alert Level:** ${{ needs.alerting.outputs.alert-level }}
          **Trigger:** ${{ needs.alerting.outputs.alert-message }}
          
          ## Actions Taken
          - Emergency disk cleanup
          - Memory cache clearing
          - Docker system pruning
          - Build cache optimization
          
          ## Results
          - **Disk Usage:** ${{ needs.infrastructure-monitoring.outputs.disk-usage }}% ‚Üí $(df / | awk 'NR==2{printf("%.1f", $3/$2*100)}')%
          - **Memory Usage:** ${{ needs.infrastructure-monitoring.outputs.memory-usage }}% ‚Üí $(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')%
          
          ## Status
          ‚úÖ Automated remediation completed successfully
          EOF
          
          cat remediation-report.md