name: Performance Monitoring

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  workflow_dispatch:
  push:
    branches: [ main ]
    paths:
      - 'src/**'
      - 'requirements.txt'
      - 'Dockerfile'

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install locust pytest-benchmark

      - name: Start application for testing
        run: |
          # Initialize database first
          python3 init_database.py || echo "Database initialization failed, continuing..."
          
          # Start application
          python3 main.py --debug &
          APP_PID=$!
          echo "APP_PID=$APP_PID" >> $GITHUB_ENV
          
          # Wait for app to start with better timeout
          echo "Waiting for application to start..."
          for i in {1..30}; do
            if curl -s http://localhost:2542/health > /dev/null 2>&1; then
              echo "Application started successfully"
              break
            fi
            echo "Attempt $i/30: Application not ready yet..."
            sleep 2
          done
          
          # Verify app is running
          curl -f http://localhost:2542/health || (echo "Application failed to start"; ps aux | grep python; exit 1)
        env:
          FORCE_DISABLE_COLLECTION: true
          COLLECTION_ENABLED: false

      - name: Run performance benchmarks
        run: |
          # Create performance test script
          cat > performance_test.py << 'EOF'
          import requests
          import time
          import statistics
          import json
          
          BASE_URL = "http://localhost:2542"
          
          def test_endpoint(endpoint, num_requests=100):
              times = []
              for i in range(num_requests):
                  start = time.time()
                  response = requests.get(f"{BASE_URL}{endpoint}")
                  end = time.time()
                  if response.status_code == 200:
                      times.append((end - start) * 1000)  # Convert to ms
              
              if times:
                  return {
                      "endpoint": endpoint,
                      "requests": len(times),
                      "avg_ms": round(statistics.mean(times), 2),
                      "min_ms": round(min(times), 2),
                      "max_ms": round(max(times), 2),
                      "p95_ms": round(statistics.quantiles(times, n=20)[18], 2) if len(times) >= 20 else "N/A",
                      "success_rate": round(len(times) / num_requests * 100, 2)
                  }
              return None
          
          # Test critical endpoints
          endpoints = ["/health", "/api/health", "/api/blacklist/active", "/api/collection/status"]
          results = []
          
          for endpoint in endpoints:
              print(f"Testing {endpoint}...")
              result = test_endpoint(endpoint)
              if result:
                  results.append(result)
          
          # Output results
          print(json.dumps(results, indent=2))
          
          # Save to file
          with open("performance_results.json", "w") as f:
              json.dump(results, f, indent=2)
          EOF
          
          python performance_test.py

      - name: Load testing with simple script
        run: |
          # Simple concurrent load test
          cat > load_test.py << 'EOF'
          import concurrent.futures
          import requests
          import time
          import json
          
          def make_request(url):
              try:
                  start = time.time()
                  response = requests.get(url, timeout=5)
                  end = time.time()
                  return {
                      "status": response.status_code,
                      "time_ms": round((end - start) * 1000, 2),
                      "success": response.status_code == 200
                  }
              except Exception as e:
                  return {"status": 0, "time_ms": 5000, "success": False, "error": str(e)}
          
          url = "http://localhost:2542/health"
          concurrent_users = 20
          requests_per_user = 10
          
          print(f"Load testing: {concurrent_users} concurrent users, {requests_per_user} requests each")
          
          start_time = time.time()
          
          with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
              futures = []
              for user in range(concurrent_users):
                  for req in range(requests_per_user):
                      futures.append(executor.submit(make_request, url))
              
              results = [future.result() for future in concurrent.futures.as_completed(futures)]
          
          end_time = time.time()
          
          successful = sum(1 for r in results if r["success"])
          total_requests = len(results)
          avg_response_time = sum(r["time_ms"] for r in results if r["success"]) / successful if successful > 0 else 0
          
          load_test_results = {
              "total_requests": total_requests,
              "successful_requests": successful,
              "failed_requests": total_requests - successful,
              "success_rate": round(successful / total_requests * 100, 2),
              "avg_response_time_ms": round(avg_response_time, 2),
              "total_time_seconds": round(end_time - start_time, 2),
              "requests_per_second": round(total_requests / (end_time - start_time), 2)
          }
          
          print(json.dumps(load_test_results, indent=2))
          
          with open("load_test_results.json", "w") as f:
              json.dump(load_test_results, f, indent=2)
          EOF
          
          python load_test.py

      - name: Generate performance report
        run: |
          cat > performance_report.md << 'EOF'
          # Performance Test Report
          
          Generated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')
          Commit: ${{ github.sha }}
          
          ## Endpoint Performance
          
          EOF
          
          if [ -f performance_results.json ]; then
            echo '```json' >> performance_report.md
            cat performance_results.json >> performance_report.md
            echo '```' >> performance_report.md
            echo '' >> performance_report.md
          fi
          
          echo '## Load Test Results' >> performance_report.md
          echo '' >> performance_report.md
          
          if [ -f load_test_results.json ]; then
            echo '```json' >> performance_report.md
            cat load_test_results.json >> performance_report.md
            echo '```' >> performance_report.md
          fi
          
          echo '' >> performance_report.md
          echo '## Performance Thresholds' >> performance_report.md
          echo '' >> performance_report.md
          echo '- âœ… **Excellent**: < 50ms average response time' >> performance_report.md
          echo '- âš ï¸ **Good**: 50-200ms average response time' >> performance_report.md
          echo '- âŒ **Poor**: > 200ms average response time' >> performance_report.md
          echo '- ðŸŽ¯ **Target Success Rate**: > 99%' >> performance_report.md

      - name: Stop application
        if: always()
        run: |
          if [ ! -z "$APP_PID" ]; then
            kill $APP_PID || true
          fi

      - name: Upload performance artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ github.run_number }}
          path: |
            performance_results.json
            load_test_results.json
            performance_report.md

      - name: Create performance summary
        run: |
          echo "## Performance Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f performance_results.json ]; then
            echo "### Endpoint Performance" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          with open('performance_results.json') as f:
              data = json.load(f)
          for result in data:
              print(f\"{result['endpoint']}: {result['avg_ms']}ms avg\")
          " >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f load_test_results.json ]; then
            echo "### Load Test Results" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            python3 -c "
          import json
          with open('load_test_results.json') as f:
              data = json.load(f)
          print(f\"Success Rate: {data['success_rate']}%\")
          print(f\"Avg Response Time: {data['avg_response_time_ms']}ms\")
          print(f\"Requests/sec: {data['requests_per_second']}\")
          " >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
          fi

  docker-performance:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event_name == 'push'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          load: true
          cache-from: type=gha
          tags: performance-test:latest

      - name: Run Docker performance test
        run: |
          # Start container
          docker run -d --name perf-test \
            -p 8080:2542 \
            -e FORCE_DISABLE_COLLECTION=true \
            -e COLLECTION_ENABLED=false \
            performance-test:latest
          
          # Wait for startup
          sleep 15
          
          # Performance test
          time curl -f http://localhost:8080/health
          time curl -f http://localhost:8080/api/health
          
          # Container stats
          docker stats perf-test --no-stream --format "table {{.CPUPerc}}\t{{.MemUsage}}\t{{.NetIO}}"
          
          # Cleanup
          docker stop perf-test
          docker rm perf-test