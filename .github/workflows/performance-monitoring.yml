name: Performance Monitoring & Optimization

on:
  schedule:
    - cron: '0 */6 * * *'  # Every 6 hours
  workflow_dispatch:
    inputs:
      test_duration:
        description: 'Test duration in seconds'
        required: false
        type: number
        default: 300
      concurrent_users:
        description: 'Number of concurrent users'
        required: false
        type: number
        default: 50
      endpoints:
        description: 'Endpoints to test (comma-separated)'
        required: false
        type: string
        default: '/health,/api/blacklist/active,/api/collection/status'

env:
  REGISTRY: registry.jclee.me
  IMAGE_NAME: blacklist
  MONITORING_RETENTION_DAYS: 30

jobs:
  # Performance baseline measurement
  performance-baseline:
    name: 📊 Performance Baseline
    runs-on: self-hosted
    timeout-minutes: 15
    outputs:
      baseline-response-time: ${{ steps.baseline.outputs.response-time }}
      baseline-throughput: ${{ steps.baseline.outputs.throughput }}
      baseline-error-rate: ${{ steps.baseline.outputs.error-rate }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup performance testing environment
        run: |
          # Install performance testing tools
          pip install --upgrade pip
          pip install requests httpx aiohttp locust matplotlib seaborn pandas

      - name: Measure baseline performance
        id: baseline
        run: |
          cat > performance_test.py << 'EOF'
import time
import statistics
import concurrent.futures
import requests
import json
from typing import List, Dict

def single_request(url: str, timeout: int = 5) -> Dict:
    start_time = time.time()
    try:
        response = requests.get(url, timeout=timeout)
        end_time = time.time()
        return {
            'success': response.status_code == 200,
            'response_time': end_time - start_time,
            'status_code': response.status_code,
            'content_length': len(response.content)
        }
    except Exception as e:
        end_time = time.time()
        return {
            'success': False,
            'response_time': end_time - start_time,
            'status_code': 0,
            'content_length': 0,
            'error': str(e)
        }

def performance_test(base_url: str, endpoints: List[str], duration: int = 60, concurrent_users: int = 10):
    print(f"🚀 Starting performance test for {duration}s with {concurrent_users} concurrent users")
    
    results = []
    start_time = time.time()
    
    def worker():
        worker_results = []
        while time.time() - start_time < duration:
            for endpoint in endpoints:
                url = f"{base_url}{endpoint}"
                result = single_request(url)
                result['endpoint'] = endpoint
                result['timestamp'] = time.time()
                worker_results.append(result)
                time.sleep(0.1)  # Small delay between requests
        return worker_results
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_users) as executor:
        futures = [executor.submit(worker) for _ in range(concurrent_users)]
        for future in concurrent.futures.as_completed(futures):
            results.extend(future.result())
    
    return results

def analyze_results(results: List[Dict]) -> Dict:
    successful_results = [r for r in results if r['success']]
    failed_results = [r for r in results if not r['success']]
    
    if not successful_results:
        return {
            'total_requests': len(results),
            'successful_requests': 0,
            'failed_requests': len(failed_results),
            'error_rate': 100.0,
            'avg_response_time': 0,
            'median_response_time': 0,
            'p95_response_time': 0,
            'p99_response_time': 0,
            'throughput': 0
        }
    
    response_times = [r['response_time'] for r in successful_results]
    
    return {
        'total_requests': len(results),
        'successful_requests': len(successful_results),
        'failed_requests': len(failed_results),
        'error_rate': (len(failed_results) / len(results)) * 100,
        'avg_response_time': statistics.mean(response_times),
        'median_response_time': statistics.median(response_times),
        'p95_response_time': statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else max(response_times),
        'p99_response_time': statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else max(response_times),
        'throughput': len(successful_results) / max(response_times) if response_times else 0
    }

if __name__ == "__main__":
    import sys
    
    base_url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:32542"
    endpoints = sys.argv[2].split(',') if len(sys.argv) > 2 else ['/health']
    duration = int(sys.argv[3]) if len(sys.argv) > 3 else 60
    concurrent_users = int(sys.argv[4]) if len(sys.argv) > 4 else 10
    
    print(f"Testing: {base_url}")
    print(f"Endpoints: {endpoints}")
    print(f"Duration: {duration}s")
    print(f"Concurrent users: {concurrent_users}")
    
    results = performance_test(base_url, endpoints, duration, concurrent_users)
    analysis = analyze_results(results)
    
    print("\n📊 Performance Results:")
    print(f"Total requests: {analysis['total_requests']}")
    print(f"Successful requests: {analysis['successful_requests']}")
    print(f"Failed requests: {analysis['failed_requests']}")
    print(f"Error rate: {analysis['error_rate']:.2f}%")
    print(f"Average response time: {analysis['avg_response_time']:.3f}s")
    print(f"Median response time: {analysis['median_response_time']:.3f}s")
    print(f"95th percentile: {analysis['p95_response_time']:.3f}s")
    print(f"99th percentile: {analysis['p99_response_time']:.3f}s")
    print(f"Throughput: {analysis['throughput']:.2f} req/s")
    
    # Save results to file
    with open('performance_results.json', 'w') as f:
        json.dump({
            'analysis': analysis,
            'raw_results': results,
            'test_config': {
                'base_url': base_url,
                'endpoints': endpoints,
                'duration': duration,
                'concurrent_users': concurrent_users
            }
        }, f, indent=2)
    
    # Output for GitHub Actions
    print(f"::set-output name=response-time::{analysis['avg_response_time']:.3f}")
    print(f"::set-output name=throughput::{analysis['throughput']:.2f}")
    print(f"::set-output name=error-rate::{analysis['error_rate']:.2f}")
EOF

          # Determine test target
          if curl -sf http://localhost:32542/health > /dev/null 2>&1; then
            BASE_URL="http://localhost:32542"
            echo "🎯 Testing local Docker instance"
          else
            BASE_URL="https://blacklist.jclee.me"
            echo "🎯 Testing production instance"
          fi
          
          # Run baseline test
          ENDPOINTS="${{ github.event.inputs.endpoints || '/health,/api/blacklist/active' }}"
          DURATION="${{ github.event.inputs.test_duration || 60 }}"
          CONCURRENT_USERS="${{ github.event.inputs.concurrent_users || 10 }}"
          
          python performance_test.py "$BASE_URL" "$ENDPOINTS" "$DURATION" "$CONCURRENT_USERS"

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-baseline-results
          path: performance_results.json
          retention-days: ${{ env.MONITORING_RETENTION_DAYS }}

  # Load testing with different scenarios
  load-testing:
    name: 🔄 Load Testing
    runs-on: self-hosted
    timeout-minutes: 20
    needs: performance-baseline
    strategy:
      matrix:
        scenario:
          - name: light-load
            users: 10
            duration: 300
          - name: moderate-load
            users: 50
            duration: 300
          - name: heavy-load
            users: 100
            duration: 180
          - name: spike-test
            users: 200
            duration: 60
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup load testing with Locust
        run: |
          pip install locust

      - name: Create Locust test file
        run: |
          cat > locustfile.py << 'EOF'
from locust import HttpUser, task, between
import random

class BlacklistUser(HttpUser):
    wait_time = between(1, 3)
    
    def on_start(self):
        """Called when a user starts"""
        pass
    
    @task(10)
    def health_check(self):
        """Health check endpoint - most frequent"""
        self.client.get("/health")
    
    @task(5)
    def get_blacklist(self):
        """Get active blacklist"""
        self.client.get("/api/blacklist/active")
    
    @task(3)
    def collection_status(self):
        """Check collection status"""
        self.client.get("/api/collection/status")
    
    @task(2)
    def fortigate_endpoint(self):
        """FortiGate integration endpoint"""
        self.client.get("/api/fortigate")
    
    @task(1)
    def statistics_page(self):
        """Statistics dashboard"""
        self.client.get("/statistics")
EOF

      - name: Run load test - ${{ matrix.scenario.name }}
        run: |
          # Determine test target
          if curl -sf http://localhost:32542/health > /dev/null 2>&1; then
            HOST="http://localhost:32542"
            echo "🎯 Load testing local Docker instance"
          else
            HOST="https://blacklist.jclee.me"
            echo "🎯 Load testing production instance"
          fi
          
          echo "🔄 Running ${{ matrix.scenario.name }} scenario"
          echo "Users: ${{ matrix.scenario.users }}"
          echo "Duration: ${{ matrix.scenario.duration }}s"
          
          # Run Locust in headless mode
          locust -f locustfile.py \
            --host="$HOST" \
            --users=${{ matrix.scenario.users }} \
            --spawn-rate=10 \
            --run-time=${{ matrix.scenario.duration }}s \
            --headless \
            --csv=load_test_${{ matrix.scenario.name }} \
            --html=load_test_${{ matrix.scenario.name }}_report.html

      - name: Analyze load test results
        run: |
          cat > analyze_load_test.py << 'EOF'
import csv
import json
import sys

def analyze_locust_results(csv_file):
    results = {
        'requests': [],
        'summary': {}
    }
    
    try:
        with open(f"{csv_file}_stats.csv", 'r') as f:
            reader = csv.DictReader(f)
            for row in reader:
                if row['Type'] == 'GET':
                    results['requests'].append({
                        'name': row['Name'],
                        'requests': int(row['Request Count']),
                        'failures': int(row['Failure Count']),
                        'avg_response_time': float(row['Average Response Time']),
                        'min_response_time': float(row['Min Response Time']),
                        'max_response_time': float(row['Max Response Time']),
                        'rps': float(row['Requests/s'])
                    })
        
        # Calculate summary
        total_requests = sum(r['requests'] for r in results['requests'])
        total_failures = sum(r['failures'] for r in results['requests'])
        avg_response_time = sum(r['avg_response_time'] * r['requests'] for r in results['requests']) / total_requests if total_requests > 0 else 0
        total_rps = sum(r['rps'] for r in results['requests'])
        
        results['summary'] = {
            'total_requests': total_requests,
            'total_failures': total_failures,
            'failure_rate': (total_failures / total_requests * 100) if total_requests > 0 else 0,
            'avg_response_time': avg_response_time,
            'total_rps': total_rps
        }
        
    except FileNotFoundError:
        print(f"CSV file {csv_file}_stats.csv not found")
        return None
    
    return results

if __name__ == "__main__":
    csv_file = sys.argv[1] if len(sys.argv) > 1 else "load_test"
    results = analyze_locust_results(csv_file)
    
    if results:
        print(f"\n📊 Load Test Analysis - {csv_file}")
        print("=" * 50)
        print(f"Total requests: {results['summary']['total_requests']}")
        print(f"Total failures: {results['summary']['total_failures']}")
        print(f"Failure rate: {results['summary']['failure_rate']:.2f}%")
        print(f"Average response time: {results['summary']['avg_response_time']:.3f}ms")
        print(f"Requests per second: {results['summary']['total_rps']:.2f}")
        
        with open(f"{csv_file}_analysis.json", 'w') as f:
            json.dump(results, f, indent=2)
EOF
          
          python analyze_load_test.py "load_test_${{ matrix.scenario.name }}"

      - name: Upload load test results
        uses: actions/upload-artifact@v3
        with:
          name: load-test-${{ matrix.scenario.name }}
          path: |
            load_test_${{ matrix.scenario.name }}_*.csv
            load_test_${{ matrix.scenario.name }}_report.html
            load_test_${{ matrix.scenario.name }}_analysis.json
          retention-days: ${{ env.MONITORING_RETENTION_DAYS }}

  # Resource monitoring during tests
  resource-monitoring:
    name: 📈 Resource Monitoring
    runs-on: self-hosted
    timeout-minutes: 25
    needs: performance-baseline
    steps:
      - name: Monitor system resources
        run: |
          cat > resource_monitor.py << 'EOF'
import psutil
import time
import json
import docker
from datetime import datetime

def monitor_resources(duration=300, interval=5):
    """Monitor system resources for specified duration"""
    results = {
        'monitoring_start': datetime.now().isoformat(),
        'duration': duration,
        'interval': interval,
        'samples': []
    }
    
    # Docker client for container stats
    try:
        docker_client = docker.from_env()
    except:
        docker_client = None
    
    start_time = time.time()
    while time.time() - start_time < duration:
        sample = {
            'timestamp': datetime.now().isoformat(),
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory': {
                'percent': psutil.virtual_memory().percent,
                'available_gb': psutil.virtual_memory().available / (1024**3),
                'used_gb': psutil.virtual_memory().used / (1024**3)
            },
            'disk': {
                'percent': psutil.disk_usage('/').percent,
                'free_gb': psutil.disk_usage('/').free / (1024**3)
            },
            'network': {
                'bytes_sent': psutil.net_io_counters().bytes_sent,
                'bytes_recv': psutil.net_io_counters().bytes_recv
            }
        }
        
        # Docker container stats
        if docker_client:
            try:
                containers = docker_client.containers.list()
                sample['containers'] = []
                for container in containers:
                    if 'blacklist' in container.name.lower():
                        stats = container.stats(stream=False)
                        sample['containers'].append({
                            'name': container.name,
                            'cpu_percent': calculate_cpu_percent(stats),
                            'memory_usage_mb': stats['memory_stats'].get('usage', 0) / (1024**2),
                            'memory_limit_mb': stats['memory_stats'].get('limit', 0) / (1024**2)
                        })
            except Exception as e:
                sample['docker_error'] = str(e)
        
        results['samples'].append(sample)
        time.sleep(interval)
    
    return results

def calculate_cpu_percent(stats):
    """Calculate CPU percentage from Docker stats"""
    try:
        cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
                   stats['precpu_stats']['cpu_usage']['total_usage']
        system_delta = stats['cpu_stats']['system_cpu_usage'] - \
                      stats['precpu_stats']['system_cpu_usage']
        
        if system_delta > 0:
            return (cpu_delta / system_delta) * len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100
    except (KeyError, ZeroDivisionError):
        pass
    return 0

if __name__ == "__main__":
    print("📈 Starting resource monitoring...")
    results = monitor_resources(duration=300, interval=5)  # 5 minutes, 5-second intervals
    
    # Save results
    with open('resource_monitoring.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    # Summary statistics
    cpu_values = [s['cpu_percent'] for s in results['samples']]
    memory_values = [s['memory']['percent'] for s in results['samples']]
    
    print(f"\n📊 Resource Monitoring Summary")
    print(f"Duration: {results['duration']}s")
    print(f"Samples: {len(results['samples'])}")
    print(f"CPU Usage - Avg: {sum(cpu_values)/len(cpu_values):.1f}%, Max: {max(cpu_values):.1f}%")
    print(f"Memory Usage - Avg: {sum(memory_values)/len(memory_values):.1f}%, Max: {max(memory_values):.1f}%")
EOF
          
          pip install psutil docker
          python resource_monitor.py &
          MONITOR_PID=$!
          
          # Let monitoring run during other tests
          sleep 300
          
          # Kill monitoring if still running
          kill $MONITOR_PID 2>/dev/null || true

      - name: Upload resource monitoring results
        uses: actions/upload-artifact@v3
        with:
          name: resource-monitoring
          path: resource_monitoring.json
          retention-days: ${{ env.MONITORING_RETENTION_DAYS }}

  # Performance analysis and reporting
  performance-analysis:
    name: 📊 Performance Analysis
    runs-on: self-hosted
    timeout-minutes: 15
    needs: [performance-baseline, load-testing, resource-monitoring]
    if: always()
    steps:
      - name: Download all performance artifacts
        uses: actions/download-artifact@v3

      - name: Generate comprehensive performance report
        run: |
          cat > generate_report.py << 'EOF'
import json
import os
import glob
from datetime import datetime

def load_json_file(filepath):
    try:
        with open(filepath, 'r') as f:
            return json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        return None

def generate_performance_report():
    report = {
        'generated_at': datetime.now().isoformat(),
        'baseline': {},
        'load_tests': {},
        'resource_monitoring': {},
        'summary': {},
        'recommendations': []
    }
    
    # Load baseline results
    baseline_file = glob.glob('**/performance_results.json', recursive=True)
    if baseline_file:
        baseline_data = load_json_file(baseline_file[0])
        if baseline_data:
            report['baseline'] = baseline_data['analysis']
    
    # Load load test results
    load_test_files = glob.glob('**/*_analysis.json', recursive=True)
    for file in load_test_files:
        scenario_name = os.path.basename(file).replace('_analysis.json', '').replace('load_test_', '')
        data = load_json_file(file)
        if data:
            report['load_tests'][scenario_name] = data['summary']
    
    # Load resource monitoring
    resource_files = glob.glob('**/resource_monitoring.json', recursive=True)
    if resource_files:
        resource_data = load_json_file(resource_files[0])
        if resource_data and resource_data.get('samples'):
            samples = resource_data['samples']
            cpu_values = [s['cpu_percent'] for s in samples]
            memory_values = [s['memory']['percent'] for s in samples]
            
            report['resource_monitoring'] = {
                'duration': resource_data.get('duration', 0),
                'samples_count': len(samples),
                'cpu': {
                    'avg': sum(cpu_values) / len(cpu_values) if cpu_values else 0,
                    'max': max(cpu_values) if cpu_values else 0,
                    'min': min(cpu_values) if cpu_values else 0
                },
                'memory': {
                    'avg': sum(memory_values) / len(memory_values) if memory_values else 0,
                    'max': max(memory_values) if memory_values else 0,
                    'min': min(memory_values) if memory_values else 0
                }
            }
    
    # Generate summary and recommendations
    if report['baseline']:
        avg_response_time = report['baseline'].get('avg_response_time', 0)
        error_rate = report['baseline'].get('error_rate', 0)
        
        report['summary']['baseline_performance'] = 'excellent' if avg_response_time < 0.1 else \
                                                   'good' if avg_response_time < 0.5 else \
                                                   'fair' if avg_response_time < 1.0 else 'poor'
        
        if avg_response_time > 0.5:
            report['recommendations'].append("Consider optimizing response times - current average exceeds 500ms")
        
        if error_rate > 1:
            report['recommendations'].append(f"Error rate is {error_rate:.2f}% - investigate failed requests")
    
    # Resource recommendations
    if report['resource_monitoring']:
        cpu_avg = report['resource_monitoring']['cpu']['avg']
        memory_avg = report['resource_monitoring']['memory']['avg']
        
        if cpu_avg > 80:
            report['recommendations'].append("High CPU usage detected - consider scaling or optimization")
        
        if memory_avg > 80:
            report['recommendations'].append("High memory usage detected - check for memory leaks")
    
    return report

def format_markdown_report(report):
    md = f"""# 📊 Performance Monitoring Report

Generated: {report['generated_at']}

## 🎯 Baseline Performance

"""
    
    if report['baseline']:
        baseline = report['baseline']
        md += f"""
- **Average Response Time**: {baseline.get('avg_response_time', 0):.3f}s
- **95th Percentile**: {baseline.get('p95_response_time', 0):.3f}s
- **99th Percentile**: {baseline.get('p99_response_time', 0):.3f}s
- **Error Rate**: {baseline.get('error_rate', 0):.2f}%
- **Throughput**: {baseline.get('throughput', 0):.2f} req/s
"""
    else:
        md += "No baseline data available.\n"
    
    # Load test results
    md += "\n## 🔄 Load Test Results\n\n"
    if report['load_tests']:
        for scenario, data in report['load_tests'].items():
            md += f"""
### {scenario.replace('-', ' ').title()}
- **Total Requests**: {data.get('total_requests', 0):,}
- **Failure Rate**: {data.get('failure_rate', 0):.2f}%
- **Average Response Time**: {data.get('avg_response_time', 0):.1f}ms
- **Requests/Second**: {data.get('total_rps', 0):.2f}

"""
    else:
        md += "No load test data available.\n"
    
    # Resource monitoring
    md += "\n## 📈 Resource Utilization\n\n"
    if report['resource_monitoring']:
        rm = report['resource_monitoring']
        md += f"""
- **Monitoring Duration**: {rm.get('duration', 0)}s
- **CPU Usage**: Avg {rm['cpu']['avg']:.1f}%, Max {rm['cpu']['max']:.1f}%
- **Memory Usage**: Avg {rm['memory']['avg']:.1f}%, Max {rm['memory']['max']:.1f}%
"""
    else:
        md += "No resource monitoring data available.\n"
    
    # Recommendations
    md += "\n## 🎯 Recommendations\n\n"
    if report['recommendations']:
        for rec in report['recommendations']:
            md += f"- {rec}\n"
    else:
        md += "- No specific recommendations at this time\n"
    
    md += f"\n---\n*Report generated by GitHub Actions Performance Monitoring*\n"
    
    return md

if __name__ == "__main__":
    print("📊 Generating performance report...")
    report = generate_performance_report()
    
    # Save JSON report
    with open('performance_report.json', 'w') as f:
        json.dump(report, f, indent=2)
    
    # Generate Markdown report
    markdown_report = format_markdown_report(report)
    with open('performance_report.md', 'w') as f:
        f.write(markdown_report)
    
    print("✅ Performance report generated!")
    print(markdown_report)
EOF
          
          python generate_report.py

      - name: Upload performance report
        uses: actions/upload-artifact@v3
        with:
          name: performance-report
          path: |
            performance_report.json
            performance_report.md
          retention-days: ${{ env.MONITORING_RETENTION_DAYS }}

      - name: Comment on PR with performance summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            try {
              const report = fs.readFileSync('performance_report.md', 'utf8');
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: report
              });
            } catch (error) {
              console.log('Could not post performance report to PR:', error);
            }

  # Performance trend analysis (compare with historical data)
  trend-analysis:
    name: 📈 Trend Analysis
    runs-on: self-hosted
    timeout-minutes: 10
    needs: performance-analysis
    if: github.ref == 'refs/heads/main'
    steps:
      - name: Download performance report
        uses: actions/download-artifact@v3
        with:
          name: performance-report

      - name: Store performance metrics for trend analysis
        run: |
          # Create trend data directory if it doesn't exist
          mkdir -p /opt/performance-trends
          
          # Copy current report with timestamp
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          cp performance_report.json "/opt/performance-trends/performance_${TIMESTAMP}.json"
          
          # Keep only last 30 reports
          find /opt/performance-trends -name "performance_*.json" -type f | sort | head -n -30 | xargs rm -f
          
          echo "📈 Performance data stored for trend analysis"
          echo "Available reports: $(ls -1 /opt/performance-trends | wc -l)"

      - name: Generate trend analysis
        run: |
          cat > trend_analysis.py << 'EOF'
import json
import glob
import statistics
from datetime import datetime, timedelta

def analyze_trends():
    # Load all historical performance data
    trend_files = sorted(glob.glob('/opt/performance-trends/performance_*.json'))
    
    if len(trend_files) < 2:
        print("📊 Insufficient data for trend analysis (need at least 2 data points)")
        return
    
    metrics = []
    for file in trend_files[-10:]:  # Last 10 reports
        try:
            with open(file, 'r') as f:
                data = json.load(f)
                if data.get('baseline'):
                    baseline = data['baseline']
                    metrics.append({
                        'timestamp': file.split('_')[-1].replace('.json', ''),
                        'response_time': baseline.get('avg_response_time', 0),
                        'error_rate': baseline.get('error_rate', 0),
                        'throughput': baseline.get('throughput', 0)
                    })
        except (FileNotFoundError, json.JSONDecodeError):
            continue
    
    if len(metrics) < 2:
        print("📊 Insufficient valid metrics for trend analysis")
        return
    
    # Calculate trends
    response_times = [m['response_time'] for m in metrics]
    error_rates = [m['error_rate'] for m in metrics]
    throughputs = [m['throughput'] for m in metrics]
    
    print("📈 Performance Trend Analysis")
    print("=" * 40)
    print(f"Data points: {len(metrics)}")
    print(f"Time range: {metrics[0]['timestamp']} to {metrics[-1]['timestamp']}")
    print()
    
    # Response time trends
    rt_trend = "improving" if response_times[-1] < response_times[0] else "degrading"
    rt_change = ((response_times[-1] - response_times[0]) / response_times[0] * 100) if response_times[0] > 0 else 0
    
    print(f"Response Time:")
    print(f"  Current: {response_times[-1]:.3f}s")
    print(f"  Average: {statistics.mean(response_times):.3f}s")
    print(f"  Trend: {rt_trend} ({rt_change:+.1f}%)")
    print()
    
    # Error rate trends
    er_trend = "improving" if error_rates[-1] < error_rates[0] else "degrading"
    er_change = error_rates[-1] - error_rates[0]
    
    print(f"Error Rate:")
    print(f"  Current: {error_rates[-1]:.2f}%")
    print(f"  Average: {statistics.mean(error_rates):.2f}%")
    print(f"  Trend: {er_trend} ({er_change:+.2f}%)")
    print()
    
    # Throughput trends
    tp_trend = "improving" if throughputs[-1] > throughputs[0] else "degrading"
    tp_change = ((throughputs[-1] - throughputs[0]) / throughputs[0] * 100) if throughputs[0] > 0 else 0
    
    print(f"Throughput:")
    print(f"  Current: {throughputs[-1]:.2f} req/s")
    print(f"  Average: {statistics.mean(throughputs):.2f} req/s")
    print(f"  Trend: {tp_trend} ({tp_change:+.1f}%)")
    print()
    
    # Alerts
    if abs(rt_change) > 20:
        print(f"⚠️  ALERT: Response time changed by {rt_change:+.1f}%")
    
    if abs(er_change) > 5:
        print(f"⚠️  ALERT: Error rate changed by {er_change:+.2f}%")
    
    if abs(tp_change) > 20:
        print(f"⚠️  ALERT: Throughput changed by {tp_change:+.1f}%")

if __name__ == "__main__":
    analyze_trends()
EOF
          
          python trend_analysis.py

  # Summary
  monitoring-summary:
    name: 📋 Monitoring Summary
    runs-on: self-hosted
    timeout-minutes: 5
    needs: [performance-baseline, load-testing, resource-monitoring, performance-analysis, trend-analysis]
    if: always()
    steps:
      - name: Generate monitoring summary
        run: |
          echo "📊 PERFORMANCE MONITORING SUMMARY"
          echo "================================="
          echo ""
          echo "🎯 Completed Tasks:"
          echo "- Performance Baseline: ${{ needs.performance-baseline.result }}"
          echo "- Load Testing: ${{ needs.load-testing.result }}"
          echo "- Resource Monitoring: ${{ needs.resource-monitoring.result }}"
          echo "- Performance Analysis: ${{ needs.performance-analysis.result }}"
          echo "- Trend Analysis: ${{ needs.trend-analysis.result }}"
          echo ""
          echo "📊 Key Metrics:"
          echo "- Baseline Response Time: ${{ needs.performance-baseline.outputs.baseline-response-time }}s"
          echo "- Baseline Throughput: ${{ needs.performance-baseline.outputs.baseline-throughput }} req/s"
          echo "- Baseline Error Rate: ${{ needs.performance-baseline.outputs.baseline-error-rate }}%"
          echo ""
          echo "⏰ Monitoring completed: $(date -u +'%Y-%m-%d %H:%M:%S UTC')"
          
          # Determine overall status
          if [[ "${{ needs.performance-baseline.result }}" == "success" && 
                "${{ needs.load-testing.result }}" == "success" && 
                "${{ needs.performance-analysis.result }}" == "success" ]]; then
            echo ""
            echo "✅ Performance monitoring completed successfully!"
          else
            echo ""
            echo "⚠️ Performance monitoring completed with some issues"
          fi