#!/usr/bin/env python3
"""
Database Migration and Consolidation Script
Generated by Real Automation System v11.1

This script consolidates multiple SQLite databases into a single location
and ensures consistent schema across all databases.
"""

import os
import shutil
import sqlite3
from datetime import datetime
from pathlib import Path


class DatabaseMigration:
    """ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ë° í†µí•© ê´€ë¦¬"""

    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_dir = Path(f"db_backup_{self.timestamp}")
        self.target_dir = Path("instance")
        self.db_mapping = {
            "blacklist": {
                "sources": [
                    "instance/blacklist.db",
                    "instance/blacklist_dev.db",
                    "data/blacklist.db",
                    "config/data/blacklist.db",
                ],
                "target": "instance/blacklist.db",
                "keep_largest": True,
            },
            "api_keys": {
                "sources": ["instance/api_keys.db", "data/api_keys.db"],
                "target": "instance/api_keys.db",
                "keep_latest": True,
            },
            "monitoring": {
                "sources": [
                    "monitoring/deployment_monitoring.db",
                    "data_backup/deployment_monitoring.db",
                ],
                "target": "instance/monitoring.db",
                "keep_latest": True,
            },
            "collection": {
                "sources": ["data/collection_config.db", "instance/secudium.db"],
                "target": "instance/collection.db",
                "merge": True,
            },
        }

    def backup_databases(self):
        """ëª¨ë“  ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…"""
        print(f"ğŸ“¦ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ì¤‘... ({self.backup_dir})")

        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_dir.mkdir(exist_ok=True)

        backed_up = []
        for db_type, config in self.db_mapping.items():
            for source in config["sources"]:
                if os.path.exists(source):
                    backup_name = f"{db_type}_{Path(source).name}_{self.timestamp}"
                    backup_path = self.backup_dir / backup_name
                    try:
                        shutil.copy2(source, backup_path)
                        backed_up.append(source)
                        print(f"  âœ“ {source} â†’ {backup_path}")
                    except Exception as e:
                        print(f"  âœ— {source} ë°±ì—… ì‹¤íŒ¨: {e}")

        print(f"  ì´ {len(backed_up)}ê°œ DB ë°±ì—… ì™„ë£Œ")
        return backed_up

    def analyze_databases(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ë° ìƒíƒœ í™•ì¸"""
        print("\nğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì¤‘...")

        analysis = {}
        for db_type, config in self.db_mapping.items():
            analysis[db_type] = {
                "files": [],
                "total_size": 0,
                "table_count": {},
                "row_count": {},
            }

            for source in config["sources"]:
                if os.path.exists(source):
                    size = os.path.getsize(source)
                    analysis[db_type]["total_size"] += size

                    # SQLite ì—°ê²°í•˜ì—¬ í…Œì´ë¸” ì •ë³´ ìˆ˜ì§‘
                    try:
                        conn = sqlite3.connect(source)
                        cursor = conn.cursor()

                        # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
                        cursor.execute(
                            "SELECT name FROM sqlite_master WHERE type='table'"
                        )
                        tables = cursor.fetchall()

                        table_info = {}
                        for table in tables:
                            table_name = table[0]
                            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                            count = cursor.fetchone()[0]
                            table_info[table_name] = count

                        conn.close()

                        analysis[db_type]["files"].append(
                            {
                                "path": source,
                                "size": size,
                                "tables": table_info,
                                "modified": datetime.fromtimestamp(
                                    os.path.getmtime(source)
                                ),
                            }
                        )

                    except Exception as e:
                        print(f"  âš ï¸ {source} ë¶„ì„ ì‹¤íŒ¨: {e}")

        return analysis

    def consolidate_databases(self, analysis):
        """ë°ì´í„°ë² ì´ìŠ¤ í†µí•©"""
        print("\nğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ í†µí•© ì¤‘...")

        # target ë””ë ‰í† ë¦¬ í™•ì¸
        self.target_dir.mkdir(exist_ok=True)

        consolidated = []
        for db_type, config in self.db_mapping.items():
            target = Path(config["target"])

            if config.get("keep_largest"):
                # ê°€ì¥ í° íŒŒì¼ ì„ íƒ
                largest = None
                largest_size = 0
                for file_info in analysis[db_type]["files"]:
                    if file_info["size"] > largest_size:
                        largest = file_info["path"]
                        largest_size = file_info["size"]

                if largest and largest != str(target):
                    print(f"  ğŸ“‚ {db_type}: {largest} â†’ {target}")
                    shutil.copy2(largest, target)
                    consolidated.append(str(target))

            elif config.get("keep_latest"):
                # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
                latest = None
                latest_time = datetime.min
                for file_info in analysis[db_type]["files"]:
                    if file_info["modified"] > latest_time:
                        latest = file_info["path"]
                        latest_time = file_info["modified"]

                if latest and latest != str(target):
                    print(f"  ğŸ“‚ {db_type}: {latest} â†’ {target}")
                    shutil.copy2(latest, target)
                    consolidated.append(str(target))

            elif config.get("merge"):
                # ì—¬ëŸ¬ DB ë³‘í•© (ê°„ë‹¨í•œ êµ¬í˜„)
                print(f"  ğŸ”€ {db_type}: ë³‘í•© ì‘ì—… í•„ìš” (ìˆ˜ë™ ì²˜ë¦¬ ê¶Œì¥)")

        print(f"  âœ“ {len(consolidated)}ê°œ DB í†µí•© ì™„ë£Œ")
        return consolidated

    def cleanup_old_databases(self, keep_backups=True):
        """ì´ì „ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì •ë¦¬"""
        print("\nğŸ—‘ï¸ ì´ì „ ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì¤‘...")

        removed = []
        for db_type, config in self.db_mapping.items():
            target = Path(config["target"])

            for source in config["sources"]:
                source_path = Path(source)
                # targetê³¼ ë‹¤ë¥¸ íŒŒì¼ë“¤ë§Œ ì‚­ì œ
                if source_path.exists() and str(source_path) != str(target):
                    try:
                        if not keep_backups or self.backup_dir.exists():
                            os.remove(source_path)
                            removed.append(str(source_path))
                            print(f"  âœ“ {source_path} ì‚­ì œ")
                    except Exception as e:
                        print(f"  âœ— {source_path} ì‚­ì œ ì‹¤íŒ¨: {e}")

        # ë¹ˆ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì •ë¦¬
        empty_db = Path("data/database.db")
        if empty_db.exists() and os.path.getsize(empty_db) == 0:
            os.remove(empty_db)
            removed.append(str(empty_db))
            print(f"  âœ“ {empty_db} (ë¹ˆ íŒŒì¼) ì‚­ì œ")

        print(f"  ì´ {len(removed)}ê°œ ì´ì „ DB íŒŒì¼ ì‚­ì œ")
        return removed

    def update_environment_config(self):
        """í™˜ê²½ ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸"""
        print("\nâš™ï¸ í™˜ê²½ ì„¤ì • ì—…ë°ì´íŠ¸ ì¤‘...")

        env_files = [".env", ".env.test", "commands/config/.env.unified"]
        updates = {
            "DATABASE_PATH": "instance/blacklist.db",
            "API_KEYS_DB": "instance/api_keys.db",
            "MONITORING_DB": "instance/monitoring.db",
            "COLLECTION_DB": "instance/collection.db",
        }

        updated_files = []
        for env_file in env_files:
            if os.path.exists(env_file):
                try:
                    with open(env_file, "r") as f:
                        content = f.read()

                    # DB ê²½ë¡œ ì—…ë°ì´íŠ¸
                    for key, value in updates.items():
                        if key in content:
                            old_pattern = f"{key}=.*"
                            new_line = f"{key}={value}"
                            import re

                            content = re.sub(old_pattern, new_line, content)

                    with open(env_file, "w") as f:
                        f.write(content)

                    updated_files.append(env_file)
                    print(f"  âœ“ {env_file} ì—…ë°ì´íŠ¸ ì™„ë£Œ")

                except Exception as e:
                    print(f"  âœ— {env_file} ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

        return updated_files

    def generate_report(self, analysis, consolidated, removed):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ ìƒì„±"""
        report = f"""# ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ë³´ê³ ì„œ

## ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ“ˆ ë¶„ì„ ê²°ê³¼

### ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©
"""
        for db_type, info in analysis.items():
            report += f"\n#### {db_type.upper()}\n"
            report += f"- íŒŒì¼ ìˆ˜: {len(info['files'])}ê°œ\n"
            report += f"- ì´ í¬ê¸°: {info['total_size'] / 1024:.1f}KB\n"

            for file_info in info["files"]:
                report += f"  - {file_info['path']}: {file_info['size']/1024:.1f}KB\n"
                if file_info.get("tables"):
                    for table, count in file_info["tables"].items():
                        report += f"    â€¢ {table}: {count} rows\n"

        report += f"""
## âœ… í†µí•© ê²°ê³¼

### í†µí•©ëœ ë°ì´í„°ë² ì´ìŠ¤
- ì´ {len(consolidated)}ê°œ ë°ì´í„°ë² ì´ìŠ¤ í†µí•©
- ëŒ€ìƒ ë””ë ‰í† ë¦¬: instance/

### ì •ë¦¬ëœ íŒŒì¼
- ì´ {len(removed)}ê°œ ì¤‘ë³µ íŒŒì¼ ì‚­ì œ
- ë°±ì—… ìœ„ì¹˜: {self.backup_dir}/

## ğŸ“ ìµœì¢… êµ¬ì¡°
```
instance/
â”œâ”€â”€ blacklist.db     # ë©”ì¸ ë¸”ë™ë¦¬ìŠ¤íŠ¸ DB
â”œâ”€â”€ api_keys.db      # API í‚¤ ê´€ë¦¬ DB
â”œâ”€â”€ monitoring.db    # ëª¨ë‹ˆí„°ë§ ë°ì´í„° DB
â””â”€â”€ collection.db    # ìˆ˜ì§‘ ì„¤ì • DB
```

## ğŸ¯ ì¶”ê°€ ê¶Œì¥ì‚¬í•­

1. **Git ì»¤ë°‹**
   ```bash
   git add -A
   git commit -m "feat: consolidate database files into single location"
   ```

2. **ë°±ì—… í™•ì¸**
   - ë°±ì—… ë””ë ‰í† ë¦¬: `{self.backup_dir}/`
   - 30ì¼ í›„ ë°±ì—… ì‚­ì œ ê¶Œì¥

3. **ì½”ë“œ ì—…ë°ì´íŠ¸**
   - DB ê²½ë¡œ ì°¸ì¡° ì½”ë“œ í™•ì¸ ë° ì—…ë°ì´íŠ¸
   - í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìœ¼ë¡œ ê²€ì¦

---
*Real Automation System v11.1ì— ì˜í•´ ìë™ ìƒì„±*
"""

        with open("db_migration_report.md", "w") as f:
            f.write(report)

        print("\nğŸ“„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: db_migration_report.md")
        return report

    def execute(self):
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        print("=" * 50)
        print("ğŸš€ ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        print("=" * 50)

        # 1. ë°±ì—…
        backed_up = self.backup_databases()

        # 2. ë¶„ì„
        analysis = self.analyze_databases()

        # 3. í†µí•©
        consolidated = self.consolidate_databases(analysis)

        # 4. ì •ë¦¬
        removed = self.cleanup_old_databases()

        # 5. í™˜ê²½ ì„¤ì • ì—…ë°ì´íŠ¸
        updated_configs = self.update_environment_config()

        # 6. ë³´ê³ ì„œ ìƒì„±
        self.generate_report(analysis, consolidated, removed)

        print("\n" + "=" * 50)
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        print("=" * 50)
        print(f"ğŸ“¦ ë°±ì—…: {len(backed_up)}ê°œ")
        print(f"ğŸ”„ í†µí•©: {len(consolidated)}ê°œ")
        print(f"ğŸ—‘ï¸ ì‚­ì œ: {len(removed)}ê°œ")
        print(f"âš™ï¸ ì„¤ì •: {len(updated_configs)}ê°œ íŒŒì¼ ì—…ë°ì´íŠ¸")
        print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
        print("  1. ì• í”Œë¦¬ì¼€ì´ì…˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("  2. Git ì»¤ë°‹")
        print("  3. ë°±ì—… í™•ì¸")


if __name__ == "__main__":
    migration = DatabaseMigration()
    migration.execute()
