#!/usr/bin/env python3
"""
SECUDIUM BeautifulSoup4 ë¶„ì„
ë¡œê·¸ì¸ í›„ ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„í•˜ì—¬ ë°ì´í„° ìœ„ì¹˜ ì°¾ê¸°
"""
import requests
from bs4 import BeautifulSoup
import re
import json
import warnings
warnings.filterwarnings('ignore', message='Unverified HTTPS request')

BASE_URL = "https://secudium.skinfosec.co.kr"
USERNAME = "nextrade"
PASSWORD = "Sprtmxm1@3"

def analyze_secudium_with_bs4():
    """BeautifulSoup4ë¡œ SECUDIUM ì „ì²´ ë¶„ì„"""
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    print("=== SECUDIUM BS4 ë¶„ì„ ì‹œì‘ ===")
    
    print("\n1. ë¡œê·¸ì¸...")
    # ë©”ì¸ í˜ì´ì§€ ì ‘ì†
    main_resp = session.get(BASE_URL, verify=False)
    print(f"   ë©”ì¸ í˜ì´ì§€: {main_resp.status_code}")
    
    # ë¡œê·¸ì¸
    login_data = {
        'lang': 'ko',
        'is_otp': 'N',
        'is_expire': 'N',
        'login_name': USERNAME,
        'password': PASSWORD,
        'otp_value': ''
    }
    
    login_resp = session.get(f"{BASE_URL}/login", params=login_data, verify=False)
    print(f"   ë¡œê·¸ì¸: {login_resp.status_code}")
    
    print("\n2. ë¡œê·¸ì¸ í›„ ë©”ì¸ í˜ì´ì§€ BS4 ë¶„ì„...")
    
    # ë¡œê·¸ì¸ í›„ ë©”ì¸ í˜ì´ì§€ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸°
    post_login_resp = session.get(BASE_URL, verify=False)
    soup = BeautifulSoup(post_login_resp.text, 'html.parser')
    
    print(f"   í˜ì´ì§€ ì œëª©: {soup.title.string if soup.title else 'None'}")
    print(f"   HTML ê¸¸ì´: {len(post_login_resp.text)} bytes")
    
    # ëª¨ë“  ë§í¬ ë¶„ì„
    print(f"\n3. ëª¨ë“  ë§í¬ ë¶„ì„...")
    links = soup.find_all('a', href=True)
    print(f"   ì´ ë§í¬ ê°œìˆ˜: {len(links)}")
    
    interesting_links = []
    for link in links:
        href = link['href']
        text = link.get_text(strip=True)
        
        # ê´€ì‹¬ìˆëŠ” í‚¤ì›Œë“œ
        keywords = ['ë³´ì•ˆ', 'ë°ì´í„°', 'ë¦¬í¬íŠ¸', 'í†µê³„', 'ë¶„ì„', 'ëª©ë¡', 'ë¦¬ìŠ¤íŠ¸', 
                   'security', 'data', 'report', 'list', 'blacklist', 'threat', 
                   'malware', 'ip', 'scan', 'dashboard', 'main', 'menu']
        
        if any(keyword in text.lower() for keyword in keywords) or \
           any(keyword in href.lower() for keyword in keywords):
            interesting_links.append((text, href))
    
    print(f"   ê´€ì‹¬ìˆëŠ” ë§í¬: {len(interesting_links)}ê°œ")
    for text, href in interesting_links[:10]:  # ì²˜ìŒ 10ê°œë§Œ
        print(f"      {text}: {href}")
    
    print(f"\n4. ë„¤ë¹„ê²Œì´ì…˜/ë©”ë‰´ êµ¬ì¡° ë¶„ì„...")
    
    # ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œë“¤ ì°¾ê¸°
    nav_elements = soup.find_all(['nav', 'menu', 'ul'])
    nav_elements.extend(soup.find_all(class_=re.compile(r'nav|menu|sidebar', re.I)))
    nav_elements.extend(soup.find_all(id=re.compile(r'nav|menu|sidebar', re.I)))
    
    print(f"   ë„¤ë¹„ê²Œì´ì…˜ ìš”ì†Œ: {len(nav_elements)}ê°œ")
    
    for i, nav in enumerate(nav_elements):
        print(f"\n   ë„¤ë¹„ê²Œì´ì…˜ {i+1}: {nav.name}")
        if nav.get('class'):
            print(f"      í´ë˜ìŠ¤: {nav.get('class')}")
        if nav.get('id'):
            print(f"      ID: {nav.get('id')}")
        
        # ë„¤ë¹„ê²Œì´ì…˜ ë‚´ë¶€ ë§í¬
        nav_links = nav.find_all('a', href=True)
        if nav_links:
            print(f"      ë‚´ë¶€ ë§í¬: {len(nav_links)}ê°œ")
            for link in nav_links[:5]:  # ì²˜ìŒ 5ê°œë§Œ
                print(f"         {link.get_text(strip=True)}: {link['href']}")
    
    print(f"\n5. í¼ ë¶„ì„...")
    
    forms = soup.find_all('form')
    print(f"   í¼ ê°œìˆ˜: {len(forms)}")
    
    for i, form in enumerate(forms):
        print(f"\n   í¼ {i+1}:")
        print(f"      ì•¡ì…˜: {form.get('action', 'None')}")
        print(f"      ë©”ì„œë“œ: {form.get('method', 'GET')}")
        
        inputs = form.find_all(['input', 'select', 'textarea'])
        print(f"      ì…ë ¥ í•„ë“œ: {len(inputs)}ê°œ")
        
        for inp in inputs:
            name = inp.get('name', '')
            input_type = inp.get('type', inp.name)
            value = inp.get('value', '')
            if name:
                print(f"         {name}: {input_type} = {value}")
    
    print(f"\n6. JavaScript ë¶„ì„...")
    
    scripts = soup.find_all('script')
    print(f"   ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸: {len(scripts)}ê°œ")
    
    js_urls = []
    js_functions = []
    ajax_urls = []
    
    for script in scripts:
        # ì™¸ë¶€ ìŠ¤í¬ë¦½íŠ¸
        if script.get('src'):
            js_urls.append(script['src'])
        
        # ì¸ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸ ë¶„ì„
        if script.string:
            script_content = script.string
            
            # í•¨ìˆ˜ ì°¾ê¸°
            functions = re.findall(r'function\s+(\w+)\s*\(', script_content)
            js_functions.extend(functions)
            
            # AJAX URL ì°¾ê¸°
            ajax_patterns = [
                r'url\s*:\s*["\']([^"\']+)["\']',
                r'fetch\s*\(\s*["\']([^"\']+)["\']',
                r'\.get\s*\(\s*["\']([^"\']+)["\']',
                r'\.post\s*\(\s*["\']([^"\']+)["\']'
            ]
            
            for pattern in ajax_patterns:
                urls = re.findall(pattern, script_content)
                ajax_urls.extend(urls)
    
    print(f"      ì™¸ë¶€ JS íŒŒì¼: {len(js_urls)}ê°œ")
    for url in js_urls[:5]:
        print(f"         {url}")
    
    print(f"      í•¨ìˆ˜: {len(set(js_functions))}ê°œ")
    for func in list(set(js_functions))[:10]:
        print(f"         {func}()")
    
    print(f"      AJAX URL: {len(set(ajax_urls))}ê°œ")
    for url in list(set(ajax_urls))[:10]:
        print(f"         {url}")
    
    print(f"\n7. í…Œì´ë¸” êµ¬ì¡° ë¶„ì„...")
    
    tables = soup.find_all('table')
    print(f"   í…Œì´ë¸” ê°œìˆ˜: {len(tables)}")
    
    for i, table in enumerate(tables):
        print(f"\n   í…Œì´ë¸” {i+1}:")
        
        rows = table.find_all('tr')
        print(f"      í–‰ ê°œìˆ˜: {len(rows)}")
        
        if rows:
            # í—¤ë” ë¶„ì„
            header_row = rows[0]
            headers = [th.get_text(strip=True) for th in header_row.find_all(['th', 'td'])]
            print(f"      í—¤ë”: {headers}")
            
            # ë°ì´í„° í–‰ ìƒ˜í”Œ
            for j, row in enumerate(rows[1:3]):  # ì²˜ìŒ 2ê°œ ë°ì´í„° í–‰
                cells = [td.get_text(strip=True) for td in row.find_all('td')]
                if cells:
                    print(f"      ë°ì´í„° {j+1}: {cells}")
    
    print(f"\n8. ìˆ¨ê²¨ì§„ ìš”ì†Œ ë° ë™ì  ì»¨í…ì¸  ë¶„ì„...")
    
    # ìˆ¨ê²¨ì§„ divë“¤
    hidden_divs = soup.find_all('div', style=re.compile(r'display\s*:\s*none', re.I))
    hidden_divs.extend(soup.find_all(class_=re.compile(r'hidden', re.I)))
    
    print(f"   ìˆ¨ê²¨ì§„ ìš”ì†Œ: {len(hidden_divs)}ê°œ")
    
    for div in hidden_divs[:5]:
        div_id = div.get('id', 'no-id')
        div_class = div.get('class', [])
        print(f"      ID: {div_id}, í´ë˜ìŠ¤: {div_class}")
        
        # ìˆ¨ê²¨ì§„ ìš”ì†Œ ë‚´ë¶€ì˜ ë§í¬ë‚˜ ë°ì´í„°
        hidden_links = div.find_all('a', href=True)
        if hidden_links:
            print(f"         ìˆ¨ê²¨ì§„ ë§í¬: {len(hidden_links)}ê°œ")
            for link in hidden_links[:3]:
                print(f"            {link.get_text(strip=True)}: {link['href']}")
    
    print(f"\n9. ì‹¤ì œ í˜ì´ì§€ ì ‘ê·¼ í…ŒìŠ¤íŠ¸...")
    
    # ë°œê²¬í•œ ë§í¬ë“¤ ì‹¤ì œ ì ‘ê·¼í•´ë³´ê¸°
    test_links = [link[1] for link in interesting_links if link[1].startswith('/')]
    
    found_data_pages = []
    
    for link in test_links[:15]:  # ì²˜ìŒ 15ê°œë§Œ í…ŒìŠ¤íŠ¸
        try:
            resp = session.get(f"{BASE_URL}{link}", verify=False, timeout=10)
            print(f"\n   {link}: {resp.status_code}")
            
            if resp.status_code == 200:
                # í˜ì´ì§€ ë‚´ìš© ë¶„ì„
                page_soup = BeautifulSoup(resp.text, 'html.parser')
                
                # IP íŒ¨í„´ ì°¾ê¸°
                page_text = page_soup.get_text()
                ips = re.findall(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', page_text)
                real_ips = [ip for ip in ips if not ip.startswith(('127.', '192.168.', '10.', '172.'))]
                
                if real_ips:
                    print(f"      âœ… IP ë°œê²¬: {len(real_ips)}ê°œ")
                    print(f"      ìƒ˜í”Œ: {real_ips[:5]}")
                    found_data_pages.append((link, real_ips))
                
                # í…Œì´ë¸”ì´ë‚˜ ë¦¬ìŠ¤íŠ¸ í™•ì¸
                page_tables = page_soup.find_all('table')
                page_lists = page_soup.find_all(['ul', 'ol'])
                
                if page_tables:
                    print(f"      í…Œì´ë¸”: {len(page_tables)}ê°œ")
                if page_lists:
                    print(f"      ë¦¬ìŠ¤íŠ¸: {len(page_lists)}ê°œ")
                
                # ë‹¤ìš´ë¡œë“œ ë§í¬ í™•ì¸
                download_links = page_soup.find_all('a', href=re.compile(r'\.(xlsx?|csv|json|xml)$', re.I))
                if download_links:
                    print(f"      ë‹¤ìš´ë¡œë“œ ë§í¬: {len(download_links)}ê°œ")
                    for dl in download_links[:3]:
                        print(f"         {dl.get_text(strip=True)}: {dl['href']}")
                
        except Exception as e:
            print(f"   {link}: ì˜¤ë¥˜ - {e}")
    
    print(f"\n10. AJAX ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸...")
    
    # ë°œê²¬í•œ AJAX URLë“¤ í…ŒìŠ¤íŠ¸
    unique_ajax_urls = list(set(ajax_urls))
    
    for ajax_url in unique_ajax_urls[:10]:  # ì²˜ìŒ 10ê°œë§Œ
        if ajax_url.startswith('/'):
            try:
                # AJAX ìŠ¤íƒ€ì¼ í—¤ë”ë¡œ ìš”ì²­
                session.headers.update({
                    'X-Requested-With': 'XMLHttpRequest',
                    'Accept': 'application/json, text/javascript, */*; q=0.01'
                })
                
                resp = session.get(f"{BASE_URL}{ajax_url}", verify=False, timeout=10)
                print(f"\n   AJAX {ajax_url}: {resp.status_code}")
                
                if resp.status_code == 200:
                    content_type = resp.headers.get('Content-Type', '')
                    print(f"      Content-Type: {content_type}")
                    
                    if 'json' in content_type:
                        try:
                            data = resp.json()
                            print(f"      JSON: {type(data)}")
                            
                            if isinstance(data, list) and len(data) > 0:
                                print(f"      ë°ì´í„° ê°œìˆ˜: {len(data)}")
                                print(f"      ì²« ë²ˆì§¸: {data[0]}")
                            elif isinstance(data, dict):
                                print(f"      í‚¤: {list(data.keys())}")
                                
                        except ValueError:
                            print(f"      JSON íŒŒì‹± ì‹¤íŒ¨")
                    else:
                        print(f"      ì‘ë‹µ ê¸¸ì´: {len(resp.text)}")
                        if len(resp.text) < 500:
                            print(f"      ë‚´ìš©: {resp.text}")
                            
            except Exception as e:
                print(f"   AJAX {ajax_url}: ì˜¤ë¥˜ - {e}")
    
    print(f"\n=== ë¶„ì„ ê²°ê³¼ ìš”ì•½ ===")
    print(f"âœ… ë§í¬: {len(interesting_links)}ê°œ ë°œê²¬")
    print(f"âœ… ë„¤ë¹„ê²Œì´ì…˜: {len(nav_elements)}ê°œ ë°œê²¬") 
    print(f"âœ… í¼: {len(forms)}ê°œ ë°œê²¬")
    print(f"âœ… í…Œì´ë¸”: {len(tables)}ê°œ ë°œê²¬")
    print(f"âœ… AJAX URL: {len(unique_ajax_urls)}ê°œ ë°œê²¬")
    
    if found_data_pages:
        print(f"ğŸ‰ ë°ì´í„° í˜ì´ì§€: {len(found_data_pages)}ê°œ ë°œê²¬!")
        for page, ips in found_data_pages:
            print(f"   {page}: {len(ips)}ê°œ IP")
    else:
        print(f"âŒ ì‹¤ì œ ë°ì´í„° í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í•¨")
    
    return found_data_pages

if __name__ == "__main__":
    data_pages = analyze_secudium_with_bs4()
    
    if data_pages:
        print(f"\nğŸ¯ ì„±ê³µ! ì´ {len(data_pages)}ê°œ ë°ì´í„° í˜ì´ì§€ ë°œê²¬!")
        total_ips = sum(len(ips) for _, ips in data_pages)
        print(f"ì „ì²´ IP: {total_ips}ê°œ")
    else:
        print(f"\nğŸ’” ì‹¤ì œ SECUDIUM ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        print(f"ìˆ˜ë™ ë‹¤ìš´ë¡œë“œë‚˜ API í† í°ì´ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.")