#!/usr/bin/env python3
"""
SECUDIUM ì‹¤ì œ ë¡œê·¸ì¸ ì„±ê³µ í…ŒìŠ¤íŠ¸
ë¡œê·¸ì¸ í›„ ì‹¤ì œ ëŒ€ì‹œë³´ë“œ/ë©”ì¸ í˜ì´ì§€ ì ‘ê·¼ í™•ì¸
"""
import requests
from bs4 import BeautifulSoup
import re
import warnings
warnings.filterwarnings('ignore', message='Unverified HTTPS request')

BASE_URL = "https://secudium.skinfosec.co.kr"
USERNAME = "nextrade"
PASSWORD = "Sprtmxm1@3"

def test_real_login_success():
    """ì‹¤ì œ ë¡œê·¸ì¸ ì„±ê³µ ì—¬ë¶€ í™•ì¸"""
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    
    print("=== SECUDIUM ì‹¤ì œ ë¡œê·¸ì¸ í…ŒìŠ¤íŠ¸ ===")
    
    print("\n1. ë¡œê·¸ì¸ ì „ ìƒíƒœ í™•ì¸...")
    before_resp = session.get(BASE_URL, verify=False)
    before_soup = BeautifulSoup(before_resp.text, 'html.parser')
    before_title = before_soup.title.string if before_soup.title else 'None'
    print(f"   ë¡œê·¸ì¸ ì „ ì œëª©: {before_title}")
    print(f"   ë¡œê·¸ì¸ ì „ URL: {before_resp.url}")
    print(f"   ë¡œê·¸ì¸ ì „ ì¿ í‚¤: {dict(session.cookies)}")
    
    print("\n2. ë‹¤ì–‘í•œ ë¡œê·¸ì¸ ë°©ë²• ì‹œë„...")
    
    login_methods = [
        # ë°©ë²• 1: ê¸°ì¡´ GET ë°©ì‹
        {
            'name': 'GET ë°©ì‹ (ê¸°ì¡´)',
            'method': 'GET',
            'url': f"{BASE_URL}/login",
            'data': {
                'lang': 'ko',
                'is_otp': 'N',
                'is_expire': 'N',
                'login_name': USERNAME,
                'password': PASSWORD,
                'otp_value': ''
            }
        },
        # ë°©ë²• 2: ë©”ì¸ í˜ì´ì§€ì— POST
        {
            'name': 'POST to main page',
            'method': 'POST',
            'url': BASE_URL,
            'data': {
                'lang': 'ko',
                'is_otp': 'N',
                'is_expire': 'N',
                'login_name': USERNAME,
                'password': PASSWORD,
                'otp_value': ''
            }
        },
        # ë°©ë²• 3: ë¡œê·¸ì¸ í¼ action í™•ì¸ í›„ POST
        {
            'name': 'POST to form action',
            'method': 'POST',
            'url': f"{BASE_URL}/loginProcess",
            'data': {
                'lang': 'ko',
                'is_otp': 'N',
                'is_expire': 'N',
                'login_name': USERNAME,
                'password': PASSWORD,
                'otp_value': ''
            }
        }
    ]
    
    successful_login = None
    
    for method_info in login_methods:
        print(f"\n   ì‹œë„: {method_info['name']}")
        
        try:
            # ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ì‹œì‘
            test_session = requests.Session()
            test_session.headers.update(session.headers)
            
            # ë©”ì¸ í˜ì´ì§€ ì ‘ì†ìœ¼ë¡œ ì´ˆê¸°í™”
            test_session.get(BASE_URL, verify=False)
            
            if method_info['method'] == 'GET':
                resp = test_session.get(method_info['url'], params=method_info['data'], 
                                      verify=False, allow_redirects=True)
            else:
                resp = test_session.post(method_info['url'], data=method_info['data'], 
                                       verify=False, allow_redirects=True)
            
            print(f"      ì‘ë‹µ: {resp.status_code}")
            print(f"      ìµœì¢… URL: {resp.url}")
            print(f"      ì¿ í‚¤: {dict(test_session.cookies)}")
            
            # ë¡œê·¸ì¸ í›„ í˜ì´ì§€ í™•ì¸
            soup = BeautifulSoup(resp.text, 'html.parser')
            title = soup.title.string if soup.title else 'None'
            print(f"      ì œëª©: {title}")
            
            # ë¡œê·¸ì¸ ì„±ê³µ ì—¬ë¶€ íŒë‹¨
            if title != '::: Login :::' and 'login' not in title.lower():
                print(f"      âœ… ë¡œê·¸ì¸ ì„±ê³µ!")
                successful_login = (method_info, test_session, resp)
                break
            else:
                print(f"      âŒ ì—¬ì „íˆ ë¡œê·¸ì¸ í˜ì´ì§€")
                
        except Exception as e:
            print(f"      ì˜¤ë¥˜: {e}")
    
    if not successful_login:
        print("\nâŒ ëª¨ë“  ë¡œê·¸ì¸ ë°©ë²• ì‹¤íŒ¨")
        return None
    
    print(f"\n3. ì„±ê³µí•œ ë¡œê·¸ì¸ìœ¼ë¡œ í˜ì´ì§€ íƒìƒ‰...")
    
    method_info, login_session, login_resp = successful_login
    
    # ë¡œê·¸ì¸ ì„±ê³µ í›„ ë‹¤ì–‘í•œ í˜ì´ì§€ ì ‘ê·¼
    test_pages = [
        '/',
        '/main',
        '/dashboard',
        '/home',
        '/index',
        '/menu',
        '/secinfo',
        '/data',
        '/report'
    ]
    
    accessible_pages = []
    
    for page in test_pages:
        try:
            resp = login_session.get(f"{BASE_URL}{page}", verify=False)
            print(f"\n   {page}: {resp.status_code}")
            
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.text, 'html.parser')
                title = soup.title.string if soup.title else 'None'
                print(f"      ì œëª©: {title}")
                
                # ë¡œê·¸ì¸ í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°
                if 'login' not in title.lower():
                    accessible_pages.append((page, resp))
                    print(f"      âœ… ì ‘ê·¼ ê°€ëŠ¥í•œ í˜ì´ì§€!")
                    
                    # í˜ì´ì§€ ë‚´ìš© ë¶„ì„
                    analyze_authenticated_page(page, resp, login_session)
                else:
                    print(f"      âŒ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨")
            
        except Exception as e:
            print(f"   {page} ì˜¤ë¥˜: {e}")
    
    return accessible_pages

def analyze_authenticated_page(page_name, response, session):
    """ì¸ì¦ëœ í˜ì´ì§€ ìƒì„¸ ë¶„ì„"""
    print(f"\n      ğŸ“‹ {page_name} í˜ì´ì§€ ë¶„ì„:")
    
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # ë§í¬ ë¶„ì„
    links = soup.find_all('a', href=True)
    data_links = []
    
    for link in links:
        href = link['href']
        text = link.get_text(strip=True)
        
        keywords = ['ë°ì´í„°', 'ë¦¬í¬íŠ¸', 'ë¸”ë™ë¦¬ìŠ¤íŠ¸', 'ìœ„í˜‘', 'ë³´ì•ˆ', 
                   'data', 'report', 'blacklist', 'threat', 'security',
                   'list', 'export', 'download']
        
        if any(keyword in text.lower() for keyword in keywords) or \
           any(keyword in href.lower() for keyword in keywords):
            data_links.append((text, href))
    
    if data_links:
        print(f"         ë°ì´í„° ê´€ë ¨ ë§í¬: {len(data_links)}ê°œ")
        for text, href in data_links[:5]:
            print(f"            {text}: {href}")
    
    # í…Œì´ë¸” ë¶„ì„
    tables = soup.find_all('table')
    if tables:
        print(f"         í…Œì´ë¸”: {len(tables)}ê°œ")
        for i, table in enumerate(tables):
            rows = table.find_all('tr')
            if len(rows) > 1:  # í—¤ë” + ë°ì´í„°
                print(f"            í…Œì´ë¸” {i+1}: {len(rows)}í–‰")
                
                # ì²« ë²ˆì§¸ ë°ì´í„° í–‰ í™•ì¸
                first_data_row = rows[1] if len(rows) > 1 else None
                if first_data_row:
                    cells = [td.get_text(strip=True) for td in first_data_row.find_all('td')]
                    print(f"               ì²« ë²ˆì§¸ í–‰: {cells}")
                    
                    # IP íŒ¨í„´ ì²´í¬
                    for cell in cells:
                        if re.match(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', cell):
                            print(f"               ğŸ¯ IP ë°œê²¬: {cell}")
    
    # JavaScript ë¶„ì„
    scripts = soup.find_all('script')
    ajax_urls = []
    
    for script in scripts:
        if script.string:
            # API í˜¸ì¶œ íŒ¨í„´
            patterns = [
                r'url\s*:\s*["\']([^"\']+)["\']',
                r'fetch\s*\(["\']([^"\']+)["\']',
                r'api_host\s*=\s*["\']([^"\']+)["\']'
            ]
            
            for pattern in patterns:
                urls = re.findall(pattern, script.string)
                ajax_urls.extend(urls)
    
    if ajax_urls:
        print(f"         AJAX URL: {len(set(ajax_urls))}ê°œ")
        for url in list(set(ajax_urls))[:3]:
            print(f"            {url}")
    
    # ì‹¤ì œ ë°ì´í„° ë§í¬ í…ŒìŠ¤íŠ¸
    if data_links:
        print(f"         ë°ì´í„° ë§í¬ ì ‘ê·¼ í…ŒìŠ¤íŠ¸:")
        for text, href in data_links[:3]:  # ì²˜ìŒ 3ê°œë§Œ
            if href.startswith('/'):
                try:
                    test_resp = session.get(f"{BASE_URL}{href}", verify=False)
                    print(f"            {href}: {test_resp.status_code}")
                    
                    if test_resp.status_code == 200:
                        # IP íŒ¨í„´ ì°¾ê¸°
                        ips = re.findall(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', test_resp.text)
                        real_ips = [ip for ip in ips if not ip.startswith(('127.', '192.168.', '10.', '172.'))]
                        
                        if real_ips:
                            print(f"               ğŸ‰ ì‹¤ì œ IP {len(real_ips)}ê°œ ë°œê²¬!")
                            print(f"               ìƒ˜í”Œ: {real_ips[:5]}")
                except:
                    pass

if __name__ == "__main__":
    accessible_pages = test_real_login_success()
    
    if accessible_pages:
        print(f"\nğŸ‰ ì„±ê³µ! {len(accessible_pages)}ê°œ ì¸ì¦ëœ í˜ì´ì§€ ì ‘ê·¼ ê°€ëŠ¥!")
        for page, _ in accessible_pages:
            print(f"   âœ… {page}")
    else:
        print(f"\nğŸ’” ì‹¤ì œ ë¡œê·¸ì¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        print(f"SECUDIUM ê³„ì • ìƒíƒœë‚˜ ì¸ì¦ ë°©ì‹ì„ í™•ì¸í•´ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.")