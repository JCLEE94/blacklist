#!/usr/bin/env python3
"""
__init__ íŒ¨í„´ ë¶„ì„ ì „ìš© ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ë¦¬íŒ©í† ë§ ì—†ì´ íŒ¨í„´ë§Œ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ìƒì„±

Sample input: Python ì†ŒìŠ¤ íŒŒì¼ë“¤
Expected output: ì¤‘ë³µ íŒ¨í„´ ë¶„ì„ ë³´ê³ ì„œ
"""

import re
import logging
from pathlib import Path
from typing import Dict, List, Set, Tuple
from collections import Counter, defaultdict

logger = logging.getLogger(__name__)


class InitPatternAnalyzer:
    """__init__ íŒ¨í„´ ë¶„ì„ê¸°"""

    def __init__(self):
        self.patterns = {
            "db_path_assignment": r"self\.db_path\s*=\s*db_path",
            "service_name_assignment": r"self\.service_name\s*=\s*service_name",
            "logger_creation": r"self\.logger\s*=\s*logging\.getLogger",
            "timestamp_creation": r"self\.(?:created_at|updated_at)\s*=\s*datetime\.now",
            "config_assignment": r"self\.\w+\s*=\s*\w+",
            "sqlite_connection": r"sqlite3\.connect",
            "super_init_call": r"super\(\)\.__init__",
            "flask_app_creation": r"Flask\(__name__\)",
            "thread_lock_creation": r"threading\.Lock\(\)",
            "default_parameter": r"def __init__\(.*=.*\)",
        }

        self.pattern_categories = {
            "database": ["db_path_assignment", "sqlite_connection"],
            "service": ["service_name_assignment", "logger_creation"],
            "timestamp": ["timestamp_creation"],
            "configuration": ["config_assignment"],
            "inheritance": ["super_init_call"],
            "web": ["flask_app_creation"],
            "threading": ["thread_lock_creation"],
            "parameters": ["default_parameter"],
        }

    def analyze_file(self, file_path: Path) -> Dict[str, List[str]]:
        """íŒŒì¼ì—ì„œ __init__ íŒ¨í„´ì„ ë¶„ì„"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()

            # __init__ ë©”ì†Œë“œ ì°¾ê¸°
            init_pattern = r"def __init__\(.*?\):(.*?)(?=def |\nclass |\Z)"
            init_methods = re.findall(init_pattern, content, re.DOTALL)

            found_patterns = {}
            for pattern_name, pattern_regex in self.patterns.items():
                matches = []
                for init_content in init_methods:
                    if re.search(pattern_regex, init_content):
                        # ë§¤ì¹˜ëœ ì¤„ë§Œ ì¶”ì¶œ (ê°„ë‹¨íˆ)
                        lines = init_content.split("\n")
                        matched_lines = [
                            line.strip()
                            for line in lines
                            if re.search(pattern_regex, line)
                        ]
                        matches.extend(matched_lines)

                if matches:
                    found_patterns[pattern_name] = matches

            return found_patterns

        except Exception as e:
            logger.error(f"Error analyzing {file_path}: {e}")
            return {}

    def get_pattern_category(self, pattern_name: str) -> str:
        """íŒ¨í„´ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ë°˜í™˜"""
        for category, patterns in self.pattern_categories.items():
            if pattern_name in patterns:
                return category
        return "other"


def analyze_codebase(src_path: Path) -> Dict[str, any]:
    """ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„"""
    analyzer = InitPatternAnalyzer()

    # Python íŒŒì¼ë“¤ ì°¾ê¸°
    python_files = list(src_path.rglob("*.py"))

    # ë¶„ì„ ê²°ê³¼ ì €ì¥
    all_patterns = defaultdict(list)
    file_pattern_map = {}
    pattern_frequency = Counter()
    category_frequency = Counter()

    print(f"ğŸ” ë¶„ì„ ì¤‘: {len(python_files)}ê°œ Python íŒŒì¼...")

    for file_path in python_files:
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ê³¼ íŠ¹ì • íŒŒì¼ë“¤ ì œì™¸
        if any(
            exclude in str(file_path) for exclude in ["test_", "__pycache__", ".pyc"]
        ):
            continue

        patterns = analyzer.analyze_file(file_path)
        if patterns:
            relative_path = file_path.relative_to(src_path)
            file_pattern_map[str(relative_path)] = patterns

            for pattern_name, matches in patterns.items():
                all_patterns[pattern_name].extend(matches)
                pattern_frequency[pattern_name] += len(matches)

                category = analyzer.get_pattern_category(pattern_name)
                category_frequency[category] += len(matches)

    return {
        "total_files": len(python_files),
        "files_with_patterns": len(file_pattern_map),
        "file_pattern_map": file_pattern_map,
        "all_patterns": dict(all_patterns),
        "pattern_frequency": dict(pattern_frequency),
        "category_frequency": dict(category_frequency),
        "analyzer": analyzer,
    }


def generate_report(analysis_results: Dict[str, any]) -> str:
    """ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
    report_lines = []

    report_lines.append("=" * 80)
    report_lines.append("ğŸ“Š __init__ íŒ¨í„´ ë¶„ì„ ë³´ê³ ì„œ")
    report_lines.append("=" * 80)
    report_lines.append("")

    # ì „ì²´ í†µê³„
    report_lines.append("ğŸ“ˆ ì „ì²´ í†µê³„:")
    report_lines.append(f"  - ì „ì²´ Python íŒŒì¼: {analysis_results['total_files']}ê°œ")
    report_lines.append(f"  - íŒ¨í„´ì´ ë°œê²¬ëœ íŒŒì¼: {analysis_results['files_with_patterns']}ê°œ")
    report_lines.append(
        f"  - ë¶„ì„ ë¹„ìœ¨: {analysis_results['files_with_patterns']/analysis_results['total_files']*100:.1f}%"
    )
    report_lines.append("")

    # ì¹´í…Œê³ ë¦¬ë³„ ë¹ˆë„
    report_lines.append("ğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ íŒ¨í„´ ë¹ˆë„:")
    for category, count in sorted(
        analysis_results["category_frequency"].items(), key=lambda x: x[1], reverse=True
    ):
        report_lines.append(f"  - {category:12}: {count:3}ê°œ")
    report_lines.append("")

    # íŒ¨í„´ë³„ ìƒì„¸ ë¹ˆë„
    report_lines.append("ğŸ” íŒ¨í„´ë³„ ìƒì„¸ ë¹ˆë„:")
    for pattern, count in sorted(
        analysis_results["pattern_frequency"].items(), key=lambda x: x[1], reverse=True
    ):
        category = analysis_results["analyzer"].get_pattern_category(pattern)
        report_lines.append(f"  - {pattern:25} ({category:10}): {count:3}ê°œ")
    report_lines.append("")

    # ì¤‘ë³µë„ê°€ ë†’ì€ íŒŒì¼ë“¤
    report_lines.append("ğŸ¯ ì¤‘ë³µë„ê°€ ë†’ì€ íŒŒì¼ë“¤ (3ê°œ ì´ìƒ íŒ¨í„´):")
    high_duplicate_files = []
    for file_path, patterns in analysis_results["file_pattern_map"].items():
        if len(patterns) >= 3:
            total_occurrences = sum(len(matches) for matches in patterns.values())
            high_duplicate_files.append((file_path, len(patterns), total_occurrences))

    high_duplicate_files.sort(key=lambda x: x[2], reverse=True)

    for file_path, pattern_count, total_occurrences in high_duplicate_files[:10]:
        report_lines.append(
            f"  - {file_path:50} ({pattern_count}ê°œ íŒ¨í„´, {total_occurrences}íšŒ ì¶œí˜„)"
        )
    report_lines.append("")

    # ë¦¬íŒ©í† ë§ ê¶Œì¥ì‚¬í•­
    report_lines.append("ğŸ’¡ ë¦¬íŒ©í† ë§ ê¶Œì¥ì‚¬í•­:")

    db_pattern_count = analysis_results["category_frequency"].get("database", 0)
    service_pattern_count = analysis_results["category_frequency"].get("service", 0)
    timestamp_pattern_count = analysis_results["category_frequency"].get("timestamp", 0)

    if db_pattern_count > 10:
        report_lines.append(f"  âœ… DatabaseMixin ë„ì… ê¶Œì¥ (í˜„ì¬ {db_pattern_count}ê°œ ì¤‘ë³µ)")

    if service_pattern_count > 15:
        report_lines.append(
            f"  âœ… BaseService í´ë˜ìŠ¤ ë„ì… ê¶Œì¥ (í˜„ì¬ {service_pattern_count}ê°œ ì¤‘ë³µ)"
        )

    if timestamp_pattern_count > 5:
        report_lines.append(
            f"  âœ… TimestampMixin ë„ì… ê¶Œì¥ (í˜„ì¬ {timestamp_pattern_count}ê°œ ì¤‘ë³µ)"
        )

    if len(high_duplicate_files) > 0:
        report_lines.append(f"  âœ… {len(high_duplicate_files)}ê°œ íŒŒì¼ì´ ì¤‘ë³µ ë¦¬íŒ©í† ë§ ëŒ€ìƒ")

    report_lines.append("")
    report_lines.append("=" * 80)

    return "\n".join(report_lines)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    src_path = Path("/home/jclee/app/blacklist/src")

    if not src_path.exists():
        print(f"âŒ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {src_path}")
        return

    # ë¶„ì„ ì‹¤í–‰
    print("ğŸš€ __init__ íŒ¨í„´ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    analysis_results = analyze_codebase(src_path)

    # ë³´ê³ ì„œ ìƒì„±
    report = generate_report(analysis_results)

    # ë³´ê³ ì„œ ì¶œë ¥
    print(report)

    # ë³´ê³ ì„œ íŒŒì¼ë¡œ ì €ì¥
    report_path = Path("CODE_QUALITY_INIT_PATTERNS_REPORT.md")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(report)

    print(f"ğŸ“„ ìƒì„¸ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_path}")


if __name__ == "__main__":
    import sys

    all_validation_failures = []
    total_tests = 0

    # Test 1: Pattern definitions
    total_tests += 1
    try:
        analyzer = InitPatternAnalyzer()
        if len(analyzer.patterns) < 5:
            all_validation_failures.append(
                "Pattern test: Insufficient patterns defined"
            )
    except Exception as e:
        all_validation_failures.append(f"Pattern test: Failed to create analyzer - {e}")

    # Test 2: Category mapping
    total_tests += 1
    try:
        analyzer = InitPatternAnalyzer()
        category = analyzer.get_pattern_category("db_path_assignment")
        if category != "database":
            all_validation_failures.append("Category test: Wrong category mapping")
    except Exception as e:
        all_validation_failures.append(f"Category test: Failed - {e}")

    # Test 3: Source path existence
    total_tests += 1
    try:
        src_path = Path("/home/jclee/app/blacklist/src")
        if not src_path.exists():
            all_validation_failures.append("Path test: Source directory does not exist")
    except Exception as e:
        all_validation_failures.append(f"Path test: Failed - {e}")

    # Final validation result
    if all_validation_failures:
        print(
            f"âŒ VALIDATION FAILED - {len(all_validation_failures)} of {total_tests} tests failed:"
        )
        for failure in all_validation_failures:
            print(f"  - {failure}")
        sys.exit(1)
    else:
        print(
            f"âœ… VALIDATION PASSED - All {total_tests} tests produced expected results"
        )
        print("Pattern analyzer is validated and ready to run")
        main()
        sys.exit(0)
