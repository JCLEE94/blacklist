#!/usr/bin/env python3
"""
REGTECH ì‘ë‹µ ë¶„ì„ - JavaScript ì½”ë“œ í™•ì¸
"""
import requests
from bs4 import BeautifulSoup
import re
import json

def analyze_regtech_response():
    """REGTECH ì‘ë‹µì˜ JavaScript ì½”ë“œ ë¶„ì„"""
    print("ğŸ” REGTECH ì‘ë‹µ ë¶„ì„")
    
    base_url = "https://regtech.fsec.or.kr"
    bearer_token = "BearereyJ0eXAiOiJKV1QiLCJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJuZXh0cmFkZSIsIm9yZ2FubmFtZSI6IuuEpeyKpO2KuOugiIzydtOuTnCIsImlkIjoibmV4dHJhZGUiLCJleHAiOjE3NTExMTkyNzYsInVzZXJuYW1lIjoi7J6l7ZmN7KSAIn0.YwZHoHZCVqDnaryluB0h5_ituxYcaRz4voT7GRfgrNrP86W8TfvBuJbHMON4tJa4AQmNP-XhC_PuAVPQTjJADA"
    
    session = requests.Session()
    session.cookies.set('regtech-va', bearer_token, domain='regtech.fsec.or.kr', path='/')
    
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
    })
    
    try:
        # POST ìš”ì²­
        form_data = {
            'page': '0',
            'tabSort': 'blacklist',
            'startDate': '20250301',
            'endDate': '20250627',
            'size': '100'
        }
        
        response = session.post(
            f"{base_url}/fcti/securityAdvisory/advisoryList",
            data=form_data,
            timeout=30
        )
        
        print(f"ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        print(f"ì‘ë‹µ í¬ê¸°: {len(response.text)} bytes")
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. JavaScript í•¨ìˆ˜ ì°¾ê¸°
        scripts = soup.find_all('script')
        print(f"\nğŸ“œ ë°œê²¬ëœ ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸: {len(scripts)}ê°œ")
        
        for i, script in enumerate(scripts):
            if script.string:
                content = script.string.strip()
                if len(content) > 50:  # ì˜ë¯¸ìˆëŠ” ìŠ¤í¬ë¦½íŠ¸ë§Œ
                    print(f"\n--- ìŠ¤í¬ë¦½íŠ¸ {i+1} ---")
                    
                    # AJAX í˜¸ì¶œ íŒ¨í„´ ì°¾ê¸°
                    ajax_patterns = [
                        r'\.ajax\s*\(',
                        r'fetch\s*\(',
                        r'XMLHttpRequest',
                        r'advisoryList',
                        r'blacklist',
                        r'loadData',
                        r'getData',
                        r'pageMove',
                        r'search'
                    ]
                    
                    for pattern in ajax_patterns:
                        if re.search(pattern, content, re.IGNORECASE):
                            print(f"âœ… '{pattern}' íŒ¨í„´ ë°œê²¬")
                            
                            # í•´ë‹¹ ë¶€ë¶„ ì¶”ì¶œ
                            matches = re.finditer(pattern, content, re.IGNORECASE)
                            for match in matches:
                                start = max(0, match.start() - 100)
                                end = min(len(content), match.end() + 200)
                                context = content[start:end]
                                print(f"ì»¨í…ìŠ¤íŠ¸: ...{context}...")
        
        # 2. ìˆ¨ê²¨ì§„ ì…ë ¥ í•„ë“œ ì°¾ê¸°
        hidden_inputs = soup.find_all('input', type='hidden')
        print(f"\nğŸ” ìˆ¨ê²¨ì§„ ì…ë ¥ í•„ë“œ: {len(hidden_inputs)}ê°œ")
        for inp in hidden_inputs:
            print(f"  - {inp.get('name', 'N/A')}: {inp.get('value', 'N/A')}")
        
        # 3. ë°ì´í„° í…Œì´ë¸” ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        data_containers = soup.find_all(['div', 'table'], id=re.compile(r'(list|data|result|blacklist)', re.I))
        print(f"\nğŸ“Š ë°ì´í„° ì»¨í…Œì´ë„ˆ í›„ë³´: {len(data_containers)}ê°œ")
        for container in data_containers:
            print(f"  - {container.name} id={container.get('id', 'N/A')} class={container.get('class', 'N/A')}")
        
        # 4. AJAX URL íŒ¨í„´ ì°¾ê¸°
        url_pattern = r'["\']([^"\']*(?:ajax|api|data|list|blacklist)[^"\']*)["\']'
        urls = re.findall(url_pattern, response.text, re.IGNORECASE)
        unique_urls = list(set(urls))
        
        print(f"\nğŸŒ ë°œê²¬ëœ URL íŒ¨í„´: {len(unique_urls)}ê°œ")
        for url in unique_urls[:10]:  # ì²˜ìŒ 10ê°œë§Œ
            if not url.startswith('http') and not url.startswith('#'):
                print(f"  - {url}")
        
        # 5. í¼ ì•¡ì…˜ í™•ì¸
        forms = soup.find_all('form')
        print(f"\nğŸ“ í¼: {len(forms)}ê°œ")
        for form in forms:
            print(f"  - action={form.get('action', 'N/A')} method={form.get('method', 'N/A')}")
        
        # 6. ë°ì´í„°ê°€ ë¡œë“œë  ê°€ëŠ¥ì„±ì´ ìˆëŠ” ìš”ì†Œ
        data_elements = soup.find_all(['tbody', 'ul', 'div'], class_=re.compile(r'(list|data|content|result)', re.I))
        print(f"\nğŸ¯ ë°ì´í„° ë¡œë“œ ëŒ€ìƒ ìš”ì†Œ: {len(data_elements)}ê°œ")
        for elem in data_elements[:5]:
            print(f"  - {elem.name} class={elem.get('class', 'N/A')} children={len(elem.find_all())}")
        
        # ì‘ë‹µ ì¼ë¶€ ì €ì¥ (ë””ë²„ê¹…ìš©)
        with open('regtech_response.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print("\nğŸ’¾ ì‘ë‹µì´ regtech_response.htmlë¡œ ì €ì¥ë¨")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    analyze_regtech_response()