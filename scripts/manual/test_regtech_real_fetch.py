#!/usr/bin/env python3
"""
REGTECH ì‹¤ì œ ë¸Œë¼ìš°ì € fetch ìš”ì²­ ì¬í˜„
"""
import requests
from datetime import datetime, timedelta
import re
from bs4 import BeautifulSoup

def test_real_fetch():
    """ë¸Œë¼ìš°ì €ì—ì„œ ìº¡ì²˜í•œ ì‹¤ì œ fetch ìš”ì²­ ì¬í˜„"""
    print("ğŸ§ª REGTECH ì‹¤ì œ fetch ìš”ì²­ í…ŒìŠ¤íŠ¸")
    
    # Bearer Token
    bearer_token = "BearereyJ0eXAiOiJKV1QiLCJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJuZXh0cmFkZSIsIm9yZ2FubmFtZSI6IuuEpeyKpO2KuOugiIydtOuTnCIsImlkIjoibmV4dHJhZGUiLCJleHAiOjE3NTExMTkyNzYsInVzZXJuYW1lIjoi7J6l7ZmN7KSAIn0.YwZHoHZCVqDnaryluB0h5_ituxYcaRz4voT7GRfgrNrP86W8TfvBuJbHMON4tJa4AQmNP-XhC_PuAVPQTjJADA"
    
    # ì„¸ì…˜ ìƒì„±
    session = requests.Session()
    
    # Bearer Token ì¿ í‚¤ ì„¤ì •
    session.cookies.set('regtech-va', bearer_token, domain='regtech.fsec.or.kr', path='/')
    
    # ë¸Œë¼ìš°ì €ì™€ ë™ì¼í•œ í—¤ë” ì„¤ì •
    headers = {
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'cache-control': 'no-cache',
        'content-type': 'application/x-www-form-urlencoded',
        'pragma': 'no-cache',
        'sec-ch-ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
        'referer': 'https://regtech.fsec.or.kr/fcti/securityAdvisory/advisoryList'
    }
    
    # ë‚ ì§œ ì„¤ì •
    end_date = datetime.now()
    start_date = end_date - timedelta(days=90)
    start_date_str = start_date.strftime('%Y%m%d')
    end_date_str = end_date.strftime('%Y%m%d')
    
    # ë¸Œë¼ìš°ì €ì™€ ë™ì¼í•œ POST ë°ì´í„° (ë‚ ì§œ ì¶”ê°€)
    form_data = {
        'page': '0',
        'tabSort': 'blacklist',
        'excelDownload': '',
        'cveId': '',
        'ipId': '',
        'estId': '',
        'startDate': start_date_str,  # ë‚ ì§œ ì¶”ê°€
        'endDate': end_date_str,      # ë‚ ì§œ ì¶”ê°€
        'findCondition': 'all',
        'findKeyword': '',
        'excelDown': ['security', 'blacklist', 'weakpoint'],  # ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
        'size': '100'  # ë” ë§ì€ ë°ì´í„° ìš”ì²­
    }
    
    try:
        print(f"ë‚ ì§œ ë²”ìœ„: {start_date_str} ~ {end_date_str}")
        print("ìš”ì²­ ë³´ë‚´ëŠ” ì¤‘...")
        
        # POST ìš”ì²­
        response = session.post(
            'https://regtech.fsec.or.kr/fcti/securityAdvisory/advisoryList',
            headers=headers,
            data=form_data,
            timeout=30
        )
        
        print(f"ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        print(f"ì‘ë‹µ í¬ê¸°: {len(response.text)} bytes")
        
        if response.status_code == 200:
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # IP íŒ¨í„´ ê²€ìƒ‰
            ip_pattern = r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b'
            
            # 1. í…Œì´ë¸” ê²€ìƒ‰
            tables = soup.find_all('table')
            print(f"\në°œê²¬ëœ í…Œì´ë¸”: {len(tables)}ê°œ")
            
            ip_list = []
            
            for i, table in enumerate(tables):
                # í…Œì´ë¸” í´ë˜ìŠ¤ë‚˜ ID í™•ì¸
                table_attrs = f"class={table.get('class', 'N/A')} id={table.get('id', 'N/A')}"
                print(f"í…Œì´ë¸” {i+1}: {table_attrs}")
                
                rows = table.find_all('tr')
                print(f"  í–‰ ìˆ˜: {len(rows)}")
                
                # í—¤ë” í™•ì¸
                headers = table.find_all('th')
                if headers:
                    header_text = [h.get_text(strip=True) for h in headers[:5]]
                    print(f"  í—¤ë”: {header_text}")
                
                # ë°ì´í„° í–‰ ì²˜ë¦¬
                for row in rows[1:]:  # í—¤ë” ì œì™¸
                    cells = row.find_all(['td', 'th'])
                    if cells:
                        row_text = ' '.join([cell.get_text(strip=True) for cell in cells])
                        
                        # IP ì°¾ê¸°
                        ips = re.findall(ip_pattern, row_text)
                        for ip in ips:
                            # ìœ íš¨í•œ ê³µì¸ IPë§Œ
                            parts = ip.split('.')
                            if (all(0 <= int(part) <= 255 for part in parts) and
                                not ip.startswith('192.168.') and
                                not ip.startswith('10.') and
                                not ip.startswith('172.') and
                                not ip.startswith('127.') and
                                not ip.startswith('0.') and
                                ip not in ip_list):
                                ip_list.append(ip)
                                print(f"    IP ë°œê²¬: {ip}")
            
            # 2. ë°ì´í„° ì˜ì—­ ê²€ìƒ‰
            data_divs = soup.find_all('div', class_=re.compile(r'(data|list|content|result)', re.I))
            print(f"\në°ì´í„° ì˜ì—­: {len(data_divs)}ê°œ")
            
            for div in data_divs[:3]:
                div_class = div.get('class', [])
                print(f"  {div_class}")
            
            # 3. ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ IP ê²€ìƒ‰
            all_text = soup.get_text()
            all_ips = re.findall(ip_pattern, all_text)
            
            for ip in all_ips:
                parts = ip.split('.')
                try:
                    if (all(0 <= int(part) <= 255 for part in parts) and
                        not ip.startswith('192.168.') and
                        not ip.startswith('10.') and
                        not ip.startswith('172.') and
                        not ip.startswith('127.') and
                        not ip.startswith('0.') and
                        ip not in ip_list):
                        ip_list.append(ip)
                except:
                    continue
            
            print(f"\nğŸ¯ ì´ ë°œê²¬ëœ IP: {len(ip_list)}ê°œ")
            
            if ip_list:
                print("ìƒ˜í”Œ IP:")
                for ip in ip_list[:10]:
                    print(f"  - {ip}")
                if len(ip_list) > 10:
                    print(f"  ... ê·¸ë¦¬ê³  {len(ip_list) - 10}ê°œ ë”")
                return True
            else:
                # ë””ë²„ê¹… ì •ë³´
                print("\nğŸ” ì¶”ê°€ ë¶„ì„:")
                
                # íŠ¹ì • í…ìŠ¤íŠ¸ íŒ¨í„´ ê²€ìƒ‰
                if 'ì´' in all_text and 'ê±´' in all_text:
                    # ì´ Nê±´ íŒ¨í„´ ì°¾ê¸°
                    total_pattern = r'ì´\s*(\d+)\s*ê±´'
                    matches = re.findall(total_pattern, all_text)
                    if matches:
                        print(f"ì´ ê±´ìˆ˜ í‘œì‹œ: {matches}")
                
                # í˜ì´ì§€ë„¤ì´ì…˜ í™•ì¸
                pagination = soup.find_all(class_=re.compile(r'(page|paging|pagination)', re.I))
                if pagination:
                    print(f"í˜ì´ì§€ë„¤ì´ì…˜ ë°œê²¬: {len(pagination)}ê°œ")
                
                # JavaScript í™•ì¸
                scripts = soup.find_all('script')
                js_keywords = ['ajax', 'load', 'fetch', 'blacklist', 'getData']
                for keyword in js_keywords:
                    for script in scripts:
                        if script.string and keyword in script.string:
                            print(f"JavaScript '{keyword}' í‚¤ì›Œë“œ ë°œê²¬")
                            break
                
                # ìˆ¨ê²¨ì§„ í•„ë“œ í™•ì¸
                hidden_inputs = soup.find_all('input', type='hidden')
                if hidden_inputs:
                    print(f"\nìˆ¨ê²¨ì§„ í•„ë“œ {len(hidden_inputs)}ê°œ:")
                    for inp in hidden_inputs[:5]:
                        print(f"  {inp.get('name', 'N/A')} = {inp.get('value', 'N/A')}")
                
                # ì‘ë‹µ ì¼ë¶€ ì €ì¥
                with open('regtech_fetch_response.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print("\nğŸ’¾ ì‘ë‹µì´ regtech_fetch_response.htmlë¡œ ì €ì¥ë¨")
                
                return False
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_real_fetch()
    if success:
        print("\nğŸ‰ ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ!")
    else:
        print("\nğŸ’¥ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ - JavaScript ë™ì  ë¡œë”©")