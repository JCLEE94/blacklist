#!/usr/bin/env python3
"""
í”„ë¡œì íŠ¸ ì •ë¦¬ ìë™í™” ìŠ¤í¬ë¦½íŠ¸
- ê¶Œí•œ ë¬¸ì œ ìˆëŠ” ë””ë ‰í† ë¦¬ ì²˜ë¦¬
- ë¶ˆí•„ìš”í•œ íŒŒì¼ ì •ë¦¬
- .gitignore ìµœì í™”
"""

import os
import shutil
import subprocess
from pathlib import Path
from typing import List, Tuple


class ProjectCleaner:
    """í”„ë¡œì íŠ¸ ì •ë¦¬ ìë™í™” í´ë˜ìŠ¤"""

    def __init__(self, project_root=None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.issues_found = []
        self.fixes_applied = []

    def clean_database_directories(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ê¶Œí•œ ë¬¸ì œ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
        print("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ê¶Œí•œ ë¬¸ì œ í•´ê²° ì¤‘...")

        problematic_dirs = [
            "test-postgresql-data/pgdata",
            "data/postgresql/pgdata",
            "config/redis-data/appendonlydir",
            "config/postgresql-data/pgdata",
            "docker/redis-data/appendonlydir",
        ]

        for dir_path in problematic_dirs:
            full_path = self.project_root / dir_path
            if full_path.exists():
                try:
                    # ê¶Œí•œ ìˆ˜ì • ì‹œë„
                    subprocess.run(
                        f"sudo chmod -R 755 {full_path}",
                        shell=True,
                        capture_output=True,
                    )
                    print(f"  âœ… {dir_path} ê¶Œí•œ ìˆ˜ì • ì™„ë£Œ")
                    self.fixes_applied.append(f"Fixed permissions for {dir_path}")
                except:
                    # ê¶Œí•œ ìˆ˜ì •ì´ ì•ˆ ë˜ë©´ .gitignoreì— ì¶”ê°€
                    self.add_to_gitignore(str(dir_path))
                    print(f"  ğŸš« {dir_path} .gitignoreì— ì¶”ê°€")
                    self.fixes_applied.append(f"Added {dir_path} to .gitignore")

    def add_to_gitignore(self, pattern: str):
        """íŒ¨í„´ì„ .gitignoreì— ì¶”ê°€"""
        gitignore_path = self.project_root / ".gitignore"

        # ê¸°ì¡´ .gitignore ì½ê¸°
        existing_patterns = set()
        if gitignore_path.exists():
            with open(gitignore_path, "r", encoding="utf-8") as f:
                existing_patterns = set(line.strip() for line in f if line.strip())

        # ìƒˆ íŒ¨í„´ ì¶”ê°€
        if pattern not in existing_patterns:
            with open(gitignore_path, "a", encoding="utf-8") as f:
                f.write(f"\n# Auto-added by project cleaner\n{pattern}\n")

    def cleanup_temporary_files(self):
        """ì„ì‹œ íŒŒì¼ ë° ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        print("ğŸ§¹ ì„ì‹œ íŒŒì¼ ë° ìºì‹œ ì •ë¦¬ ì¤‘...")

        cleanup_patterns = [
            "**/__pycache__",
            "**/*.pyc",
            "**/*.pyo",
            "**/.pytest_cache",
            "**/node_modules",
            "**/.coverage",
            "**/htmlcov*",
            "**/*.log",
            "**/.DS_Store",
        ]

        cleaned_count = 0
        for pattern in cleanup_patterns:
            for path in self.project_root.glob(pattern):
                if path.is_file():
                    try:
                        path.unlink()
                        cleaned_count += 1
                    except:
                        pass
                elif path.is_dir():
                    try:
                        shutil.rmtree(path)
                        cleaned_count += 1
                    except:
                        pass

        if cleaned_count > 0:
            print(f"  âœ… {cleaned_count}ê°œ ì„ì‹œ íŒŒì¼/ë””ë ‰í† ë¦¬ ì •ë¦¬ ì™„ë£Œ")
            self.fixes_applied.append(f"Cleaned {cleaned_count} temporary files")
        else:
            print("  âœ… ì •ë¦¬í•  ì„ì‹œ íŒŒì¼ ì—†ìŒ")

    def optimize_gitignore(self):
        """gitignore íŒŒì¼ ìµœì í™”"""
        print("ğŸ“ .gitignore ìµœì í™” ì¤‘...")

        essential_patterns = [
            "# Python",
            "__pycache__/",
            "*.pyc",
            "*.pyo",
            "*.egg-info/",
            ".pytest_cache/",
            ".coverage",
            "htmlcov*/",
            "",
            "# Database data",
            "*.db",
            "pgdata/",
            "appendonlydir/",
            "",
            "# Environment",
            ".env",
            ".env.local",
            ".venv/",
            "venv/",
            "",
            "# IDE",
            ".vscode/",
            ".idea/",
            "*.swp",
            "*.swo",
            "",
            "# OS",
            ".DS_Store",
            "Thumbs.db",
            "",
            "# Logs",
            "*.log",
            "logs/*.log",
        ]

        gitignore_path = self.project_root / ".gitignore"

        # ê¸°ì¡´ ë‚´ìš© ì½ê¸°
        existing_content = ""
        if gitignore_path.exists():
            with open(gitignore_path, "r", encoding="utf-8") as f:
                existing_content = f.read()

        # í•„ìˆ˜ íŒ¨í„´ë“¤ì´ ì—†ìœ¼ë©´ ì¶”ê°€
        missing_patterns = []
        for pattern in essential_patterns:
            if pattern.strip() and pattern not in existing_content:
                missing_patterns.append(pattern)

        if missing_patterns:
            with open(gitignore_path, "a", encoding="utf-8") as f:
                f.write(f"\n# Auto-optimized by project cleaner\n")
                for pattern in missing_patterns:
                    f.write(f"{pattern}\n")

            print(f"  âœ… .gitignoreì— {len(missing_patterns)}ê°œ íŒ¨í„´ ì¶”ê°€")
            self.fixes_applied.append(
                f"Added {len(missing_patterns)} patterns to .gitignore"
            )
        else:
            print("  âœ… .gitignore ì´ë¯¸ ìµœì í™”ë¨")

    def check_large_files(self) -> List[Tuple[str, int]]:
        """í° íŒŒì¼ë“¤ ì°¾ê¸° (10MB ì´ìƒ)"""
        print("ğŸ“ í° íŒŒì¼ ê²€ì‚¬ ì¤‘...")

        large_files = []
        for file_path in self.project_root.rglob("*"):
            if file_path.is_file():
                try:
                    size = file_path.stat().st_size
                    if size > 10 * 1024 * 1024:  # 10MB
                        large_files.append(
                            (str(file_path.relative_to(self.project_root)), size)
                        )
                except:
                    pass

        if large_files:
            print(f"  âš ï¸ {len(large_files)}ê°œ í° íŒŒì¼ ë°œê²¬:")
            for file_path, size in large_files[:5]:  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
                size_mb = size / (1024 * 1024)
                print(f"    - {file_path}: {size_mb:.1f}MB")
        else:
            print("  âœ… í° íŒŒì¼ ì—†ìŒ")

        return large_files

    def optimize_package_json_scripts(self):
        """package.json ìŠ¤í¬ë¦½íŠ¸ ìµœì í™”"""
        print("ğŸ“¦ package.json ìŠ¤í¬ë¦½íŠ¸ ìµœì í™” ì¤‘...")

        package_json_path = self.project_root / "package.json"
        if not package_json_path.exists():
            print("  âš ï¸ package.json ì—†ìŒ")
            return

        import json

        with open(package_json_path, "r", encoding="utf-8") as f:
            package_data = json.load(f)

        if "scripts" not in package_data:
            print("  âœ… scripts ì„¹ì…˜ ì—†ìŒ")
            return

        scripts = package_data["scripts"]
        original_count = len(scripts)

        # í•µì‹¬ ìŠ¤í¬ë¦½íŠ¸ë“¤ ì •ì˜
        essential_scripts = {
            "start": "python scripts/run_with_credentials.py",
            "test": "pytest tests/ -v",
            "build": "docker build -t blacklist:latest .",
            "lint": "python -m flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics",
            "format": "black src/ tests/ && isort src/ tests/",
            "clean": "find . -type d -name '__pycache__' -exec rm -rf {} + 2>/dev/null || true",
            "version": "python scripts/auto_version_manager.py",
        }

        # ì¤‘ë³µë˜ê±°ë‚˜ ë¶ˆí•„ìš”í•œ ìŠ¤í¬ë¦½íŠ¸ ì •ë¦¬
        optimized_scripts = {}
        for key, value in scripts.items():
            if key in essential_scripts:
                optimized_scripts[key] = essential_scripts[key]
            elif key in ["dev", "deploy", "docker:build", "docker:push", "docker:run"]:
                # ìœ ìš©í•œ ìŠ¤í¬ë¦½íŠ¸ë“¤ì€ ìœ ì§€
                optimized_scripts[key] = value

        # í•„ìˆ˜ ìŠ¤í¬ë¦½íŠ¸ ì¶”ê°€
        for key, value in essential_scripts.items():
            if key not in optimized_scripts:
                optimized_scripts[key] = value

        if len(optimized_scripts) != original_count:
            package_data["scripts"] = optimized_scripts
            with open(package_json_path, "w", encoding="utf-8") as f:
                json.dump(package_data, f, indent=2, ensure_ascii=False)

            print(f"  âœ… ìŠ¤í¬ë¦½íŠ¸ ìµœì í™”: {original_count}ê°œ â†’ {len(optimized_scripts)}ê°œ")
            self.fixes_applied.append(
                f"Optimized package.json scripts: {original_count} â†’ {len(optimized_scripts)}"
            )
        else:
            print("  âœ… ìŠ¤í¬ë¦½íŠ¸ ì´ë¯¸ ìµœì í™”ë¨")

    def run_all_cleanups(self):
        """ëª¨ë“  ì •ë¦¬ ì‘ì—… ì‹¤í–‰"""
        print("ğŸš€ í”„ë¡œì íŠ¸ ì •ë¦¬ ìë™í™” ì‹œì‘")
        print("=" * 50)

        self.clean_database_directories()
        self.cleanup_temporary_files()
        self.optimize_gitignore()
        large_files = self.check_large_files()
        self.optimize_package_json_scripts()

        print("=" * 50)
        print("âœ… í”„ë¡œì íŠ¸ ì •ë¦¬ ì™„ë£Œ")
        print(f"ğŸ“Š ì ìš©ëœ ìˆ˜ì •ì‚¬í•­: {len(self.fixes_applied)}ê°œ")

        for fix in self.fixes_applied:
            print(f"  - {fix}")

        if large_files:
            print(f"\nâš ï¸ ì£¼ì˜: {len(large_files)}ê°œ í° íŒŒì¼ ë°œê²¬ë¨")

        return {"fixes_applied": self.fixes_applied, "large_files": large_files}


if __name__ == "__main__":
    cleaner = ProjectCleaner()
    result = cleaner.run_all_cleanups()
