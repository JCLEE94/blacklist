"""
ì˜ˆì¸¡ì  ë¬¸ì œ ê°ì§€ ì—”ì§„

AI ìë™í™” í”Œë«í¼ì˜ ê³ ê¸‰ ì˜ˆì¸¡ ë¶„ì„ ì‹œìŠ¤í…œ
- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ íŒ¨í„´ ì¸ì‹
- ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ë° ì˜ˆì¸¡
- ì´ìƒ ì§•í›„ ì¡°ê¸° ê°ì§€
- ìë™í™” ì‹¤íŒ¨ ìœ„í—˜ ì˜ˆì¸¡
"""

import logging
import statistics
from collections import defaultdict, deque
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

try:
    from src.core.automation.korean_alert_system import get_korean_alert_system
    from src.utils.structured_logging import get_logger
except ImportError:

    def get_logger(name):
        return logging.getLogger(name)

    def get_korean_alert_system():
        return None


logger = get_logger(__name__)


class PredictionType(Enum):
    """ì˜ˆì¸¡ ìœ í˜•"""

    TREND_ANALYSIS = "trend_analysis"
    ANOMALY_DETECTION = "anomaly_detection"
    FAILURE_PREDICTION = "failure_prediction"
    RESOURCE_FORECASTING = "resource_forecasting"
    AUTOMATION_SUCCESS = "automation_success"
    MERGE_CONFLICT_RISK = "merge_conflict_risk"


class RiskLevel(Enum):
    """ìœ„í—˜ ìˆ˜ì¤€"""

    VERY_LOW = "very_low"  # 0-20%
    LOW = "low"  # 20-40%
    MEDIUM = "medium"  # 40-60%
    HIGH = "high"  # 60-80%
    VERY_HIGH = "very_high"  # 80-100%


@dataclass
class PredictionResult:
    """ì˜ˆì¸¡ ê²°ê³¼"""

    prediction_type: PredictionType
    timestamp: datetime
    metric_name: str
    current_value: float
    predicted_value: float
    confidence: float
    risk_level: RiskLevel
    time_horizon: timedelta
    factors: List[str]
    recommendations: List[str]
    metadata: Dict[str, Any] = None


@dataclass
class AnomalyScore:
    """ì´ìƒ ì§•í›„ ì ìˆ˜"""

    metric_name: str
    timestamp: datetime
    value: float
    anomaly_score: float
    threshold: float
    is_anomaly: bool
    severity: str
    context: Dict[str, Any] = None


class TimeSeriesAnalyzer:
    """ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ê¸°"""

    def __init__(self, window_size: int = 20):
        self.window_size = window_size

    def detect_trend(self, values: List[float]) -> Tuple[str, float]:
        """íŠ¸ë Œë“œ ê°ì§€ (ì„ í˜• íšŒê·€ ê¸°ë°˜)"""
        if len(values) < 3:
            return "stable", 0.0

        n = len(values)
        x = list(range(n))

        # ì„ í˜• íšŒê·€ ê³„ì‚°
        x_mean = statistics.mean(x)
        y_mean = statistics.mean(values)

        numerator = sum((x[i] - x_mean) * (values[i] - y_mean) for i in range(n))
        denominator = sum((x[i] - x_mean) ** 2 for i in range(n))

        if denominator == 0:
            return "stable", 0.0

        slope = numerator / denominator

        # íŠ¸ë Œë“œ ë¶„ë¥˜
        if slope > 0.1:
            return "increasing", slope
        elif slope < -0.1:
            return "decreasing", slope
        else:
            return "stable", slope

    def calculate_volatility(self, values: List[float]) -> float:
        """ë³€ë™ì„± ê³„ì‚° (í‘œì¤€í¸ì°¨ ê¸°ë°˜)"""
        if len(values) < 2:
            return 0.0
        return statistics.stdev(values)

    def detect_seasonality(self, values: List[float], period: int = 7) -> bool:
        """ê³„ì ˆì„± íŒ¨í„´ ê°ì§€"""
        if len(values) < period * 2:
            return False

        # ìê¸°ìƒê´€ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
        correlations = []
        for lag in range(1, min(period + 1, len(values) // 2)):
            corr = self._calculate_autocorrelation(values, lag)
            correlations.append(corr)

        # ê°•í•œ ì£¼ê¸°ì  íŒ¨í„´ì´ ìˆìœ¼ë©´ True
        max_correlation = max(correlations) if correlations else 0
        return max_correlation > 0.6

    def _calculate_autocorrelation(self, values: List[float], lag: int) -> float:
        """ìê¸°ìƒê´€ ê³„ì‚°"""
        n = len(values)
        if lag >= n:
            return 0.0

        mean_val = statistics.mean(values)

        numerator = sum(
            (values[i] - mean_val) * (values[i - lag] - mean_val) for i in range(lag, n)
        )
        denominator = sum((values[i] - mean_val) ** 2 for i in range(n))

        if denominator == 0:
            return 0.0

        return numerator / denominator

    def predict_next_values(self, values: List[float], horizon: int = 5) -> List[float]:
        """ë‹¤ìŒ ê°’ë“¤ ì˜ˆì¸¡ (ë‹¨ìˆœ ì´ë™í‰ê·  + íŠ¸ë Œë“œ)"""
        if len(values) < 3:
            return [values[-1]] * horizon if values else [0] * horizon

        # ìµœê·¼ íŠ¸ë Œë“œ ê³„ì‚°
        trend, slope = self.detect_trend(values[-min(10, len(values)) :])

        # ì´ë™í‰ê·  ê³„ì‚°
        window = min(5, len(values))
        moving_avg = statistics.mean(values[-window:])

        # ì˜ˆì¸¡ê°’ ìƒì„±
        predictions = []
        for i in range(1, horizon + 1):
            predicted = moving_avg + (slope * i)
            predictions.append(predicted)

        return predictions


class AnomalyDetector:
    """ì´ìƒ ì§•í›„ íƒì§€ê¸°"""

    def __init__(self):
        self.baseline_windows = defaultdict(deque)
        self.anomaly_thresholds = {
            "git_changes": (50, 100),  # (warning, critical)
            "test_coverage": (30, 15),  # ë‚®ì„ìˆ˜ë¡ ë‚˜ì¨
            "api_response_time": (100, 200),
            "memory_usage": (70, 85),
            "cpu_usage": (70, 85),
            "file_violations": (3, 5),
        }

    def update_baseline(self, metric_name: str, value: float, max_size: int = 100):
        """ê¸°ì¤€ì„  ë°ì´í„° ì—…ë°ì´íŠ¸"""
        if metric_name not in self.baseline_windows:
            self.baseline_windows[metric_name] = deque(maxlen=max_size)

        self.baseline_windows[metric_name].append(
            {"value": value, "timestamp": datetime.now()}
        )

    def detect_anomaly(self, metric_name: str, current_value: float) -> AnomalyScore:
        """ì´ìƒ ì§•í›„ ê°ì§€"""
        baseline = self.baseline_windows.get(metric_name, deque())

        if len(baseline) < 5:
            # ë°ì´í„° ë¶€ì¡±ì‹œ ì„ê³„ê°’ ê¸°ë°˜ ê²€ì‚¬
            return self._threshold_based_detection(metric_name, current_value)

        # í†µê³„ì  ì´ìƒ ê°ì§€
        values = [item["value"] for item in baseline]
        mean_val = statistics.mean(values)
        std_val = statistics.stdev(values) if len(values) > 1 else 0

        # Z-score ê³„ì‚°
        z_score = abs(current_value - mean_val) / std_val if std_val > 0 else 0

        # ì´ìƒ ì ìˆ˜ ê³„ì‚° (0-1 ë²”ìœ„)
        anomaly_score = min(z_score / 3.0, 1.0)  # 3-sigma ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”

        # ì´ìƒ ì—¬ë¶€ íŒë‹¨
        is_anomaly = z_score > 2.5  # 2.5-sigma ì´ìƒ

        # ì‹¬ê°ë„ ê³„ì‚°
        if z_score > 3.5:
            severity = "critical"
        elif z_score > 2.5:
            severity = "high"
        elif z_score > 2.0:
            severity = "medium"
        else:
            severity = "low"

        return AnomalyScore(
            metric_name=metric_name,
            timestamp=datetime.now(),
            value=current_value,
            anomaly_score=anomaly_score,
            threshold=mean_val + (2.5 * std_val),
            is_anomaly=is_anomaly,
            severity=severity,
            context={
                "baseline_mean": mean_val,
                "baseline_std": std_val,
                "z_score": z_score,
                "baseline_size": len(baseline),
            },
        )

    def _threshold_based_detection(
        self, metric_name: str, value: float
    ) -> AnomalyScore:
        """ì„ê³„ê°’ ê¸°ë°˜ ì´ìƒ ê°ì§€"""
        thresholds = self.anomaly_thresholds.get(metric_name, (50, 100))
        warning_threshold, critical_threshold = thresholds

        # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ëŠ” ì—­ë°©í–¥ (ë‚®ì„ìˆ˜ë¡ ë‚˜ì¨)
        if metric_name == "test_coverage":
            if value <= critical_threshold:
                anomaly_score = 1.0
                is_anomaly = True
                severity = "critical"
            elif value <= warning_threshold:
                anomaly_score = 0.7
                is_anomaly = True
                severity = "high"
            else:
                anomaly_score = 0.0
                is_anomaly = False
                severity = "low"
        else:
            # ì¼ë°˜ ë©”íŠ¸ë¦­ (ë†’ì„ìˆ˜ë¡ ë‚˜ì¨)
            if value >= critical_threshold:
                anomaly_score = 1.0
                is_anomaly = True
                severity = "critical"
            elif value >= warning_threshold:
                anomaly_score = 0.7
                is_anomaly = True
                severity = "high"
            else:
                anomaly_score = max(0, value / warning_threshold - 0.5)
                is_anomaly = False
                severity = "low"

        return AnomalyScore(
            metric_name=metric_name,
            timestamp=datetime.now(),
            value=value,
            anomaly_score=anomaly_score,
            threshold=critical_threshold,
            is_anomaly=is_anomaly,
            severity=severity,
            context={"method": "threshold_based", "thresholds": thresholds},
        )


class AutomationRiskPredictor:
    """ìë™í™” ìœ„í—˜ ì˜ˆì¸¡ê¸°"""

    def __init__(self):
        self.risk_factors = {
            "git_changes_high": 0.3,  # Git ë³€ê²½ì‚¬í•­ ë§ìŒ
            "test_coverage_low": 0.4,  # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë‚®ìŒ
            "file_violations": 0.2,  # íŒŒì¼ í¬ê¸° ìœ„ë°˜
            "api_performance_bad": 0.25,  # API ì„±ëŠ¥ ë‚˜ì¨
            "memory_pressure": 0.2,  # ë©”ëª¨ë¦¬ ì••ë°•
            "recent_failures": 0.5,  # ìµœê·¼ ì‹¤íŒ¨ ì´ë ¥
            "complex_changes": 0.35,  # ë³µì¡í•œ ë³€ê²½ì‚¬í•­
            "weekend_deployment": 0.15,  # ì£¼ë§/ì•¼ê°„ ì‘ì—…
            "dependency_conflicts": 0.4,  # ì˜ì¡´ì„± ì¶©ëŒ
        }

        self.success_history = deque(maxlen=50)

    def predict_automation_success(
        self, current_metrics: Dict[str, float], automation_context: Dict[str, Any]
    ) -> PredictionResult:
        """ìë™í™” ì„±ê³µ í™•ë¥  ì˜ˆì¸¡"""

        risk_score = 0.0
        active_factors = []

        # ê° ìœ„í—˜ ìš”ì†Œ í‰ê°€
        if current_metrics.get("git_changes", 0) > 100:
            risk_score += self.risk_factors["git_changes_high"]
            active_factors.append("Git ë³€ê²½ì‚¬í•­ ê³¼ë‹¤ (100ê°œ ì´ˆê³¼)")

        if current_metrics.get("test_coverage", 100) < 30:
            risk_score += self.risk_factors["test_coverage_low"]
            active_factors.append("í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡± (30% ë¯¸ë§Œ)")

        if current_metrics.get("file_violations", 0) > 3:
            risk_score += self.risk_factors["file_violations"]
            active_factors.append("íŒŒì¼ í¬ê¸° ìœ„ë°˜ ì¡´ì¬")

        if current_metrics.get("api_response_time", 0) > 200:
            risk_score += self.risk_factors["api_performance_bad"]
            active_factors.append("API ì„±ëŠ¥ ì €í•˜ (200ms ì´ˆê³¼)")

        if current_metrics.get("memory_usage", 0) > 80:
            risk_score += self.risk_factors["memory_pressure"]
            active_factors.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ (80% ì´ˆê³¼)")

        # ìµœê·¼ ì‹¤íŒ¨ ì´ë ¥ í™•ì¸
        recent_failures = self._count_recent_failures()
        if recent_failures > 2:
            risk_score += self.risk_factors["recent_failures"]
            active_factors.append(f"ìµœê·¼ {recent_failures}íšŒ ì‹¤íŒ¨ ì´ë ¥")

        # ì‹œê°„ëŒ€ ìœ„í—˜ ìš”ì†Œ (ì£¼ë§/ì•¼ê°„)
        if self._is_risky_time():
            risk_score += self.risk_factors["weekend_deployment"]
            active_factors.append("ìœ„í—˜ ì‹œê°„ëŒ€ ì‘ì—… (ì£¼ë§/ì•¼ê°„)")

        # ë³µì¡ì„± í‰ê°€
        complexity_score = self._evaluate_complexity(automation_context)
        if complexity_score > 0.7:
            risk_score += self.risk_factors["complex_changes"]
            active_factors.append("ë†’ì€ ë³µì¡ë„ ë³€ê²½ì‚¬í•­")

        # ì„±ê³µ í™•ë¥  ê³„ì‚° (ìœ„í—˜ ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜)
        success_probability = max(0.1, 1.0 - min(risk_score, 0.9))

        # ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
        if success_probability >= 0.8:
            risk_level = RiskLevel.VERY_LOW
        elif success_probability >= 0.6:
            risk_level = RiskLevel.LOW
        elif success_probability >= 0.4:
            risk_level = RiskLevel.MEDIUM
        elif success_probability >= 0.2:
            risk_level = RiskLevel.HIGH
        else:
            risk_level = RiskLevel.VERY_HIGH

        # ì¶”ì²œì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(active_factors, risk_level)

        return PredictionResult(
            prediction_type=PredictionType.AUTOMATION_SUCCESS,
            timestamp=datetime.now(),
            metric_name="automation_success_rate",
            current_value=self._calculate_current_success_rate(),
            predicted_value=success_probability * 100,
            confidence=0.85,  # ì‹ ë¢°ë„
            risk_level=risk_level,
            time_horizon=timedelta(hours=2),
            factors=active_factors,
            recommendations=recommendations,
            metadata={
                "risk_score": risk_score,
                "complexity_score": complexity_score,
                "recent_failures": recent_failures,
            },
        )

    def predict_merge_conflict_risk(
        self, git_changes: int, file_types: List[str], change_complexity: Dict[str, int]
    ) -> PredictionResult:
        """ë¨¸ì§€ ì¶©ëŒ ìœ„í—˜ ì˜ˆì¸¡"""

        risk_factors = []
        risk_score = 0.0

        # ë³€ê²½ì‚¬í•­ ìˆ˜ëŸ‰ ìœ„í—˜
        if git_changes > 150:
            risk_score += 0.4
            risk_factors.append(f"ë³€ê²½ì‚¬í•­ ê³¼ë‹¤ ({git_changes}ê°œ)")
        elif git_changes > 100:
            risk_score += 0.2
            risk_factors.append(f"ë³€ê²½ì‚¬í•­ ë§ìŒ ({git_changes}ê°œ)")

        # íŒŒì¼ ìœ í˜•ë³„ ìœ„í—˜ë„
        high_risk_files = ["models.py", "__init__.py", "main.py", "settings.py"]
        risky_files = [
            f for f in file_types if any(risk in f for risk in high_risk_files)
        ]

        if risky_files:
            risk_score += 0.3
            risk_factors.append(f"ê³ ìœ„í—˜ íŒŒì¼ í¬í•¨: {risky_files[:3]}")

        # ë³µì¡ë„ ê¸°ë°˜ ìœ„í—˜
        total_complexity = sum(change_complexity.values())
        if total_complexity > 1000:
            risk_score += 0.25
            risk_factors.append("ë†’ì€ ë³€ê²½ ë³µì¡ë„")

        # ë™ì‹œ ì‘ì—…ì ìˆ˜ (ê°€ìƒì˜ ë©”íŠ¸ë¦­)
        concurrent_developers = change_complexity.get("concurrent_developers", 1)
        if concurrent_developers > 3:
            risk_score += 0.2
            risk_factors.append(f"ë™ì‹œ ì‘ì—…ì ë‹¤ìˆ˜ ({concurrent_developers}ëª…)")

        conflict_probability = min(risk_score, 0.9)

        # ìœ„í—˜ ìˆ˜ì¤€ ê²°ì •
        if conflict_probability >= 0.7:
            risk_level = RiskLevel.VERY_HIGH
        elif conflict_probability >= 0.5:
            risk_level = RiskLevel.HIGH
        elif conflict_probability >= 0.3:
            risk_level = RiskLevel.MEDIUM
        elif conflict_probability >= 0.1:
            risk_level = RiskLevel.LOW
        else:
            risk_level = RiskLevel.VERY_LOW

        recommendations = (
            [
                "ë³€ê²½ì‚¬í•­ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì»¤ë°‹",
                "ìì£¼ ë©”ì¸ ë¸Œëœì¹˜ì™€ ë™ê¸°í™”",
                "ê³ ìœ„í—˜ íŒŒì¼ì€ ë‹¨ë… ì‘ì—… ê¶Œì¥",
                "ì½”ë“œ ë¦¬ë·° ê°•í™”",
            ]
            if risk_level != RiskLevel.VERY_LOW
            else ["í˜„ì¬ ìƒíƒœ ìœ ì§€"]
        )

        return PredictionResult(
            prediction_type=PredictionType.MERGE_CONFLICT_RISK,
            timestamp=datetime.now(),
            metric_name="merge_conflict_probability",
            current_value=0.0,  # í˜„ì¬ ì¶©ëŒ ì—†ìŒ
            predicted_value=conflict_probability * 100,
            confidence=0.75,
            risk_level=risk_level,
            time_horizon=timedelta(hours=6),
            factors=risk_factors,
            recommendations=recommendations,
        )

    def _count_recent_failures(self) -> int:
        """ìµœê·¼ ì‹¤íŒ¨ íšŸìˆ˜ ê³„ì‚°"""
        cutoff_time = datetime.now() - timedelta(hours=24)
        return len(
            [
                h
                for h in self.success_history
                if h.get("success", True) == False
                and h.get("timestamp", datetime.min) > cutoff_time
            ]
        )

    def _is_risky_time(self) -> bool:
        """ìœ„í—˜ ì‹œê°„ëŒ€ í™•ì¸ (ì£¼ë§, ì•¼ê°„)"""
        now = datetime.now()

        # ì£¼ë§ í™•ì¸
        if now.weekday() >= 5:  # í† ìš”ì¼(5), ì¼ìš”ì¼(6)
            return True

        # ì•¼ê°„ ì‹œê°„ í™•ì¸ (22ì‹œ~6ì‹œ)
        if now.hour >= 22 or now.hour <= 6:
            return True

        return False

    def _evaluate_complexity(self, context: Dict[str, Any]) -> float:
        """ë³€ê²½ì‚¬í•­ ë³µì¡ë„ í‰ê°€"""
        complexity_score = 0.0

        # ì˜í–¥ë°›ëŠ” íŒŒì¼ ìˆ˜
        files_count = context.get("affected_files", 0)
        complexity_score += min(files_count / 50, 0.3)

        # ì½”ë“œ ë¼ì¸ ìˆ˜
        lines_changed = context.get("lines_changed", 0)
        complexity_score += min(lines_changed / 1000, 0.3)

        # ìƒˆ ì˜ì¡´ì„± ì¶”ê°€
        new_dependencies = context.get("new_dependencies", 0)
        complexity_score += new_dependencies * 0.1

        # ë°ì´í„°ë² ì´ìŠ¤ ë³€ê²½
        if context.get("database_changes", False):
            complexity_score += 0.2

        # API ë³€ê²½
        if context.get("api_changes", False):
            complexity_score += 0.15

        return min(complexity_score, 1.0)

    def _calculate_current_success_rate(self) -> float:
        """í˜„ì¬ ì„±ê³µë¥  ê³„ì‚°"""
        if not self.success_history:
            return 80.0  # ê¸°ë³¸ê°’

        successful = len([h for h in self.success_history if h.get("success", True)])
        return (successful / len(self.success_history)) * 100

    def _generate_recommendations(
        self, risk_factors: List[str], risk_level: RiskLevel
    ) -> List[str]:
        """ìœ„í—˜ ìš”ì†Œ ê¸°ë°˜ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []

        if risk_level == RiskLevel.VERY_HIGH:
            recommendations.extend(
                [
                    "ğŸš¨ ìë™í™” ì‹¤í–‰ ì¤‘ë‹¨ ê¶Œì¥",
                    "ìœ„í—˜ ìš”ì†Œ í•´ê²° í›„ ì¬ì‹œë„",
                    "ìˆ˜ë™ ê²€ì¦ ë‹¨ê³„ ì¶”ê°€",
                ]
            )

        if "Git ë³€ê²½ì‚¬í•­" in str(risk_factors):
            recommendations.append("ë³€ê²½ì‚¬í•­ì„ ê·¸ë£¹ë³„ë¡œ ë¶„í• í•˜ì—¬ ë‹¨ê³„ì  ì²˜ë¦¬")

        if "í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€" in str(risk_factors):
            recommendations.append("í•µì‹¬ ê¸°ëŠ¥ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìš°ì„  ì¶”ê°€")

        if "íŒŒì¼ í¬ê¸°" in str(risk_factors):
            recommendations.append("í° íŒŒì¼ì„ ëª¨ë“ˆë³„ë¡œ ë¶„í• ")

        if "ì„±ëŠ¥" in str(risk_factors):
            recommendations.append("ì„±ëŠ¥ ìµœì í™” í›„ ìë™í™” ì¬ì‹¤í–‰")

        if "ë©”ëª¨ë¦¬" in str(risk_factors):
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ì •ë¦¬ í›„ ì§„í–‰")

        if not recommendations:
            recommendations = ["í˜„ì¬ ìƒíƒœë¡œ ì•ˆì „í•œ ì§„í–‰ ê°€ëŠ¥"]

        return recommendations

    def record_automation_result(self, success: bool, context: Dict[str, Any]):
        """ìë™í™” ê²°ê³¼ ê¸°ë¡"""
        self.success_history.append(
            {"timestamp": datetime.now(), "success": success, "context": context}
        )


class PredictiveEngine:
    """ì˜ˆì¸¡ì  ë¬¸ì œ ê°ì§€ ì—”ì§„ ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.logger = get_logger(self.__class__.__name__)
        self.time_series_analyzer = TimeSeriesAnalyzer()
        self.anomaly_detector = AnomalyDetector()
        self.risk_predictor = AutomationRiskPredictor()
        self.alert_system = get_korean_alert_system()

        # ë©”íŠ¸ë¦­ íˆìŠ¤í† ë¦¬ ì €ì¥
        self.metrics_history = defaultdict(list)
        self.prediction_cache = {}
        self.last_prediction_time = {}

    def analyze_system_health(
        self, current_metrics: Dict[str, float]
    ) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì „ë°˜ì  ê±´ê°•ë„ ë¶„ì„"""
        analysis_results = {
            "timestamp": datetime.now(),
            "overall_health": "unknown",
            "predictions": [],
            "anomalies": [],
            "recommendations": [],
            "risk_assessment": {},
        }

        try:
            # ê° ë©”íŠ¸ë¦­ë³„ ë¶„ì„
            for metric_name, value in current_metrics.items():
                # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
                self.metrics_history[metric_name].append(
                    {"timestamp": datetime.now(), "value": value}
                )

                # ìµœê·¼ 100ê°œë§Œ ìœ ì§€
                if len(self.metrics_history[metric_name]) > 100:
                    self.metrics_history[metric_name] = self.metrics_history[
                        metric_name
                    ][-100:]

                # ë² ì´ìŠ¤ë¼ì¸ ì—…ë°ì´íŠ¸
                self.anomaly_detector.update_baseline(metric_name, value)

                # ì´ìƒ ì§•í›„ ê°ì§€
                anomaly = self.anomaly_detector.detect_anomaly(metric_name, value)
                if anomaly.is_anomaly:
                    analysis_results["anomalies"].append(asdict(anomaly))

                    # ì´ìƒ ì§•í›„ ì•Œë¦¼
                    if self.alert_system:
                        self.alert_system.send_system_status_alert(
                            metric_name,
                            value,
                            anomaly.threshold,
                            trend=self._get_metric_trend(metric_name),
                        )

            # íŠ¸ë Œë“œ ì˜ˆì¸¡
            trend_predictions = self._generate_trend_predictions()
            analysis_results["predictions"].extend(trend_predictions)

            # ìë™í™” ìœ„í—˜ ì˜ˆì¸¡
            automation_prediction = self.risk_predictor.predict_automation_success(
                current_metrics, {"timestamp": datetime.now()}
            )
            analysis_results["predictions"].append(asdict(automation_prediction))

            # ì „ì²´ ê±´ê°•ë„ í‰ê°€
            overall_health = self._calculate_overall_health(
                current_metrics, analysis_results["anomalies"]
            )
            analysis_results["overall_health"] = overall_health

            # ì¢…í•© ì¶”ì²œì‚¬í•­
            analysis_results["recommendations"] = self._generate_system_recommendations(
                analysis_results["anomalies"], automation_prediction
            )

            self.logger.info(f"ì‹œìŠ¤í…œ ê±´ê°•ë„ ë¶„ì„ ì™„ë£Œ: {overall_health}")

        except Exception as e:
            self.logger.error(f"ì‹œìŠ¤í…œ ê±´ê°•ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            analysis_results["overall_health"] = "error"

        return analysis_results

    def _generate_trend_predictions(self) -> List[Dict[str, Any]]:
        """íŠ¸ë Œë“œ ê¸°ë°˜ ì˜ˆì¸¡ ìƒì„±"""
        predictions = []

        for metric_name, history in self.metrics_history.items():
            if len(history) < 5:
                continue

            values = [h["value"] for h in history[-20:]]  # ìµœê·¼ 20ê°œ

            # íŠ¸ë Œë“œ ë¶„ì„
            trend, slope = self.time_series_analyzer.detect_trend(values)

            if trend != "stable":
                # ë¯¸ë˜ ê°’ ì˜ˆì¸¡
                future_values = self.time_series_analyzer.predict_next_values(values, 5)

                prediction = PredictionResult(
                    prediction_type=PredictionType.TREND_ANALYSIS,
                    timestamp=datetime.now(),
                    metric_name=metric_name,
                    current_value=values[-1],
                    predicted_value=future_values[2],  # 3ë‹¨ê³„ í›„ ì˜ˆì¸¡ê°’
                    confidence=self._calculate_trend_confidence(values),
                    risk_level=self._assess_trend_risk(metric_name, trend, slope),
                    time_horizon=timedelta(minutes=90),  # 3ë‹¨ê³„ * 30ë¶„
                    factors=[f"íŠ¸ë Œë“œ: {trend}", f"ê¸°ìš¸ê¸°: {slope:.3f}"],
                    recommendations=self._get_trend_recommendations(metric_name, trend),
                )

                predictions.append(asdict(prediction))

        return predictions

    def _calculate_trend_confidence(self, values: List[float]) -> float:
        """íŠ¸ë Œë“œ ì‹ ë¢°ë„ ê³„ì‚°"""
        if len(values) < 5:
            return 0.5

        # R-squared ìœ ì‚¬ ê³„ì‚° (ë‹¨ìˆœí™”ëœ ë²„ì „)
        trend, slope = self.time_series_analyzer.detect_trend(values)

        # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ ê³„ì‚°
        n = len(values)
        x = list(range(n))
        y_mean = statistics.mean(values)

        predicted = [slope * i + values[0] for i in x]
        ss_res = sum((values[i] - predicted[i]) ** 2 for i in range(n))
        ss_tot = sum((values[i] - y_mean) ** 2 for i in range(n))

        if ss_tot == 0:
            return 0.5

        r_squared = 1 - (ss_res / ss_tot)
        return max(0.1, min(0.95, r_squared))

    def _assess_trend_risk(
        self, metric_name: str, trend: str, slope: float
    ) -> RiskLevel:
        """íŠ¸ë Œë“œ ìœ„í—˜ë„ í‰ê°€"""
        # ë©”íŠ¸ë¦­ë³„ ìœ„í—˜ ë°©í–¥ ì •ì˜
        bad_trends = {
            "git_changes": "increasing",
            "api_response_time": "increasing",
            "memory_usage": "increasing",
            "cpu_usage": "increasing",
            "file_violations": "increasing",
            "test_coverage": "decreasing",  # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ëŠ” ê°ì†Œê°€ ë‚˜ì¨
        }

        bad_trend = bad_trends.get(metric_name, "increasing")

        if trend == bad_trend:
            if abs(slope) > 2.0:
                return RiskLevel.VERY_HIGH
            elif abs(slope) > 1.0:
                return RiskLevel.HIGH
            elif abs(slope) > 0.5:
                return RiskLevel.MEDIUM
            else:
                return RiskLevel.LOW
        else:
            return RiskLevel.VERY_LOW

    def _get_trend_recommendations(self, metric_name: str, trend: str) -> List[str]:
        """íŠ¸ë Œë“œë³„ ì¶”ì²œì‚¬í•­"""
        recommendations_map = {
            ("git_changes", "increasing"): [
                "ë³€ê²½ì‚¬í•­ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì»¤ë°‹",
                "ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ë³€ê²½ì‚¬í•­ë¶€í„° ì²˜ë¦¬",
            ],
            ("test_coverage", "decreasing"): [
                "ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€",
                "ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ê²€í†  ë° ë³´ì™„",
            ],
            ("api_response_time", "increasing"): [
                "API ì„±ëŠ¥ ìµœì í™” í•„ìš”",
                "ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”",
                "ìºì‹± ì „ëµ ê²€í† ",
            ],
            ("memory_usage", "increasing"): [
                "ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í™•ì¸",
                "ë¶ˆí•„ìš”í•œ ê°ì²´ ì •ë¦¬",
                "ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”",
            ],
        }

        return recommendations_map.get((metric_name, trend), ["ì¶”ì„¸ ëª¨ë‹ˆí„°ë§ ê³„ì†"])

    def _calculate_overall_health(
        self, metrics: Dict[str, float], anomalies: List[Dict[str, Any]]
    ) -> str:
        """ì „ì²´ ì‹œìŠ¤í…œ ê±´ê°•ë„ ê³„ì‚°"""

        # ì´ìƒ ì§•í›„ ì ìˆ˜ ê³„ì‚°
        anomaly_score = 0
        for anomaly in anomalies:
            if anomaly.get("severity") == "critical":
                anomaly_score += 30
            elif anomaly.get("severity") == "high":
                anomaly_score += 20
            elif anomaly.get("severity") == "medium":
                anomaly_score += 10

        # ë©”íŠ¸ë¦­ë³„ ê±´ê°•ë„ ì ìˆ˜
        health_score = 100

        # Git ë³€ê²½ì‚¬í•­ (ë§ì„ìˆ˜ë¡ ìœ„í—˜)
        git_changes = metrics.get("git_changes", 0)
        if git_changes > 130:
            health_score -= 25
        elif git_changes > 100:
            health_score -= 15

        # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ (ë‚®ì„ìˆ˜ë¡ ìœ„í—˜)
        test_coverage = metrics.get("test_coverage", 100)
        if test_coverage < 20:
            health_score -= 30
        elif test_coverage < 50:
            health_score -= 15

        # API ì„±ëŠ¥
        api_time = metrics.get("api_response_time", 0)
        if api_time > 200:
            health_score -= 20
        elif api_time > 100:
            health_score -= 10

        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
        memory_usage = metrics.get("memory_usage", 0)
        cpu_usage = metrics.get("cpu_usage", 0)
        if max(memory_usage, cpu_usage) > 85:
            health_score -= 15
        elif max(memory_usage, cpu_usage) > 70:
            health_score -= 8

        # ì´ìƒ ì§•í›„ ì ìˆ˜ ë°˜ì˜
        health_score -= anomaly_score

        # ê±´ê°•ë„ ë¶„ë¥˜
        health_score = max(0, health_score)

        if health_score >= 90:
            return "excellent"
        elif health_score >= 75:
            return "good"
        elif health_score >= 60:
            return "fair"
        elif health_score >= 40:
            return "poor"
        else:
            return "critical"

    def _generate_system_recommendations(
        self, anomalies: List[Dict[str, Any]], automation_prediction: PredictionResult
    ) -> List[str]:
        """ì‹œìŠ¤í…œ ì „ì²´ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []

        # ì´ìƒ ì§•í›„ ê¸°ë°˜ ì¶”ì²œ
        critical_anomalies = [a for a in anomalies if a.get("severity") == "critical"]
        if critical_anomalies:
            recommendations.append("ğŸš¨ ê¸´ê¸‰: ì‹¬ê°í•œ ì‹œìŠ¤í…œ ì´ìƒ ì§•í›„ í•´ê²° í•„ìš”")

        high_anomalies = [a for a in anomalies if a.get("severity") == "high"]
        if high_anomalies:
            recommendations.append("âš ï¸ ì£¼ì˜: ì‹œìŠ¤í…œ ì„±ëŠ¥ ì €í•˜ ìš”ì¸ ì ê²€")

        # ìë™í™” ìœ„í—˜ ê¸°ë°˜ ì¶”ì²œ
        if automation_prediction.risk_level in [RiskLevel.HIGH, RiskLevel.VERY_HIGH]:
            recommendations.append("ğŸ¤– ìë™í™” ìœ„í—˜ë„ ë†’ìŒ - ë‹¨ê³„ì  ì ‘ê·¼ ê¶Œì¥")

        # ì¼ë°˜ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.extend(
                ["âœ… ì‹œìŠ¤í…œ ìƒíƒœ ì–‘í˜¸ - í˜„ì¬ ìˆ˜ì¤€ ìœ ì§€", "ğŸ“Š ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ê¶Œì¥"]
            )

        return recommendations

    def _get_metric_trend(self, metric_name: str) -> Optional[str]:
        """ë©”íŠ¸ë¦­ íŠ¸ë Œë“œ ì¡°íšŒ"""
        history = self.metrics_history.get(metric_name, [])
        if len(history) < 5:
            return None

        values = [h["value"] for h in history[-10:]]
        trend, _ = self.time_series_analyzer.detect_trend(values)
        return trend

    def get_prediction_summary(self) -> Dict[str, Any]:
        """ì˜ˆì¸¡ ìš”ì•½ ì •ë³´"""
        return {
            "metrics_tracked": len(self.metrics_history),
            "total_history_points": sum(len(h) for h in self.metrics_history.values()),
            "recent_anomalies": len(
                [a for a in self.anomaly_detector.baseline_windows.values()]
            ),
            "automation_success_rate": self.risk_predictor._calculate_current_success_rate(),
            "system_uptime": datetime.now().isoformat(),
            "prediction_capabilities": [pt.value for pt in PredictionType],
            "risk_levels": [rl.value for rl in RiskLevel],
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_predictive_engine = None


def get_predictive_engine() -> PredictiveEngine:
    """ì˜ˆì¸¡ ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _predictive_engine
    if _predictive_engine is None:
        _predictive_engine = PredictiveEngine()
    return _predictive_engine


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    engine = get_predictive_engine()

    # ìƒ˜í”Œ ë©”íŠ¸ë¦­ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_metrics = {
        "git_changes": 133,
        "test_coverage": 19.0,
        "api_response_time": 65.0,
        "memory_usage": 45.0,
        "cpu_usage": 25.0,
        "file_violations": 2,
    }

    # ì‹œìŠ¤í…œ ê±´ê°•ë„ ë¶„ì„
    analysis = engine.analyze_system_health(test_metrics)

    print("ğŸ”® ì˜ˆì¸¡ì  ë¬¸ì œ ê°ì§€ ì—”ì§„ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"ì „ì²´ ê±´ê°•ë„: {analysis['overall_health']}")
    print(f"ì´ìƒ ì§•í›„: {len(analysis['anomalies'])}ê°œ")
    print(f"ì˜ˆì¸¡ ê²°ê³¼: {len(analysis['predictions'])}ê°œ")
    print(f"ì¶”ì²œì‚¬í•­: {len(analysis['recommendations'])}ê°œ")

    # ìƒì„¸ ì •ë³´ ì¶œë ¥
    for recommendation in analysis["recommendations"]:
        print(f"ğŸ’¡ {recommendation}")

    # ì˜ˆì¸¡ ìš”ì•½
    summary = engine.get_prediction_summary()
    print(f"\nğŸ“Š ì˜ˆì¸¡ ì‹œìŠ¤í…œ ìš”ì•½:")
    for key, value in summary.items():
        print(f"  {key}: {value}")
