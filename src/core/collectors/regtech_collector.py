#!/usr/bin/env python3
"""
REGTECH ìˆ˜ì§‘ê¸° - ëª¨ë“ˆí™”ëœ êµ¬ì¡°
BaseCollector ìƒì† ë° ê°•í™”ëœ ì—ëŸ¬ í•¸ë“¤ë§
"""

import asyncio
import logging
import os
from datetime import datetime, timedelta
from typing import Any, Dict, List

import requests

from ..common.ip_utils import IPUtils
from .helpers.data_transform import RegtechDataTransform
from .helpers.request_utils import RegtechRequestUtils
from .helpers.validation_utils import RegtechValidationUtils
from .regtech_auth import RegtechAuth
from .regtech_browser import RegtechBrowserAutomation
from .regtech_data_processor import RegtechDataProcessor
from .unified_collector import BaseCollector, CollectionConfig

logger = logging.getLogger(__name__)


class RegtechCollector(BaseCollector):
    """
    BaseCollectorë¥¼ ìƒì†ë°›ì€ REGTECH ìˆ˜ì§‘ê¸°
    ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ê°•í™”ëœ ì—ëŸ¬ í•¸ë“¤ë§ê³¼ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨
    """

    def __init__(self, config: CollectionConfig):
        super().__init__("REGTECH", config)

        # ê¸°ë³¸ ì„¤ì •
        self.base_url = "https://regtech.fsec.or.kr"
        self.config_data = {}

        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
        self.username = os.getenv("REGTECH_USERNAME")
        self.password = os.getenv("REGTECH_PASSWORD")

        # DBì—ì„œ ì„¤ì • ë¡œë“œ (ì„ íƒì )
        self._load_db_config()

        # ì—ëŸ¬ í•¸ë“¤ë§ ì„¤ì •
        self.max_page_errors = 5
        self.session_retry_limit = 3
        self.request_timeout = 30
        self.page_delay = 1

        # ìƒíƒœ ì¶”ì 
        self.current_session = None
        self.page_error_count = 0
        self.total_collected = 0

        # ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™”
        self.auth = RegtechAuth(self.base_url, self.username, self.password)
        self.browser_automation = RegtechBrowserAutomation(
            self.base_url, self.username, self.password
        )
        self.data_processor = RegtechDataProcessor()

        # Helper ê°ì²´ë“¤ ì´ˆê¸°í™”
        self.request_utils = RegtechRequestUtils(self.base_url, self.request_timeout)
        self.data_transform = RegtechDataTransform()
        self.validation_utils = RegtechValidationUtils()
        self.validation_utils.set_ip_utils(IPUtils)

        # ë°ì´í„° í”„ë¡œì„¸ì„œì— ê²€ì¦ ìœ í‹¸ë¦¬í‹° ì„¤ì •
        self.data_processor.validation_utils = self.validation_utils

        logger.info("REGTECH collector initialized with modular components")

    def _load_db_config(self):
        """DBì—ì„œ ì„¤ì • ë¡œë“œ (ì„ íƒì )"""
        try:
            from ..database.collection_settings import CollectionSettingsDB

            self.db = CollectionSettingsDB()

            # DBì—ì„œ REGTECH ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            source_config = self.db.get_source_config("regtech")
            credentials = self.db.get_credentials("regtech")

            if source_config:
                self.base_url = source_config.get("base_url", self.base_url)
                self.config_data = source_config.get("config", {})

            if credentials:
                self.username = credentials["username"]
                self.password = credentials["password"]
            else:
                # í™˜ê²½ë³€ìˆ˜ fallback
                self.username = os.getenv("REGTECH_USERNAME")
                self.password = os.getenv("REGTECH_PASSWORD")

        except ImportError:
            # DB ì—†ìœ¼ë©´ ê¸°ë³¸ê°’/í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
            self.base_url = "https://regtech.fsec.or.kr"
            self.username = os.getenv("REGTECH_USERNAME")
            self.password = os.getenv("REGTECH_PASSWORD")
            self.config_data = {}

    def set_cookie_string(self, cookie_string: str):
        """ì™¸ë¶€ì—ì„œ ì¿ í‚¤ ë¬¸ìì—´ ì„¤ì •"""
        self.auth.set_cookie_string(cookie_string)
        logger.info("Cookie string updated through auth module")

    @property
    def source_type(self) -> str:
        return "REGTECH"

    async def _collect_data(self) -> List[Any]:
        """
        ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ ë©”ì„œë“œ - ìë™ ì¿ í‚¤ ê´€ë¦¬ í¬í•¨
        """
        # 1. ì¿ í‚¤ê°€ ì—†ìœ¼ë©´ ìë™ ì¶”ì¶œ ì‹œë„
        if not self.auth.cookie_auth_mode:
            logger.info("ğŸ”„ No cookies available - attempting automatic extraction...")
            cookie_string = self.browser_automation.auto_extract_cookies()
            if cookie_string:
                self.auth.set_cookie_string(cookie_string)
                logger.info("âœ… Automatic cookie extraction successful")
            else:
                logger.warning(
                    "âŒ Automatic cookie extraction failed - falling back to login mode"
                )
                return await self._collect_with_login()

        # 2. ì¿ í‚¤ ê¸°ë°˜ ìˆ˜ì§‘ ì‹œë„
        if self.auth.cookie_auth_mode:
            collected_data = await self._collect_with_cookies()

            # 3. ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì¿ í‚¤ ë§Œë£Œ ì˜ì‹¬ ì‹œ ì¬ì¶”ì¶œ ì‹œë„
            if not collected_data:
                logger.warning(
                    "ğŸ”„ No data collected - cookies might be expired, attempting re-extraction..."
                )
                cookie_string = self.browser_automation.auto_extract_cookies()
                if cookie_string:
                    self.auth.set_cookie_string(cookie_string)
                    logger.info(
                        "âœ… Cookie re-extraction successful - retrying collection..."
                    )
                    collected_data = await self._collect_with_cookies()
                else:
                    logger.error(
                        "âŒ Cookie re-extraction failed - falling back to login mode"
                    )
                    return await self._collect_with_login()

            return collected_data
        else:
            return await self._collect_with_login()

    async def _collect_with_cookies(self) -> List[Any]:
        """ì¿ í‚¤ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘"""
        collected_ips = []

        try:
            # ì¸ì¦ëœ ì„¸ì…˜ ìƒì„±
            session = self.auth.create_authenticated_session()

            logger.info("Starting cookie-based data collection")

            # ì‹¤ì œ REGTECH ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ëŠ” ë¸”ë™ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë“¤
            blacklist_urls = [
                "/board/11/boardList",  # ê³µì§€ì‚¬í•­ ê²Œì‹œíŒ (ìœ„í˜‘ ì •ë³´ í¬í•¨ ê°€ëŠ¥)
                "/fcti/securityAdvisory/advisoryList",  # ë³´ì•ˆ ê¶Œê³  ëª©ë¡
                "/fcti/securityAdvisory/blacklistDownload",  # ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ
                "/fcti/threat/threatList",  # ìœ„í˜‘ ì •ë³´ ëª©ë¡
                "/fcti/threat/ipBlacklist",  # IP ë¸”ë™ë¦¬ìŠ¤íŠ¸
                "/fcti/report/threatReport",  # ìœ„í˜‘ ë¦¬í¬íŠ¸
                "/board/boardList?menuCode=FCTI",  # FCTI ê´€ë ¨ ê²Œì‹œíŒ
                "/threat/intelligence/ipList",  # ìœ„í˜‘ ì¸í…”ë¦¬ì „ìŠ¤ IP ëª©ë¡
            ]

            for path in blacklist_urls:
                try:
                    url = f"{self.base_url}{path}"
                    logger.info(f"Trying URL: {url}")

                    response = session.get(
                        url, verify=False, timeout=self.request_timeout
                    )

                    # ì¿ í‚¤ ë§Œë£Œ í™•ì¸
                    if self.auth._is_cookie_expired(response):
                        logger.warning(
                            f"Cookies expired at {url} - will trigger re-extraction"
                        )
                        return []  # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ ìƒìœ„ì—ì„œ ì¬ì¶”ì¶œ íŠ¸ë¦¬ê±°

                    if response.status_code == 200:
                        content_type = response.headers.get("content-type", "").lower()

                        # ë°ì´í„° í”„ë¡œì„¸ì„œë¡œ ìœ„ì„
                        if "excel" in content_type or "spreadsheet" in content_type:
                            ips = await self.data_processor.process_excel_response(
                                response
                            )
                            if ips:
                                collected_ips.extend(ips)
                                logger.info(
                                    f"Collected {len(ips)} IPs from Excel download"
                                )
                                break

                        elif "text/html" in content_type:
                            ips = await self.data_processor.process_html_response(
                                response
                            )
                            if ips:
                                collected_ips.extend(ips)
                                logger.info(f"Collected {len(ips)} IPs from HTML page")
                                if len(ips) > 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
                                    break

                        elif "application/json" in content_type:
                            ips = await self.data_processor.process_json_response(
                                response
                            )
                            if ips:
                                collected_ips.extend(ips)
                                logger.info(f"Collected {len(ips)} IPs from JSON API")
                                break

                    elif (
                        response.status_code == 302
                        and "login" in response.headers.get("Location", "")
                    ):
                        logger.warning("Redirected to login - cookies may be expired")
                        break

                except Exception as e:
                    logger.error(f"Error accessing {path}: {e}")
                    continue

            # ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦ ë° ë³€í™˜
            if collected_ips:
                validated_ips = self.data_processor.validate_and_transform_data(
                    collected_ips
                )
                logger.info(
                    f"Validated {len(validated_ips)} out of {len(collected_ips)} collected IPs"
                )
                return validated_ips
            else:
                logger.warning("No IPs collected - check cookies or access permissions")
                return []

        except Exception as e:
            logger.error(f"Cookie-based collection failed: {e}")
            return []

    async def _collect_with_login(self) -> List[Any]:
        """ê¸°ì¡´ ë¡œê·¸ì¸ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘"""
        collected_ips = []
        session_retry_count = 0

        while session_retry_count < self.session_retry_limit:
            try:
                # ì„¸ì…˜ ì´ˆê¸°í™”
                session = self.request_utils.create_session()
                self.current_session = session

                # ë¡œê·¸ì¸ ì‹œë„
                if not self._robust_login(session):
                    raise Exception("ë¡œê·¸ì¸ ì‹¤íŒ¨ í›„ ì¬ì‹œë„ í•œê³„ ë„ë‹¬")

                # ë°ì´í„° ìˆ˜ì§‘
                start_date, end_date = self.data_transform.get_date_range(self.config)
                collected_ips = await self._robust_collect_ips(
                    session, start_date, end_date
                )

                # ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ ì™„ë£Œ
                logger.info(f"REGTECH ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_ips)}ê°œ IP")
                break

            except requests.exceptions.ConnectionError as e:
                session_retry_count += 1
                logger.warning(
                    f"ì—°ê²° ì˜¤ë¥˜ (ì¬ì‹œë„ {session_retry_count}/{self.session_retry_limit}): {e}"
                )
                if session_retry_count < self.session_retry_limit:
                    await asyncio.sleep(5 * session_retry_count)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„

            except requests.exceptions.Timeout as e:
                session_retry_count += 1
                logger.warning(
                    f"íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ (ì¬ì‹œë„ {session_retry_count}/{self.session_retry_limit}): {e}"
                )
                if session_retry_count < self.session_retry_limit:
                    await asyncio.sleep(3 * session_retry_count)

            except Exception as e:
                logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                session_retry_count += 1
                if session_retry_count < self.session_retry_limit:
                    await asyncio.sleep(2 * session_retry_count)

            finally:
                if hasattr(self, "current_session") and self.current_session:
                    self.current_session.close()
                    self.current_session = None

        if session_retry_count >= self.session_retry_limit:
            raise Exception(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ({self.session_retry_limit}) ì´ˆê³¼")

        return collected_ips

    def _robust_login(self, session: requests.Session) -> bool:
        """ê°•í™”ëœ ë¡œê·¸ì¸ ë¡œì§ - ì¸ì¦ ëª¨ë“ˆë¡œ ìœ„ì„"""
        return self.auth.robust_login(session)

    async def _robust_collect_ips(
        self, session: requests.Session, start_date: str, end_date: str
    ) -> List[Dict[str, Any]]:
        """ê°•í™”ëœ IP ìˆ˜ì§‘ ë¡œì§"""
        all_ips = []
        page = 0
        consecutive_errors = 0
        max_pages = 100  # ì•ˆì „ì¥ì¹˜

        logger.info(f"IP ìˆ˜ì§‘ ì‹œì‘: {start_date} ~ {end_date}")

        while page < max_pages and consecutive_errors < self.max_page_errors:
            try:
                # ì·¨ì†Œ ìš”ì²­ í™•ì¸
                if self.validation_utils.should_cancel(
                    getattr(self, "_cancel_event", None)
                ):
                    logger.info("ì‚¬ìš©ì ì·¨ì†Œ ìš”ì²­ìœ¼ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break

                # í˜ì´ì§€ ì§€ì—°
                if page > 0:
                    await asyncio.sleep(self.page_delay)

                # í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘
                page_ips = await self.request_utils.collect_single_page(
                    session, page, start_date, end_date
                )

                # IP ìœ íš¨ì„± ê²€ì‚¬ ì ìš©
                valid_page_ips = []
                for ip_data in page_ips:
                    if self.validation_utils.is_valid_ip(ip_data.get("ip", "")):
                        valid_page_ips.append(ip_data)
                page_ips = valid_page_ips

                if not page_ips:
                    logger.info(f"í˜ì´ì§€ {page + 1}ì—ì„œ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ, ìˆ˜ì§‘ ì¢…ë£Œ")
                    break

                all_ips.extend(page_ips)
                consecutive_errors = 0  # ì„±ê³µ ì‹œ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹

                logger.info(
                    f"í˜ì´ì§€ {page + 1}: {len(page_ips)}ê°œ ìˆ˜ì§‘ (ì´ {len(all_ips)}ê°œ)"
                )
                page += 1

            except requests.exceptions.RequestException as e:
                consecutive_errors += 1
                logger.warning(
                    f"í˜ì´ì§€ {page + 1} ìˆ˜ì§‘ ì‹¤íŒ¨ (ì—°ì† ì—ëŸ¬: {consecutive_errors}/{self.max_page_errors}): {e}"
                )

                if consecutive_errors < self.max_page_errors:
                    await asyncio.sleep(2 * consecutive_errors)  # ì ì§„ì  ì§€ì—°

            except Exception as e:
                consecutive_errors += 1
                logger.error(f"í˜ì´ì§€ {page + 1} ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

                if consecutive_errors < self.max_page_errors:
                    await asyncio.sleep(1)

        if consecutive_errors >= self.max_page_errors:
            logger.error(f"ì—°ì† í˜ì´ì§€ ì—ëŸ¬ í•œê³„ ë„ë‹¬ ({self.max_page_errors})")

        # ì¤‘ë³µ ì œê±°
        unique_ips = self.data_processor.remove_duplicates(all_ips)
        logger.info(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ìˆ˜ì§‘: {len(unique_ips)}ê°œ IP")

        return unique_ips

    def _transform_data(self, raw_data: dict) -> dict:
        """ë°ì´í„° ë³€í™˜ - í—¬í¼ ëª¨ë“ˆ ìœ„ì„"""
        return self.data_transform.transform_data(raw_data)

    def collect_from_web(
        self, start_date: str = None, end_date: str = None
    ) -> Dict[str, Any]:
        """
        ì›¹ ìˆ˜ì§‘ ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œ (ë™ê¸° ë˜í¼)
        collection_service.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ì¸í„°í˜ì´ìŠ¤
        """
        import asyncio

        try:
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            if not start_date or not end_date:
                end_date = datetime.now().strftime("%Y-%m-%d")
                start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")

            # ë¹„ë™ê¸° ìˆ˜ì§‘ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                collected_data = loop.run_until_complete(self._collect_data())
                return {
                    "success": True,
                    "data": collected_data,
                    "count": len(collected_data),
                    "message": f"REGTECHì—ì„œ {len(collected_data)}ê°œ IP ìˆ˜ì§‘ ì™„ë£Œ",
                }
            finally:
                loop.close()

        except Exception as e:
            logger.error(f"REGTECH ì›¹ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "data": [],
                "count": 0,
                "error": str(e),
                "message": f"REGTECH ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}",
            }


if __name__ == "__main__":
    # ëª¨ë“ˆí™”ëœ REGTECH ì»´ë ‰í„° í…ŒìŠ¤íŠ¸
    import sys

    all_validation_failures = []
    total_tests = 0

    # Test 1: ê¸°ë³¸ ì»´ë ‰í„° ìƒì„±
    total_tests += 1
    try:
        from .unified_collector import CollectionConfig

        config = CollectionConfig()
        collector = RegtechCollector(config)
        if not hasattr(collector, "auth") or not hasattr(collector, "data_processor"):
            all_validation_failures.append("í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸ ëˆ„ë½")
    except Exception as e:
        all_validation_failures.append(f"ì»´ë ‰í„° ìƒì„± ì‹¤íŒ¨: {e}")

    # Test 2: ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
    total_tests += 1
    try:
        from .unified_collector import CollectionConfig

        config = CollectionConfig()
        collector = RegtechCollector(config)
        required_methods = [
            "_collect_data",
            "_collect_with_cookies",
            "collect_from_web",
        ]
        for method_name in required_methods:
            if not hasattr(collector, method_name):
                all_validation_failures.append(f"í•„ìˆ˜ ë©”ì„œë“œ ëˆ„ë½: {method_name}")
    except Exception as e:
        all_validation_failures.append(f"ë©”ì„œë“œ í™•ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    # Test 3: ì¿ í‚¤ ì„¤ì • í…ŒìŠ¤íŠ¸
    total_tests += 1
    try:
        from .unified_collector import CollectionConfig

        config = CollectionConfig()
        collector = RegtechCollector(config)
        collector.set_cookie_string("test_cookie=test_value")
        # ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ë˜ë©´ ì„±ê³µ
    except Exception as e:
        all_validation_failures.append(f"ì¿ í‚¤ ì„¤ì • í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

    # ìµœì¢… ê²€ì¦ ê²°ê³¼
    if all_validation_failures:
        print(
            f"âŒ VALIDATION FAILED - {len(all_validation_failures)} of {total_tests} tests failed:"
        )
        for failure in all_validation_failures:
            print(f"  - {failure}")
        sys.exit(1)
    else:
        print(
            f"âœ… VALIDATION PASSED - All {total_tests} tests produced expected results"
        )
        print("Modularized RegtechCollector is validated and ready for use")
        sys.exit(0)
