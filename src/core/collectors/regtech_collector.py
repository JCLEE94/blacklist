#!/usr/bin/env python3
"""
í–¥ìƒëœ REGTECH ìˆ˜ì§‘ê¸° - BaseCollector ìƒì† ë° ê°•í™”ëœ ì—ëŸ¬ í•¸ë“¤ë§
"""

import asyncio
import logging
import os
import json
import time
from datetime import datetime
from typing import Any, Dict, List

import requests
from bs4 import BeautifulSoup

from ..common.ip_utils import IPUtils
from .helpers.data_transform import RegtechDataTransform
from .helpers.request_utils import RegtechRequestUtils
from .helpers.validation_utils import RegtechValidationUtils
from .unified_collector import BaseCollector, CollectionConfig

logger = logging.getLogger(__name__)


class RegtechCollector(BaseCollector):
    """
    BaseCollectorë¥¼ ìƒì†ë°›ì€ REGTECH ìˆ˜ì§‘ê¸°
    ê°•í™”ëœ ì—ëŸ¬ í•¸ë“¤ë§ê³¼ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨
    """

    def __init__(self, config: CollectionConfig):
        super().__init__("REGTECH", config)

        # ê¸°ë³¸ ì„¤ì •
        self.base_url = "https://regtech.fsec.or.kr"
        self.config_data = {}
        
        # ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦ ì„¤ì •
        self.cookie_string = None
        self.session_cookies = {}
        self.cookie_file_path = 'regtech_cookies.json'
        self.auto_extract_cookies = True  # ìë™ ì¿ í‚¤ ì¶”ì¶œ í™œì„±í™”
        
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ë¡œë“œ
        self.username = os.getenv('REGTECH_USERNAME')
        self.password = os.getenv('REGTECH_PASSWORD')
        
        # ì¿ í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ë˜ëŠ” íŒŒì¼ì—ì„œ)
        self.cookie_string = os.getenv('REGTECH_COOKIES', '')
        
        # ì €ì¥ëœ ì¿ í‚¤ ë¡œë“œ ì‹œë„
        if not self.cookie_string:
            self.cookie_string = self._load_saved_cookies()
        
        # DBì—ì„œ ì„¤ì • ë¡œë“œ (ì„ íƒì )
        try:
            from ..database.collection_settings import CollectionSettingsDB
            self.db = CollectionSettingsDB()
            
            # DBì—ì„œ REGTECH ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            source_config = self.db.get_source_config("regtech")
            credentials = self.db.get_credentials("regtech")
            
            if source_config:
                self.base_url = source_config.get("base_url", self.base_url)
                self.config_data = source_config.get("config", {})
            
            if credentials:
                self.username = credentials["username"]
                self.password = credentials["password"]
            else:
                # í™˜ê²½ë³€ìˆ˜ fallback
                self.username = os.getenv("REGTECH_USERNAME")
                self.password = os.getenv("REGTECH_PASSWORD")
                
        except ImportError:
            # DB ì—†ìœ¼ë©´ ê¸°ë³¸ê°’/í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©
            self.base_url = "https://regtech.fsec.or.kr"
            self.username = os.getenv("REGTECH_USERNAME")
            self.password = os.getenv("REGTECH_PASSWORD")
            self.config_data = {}

        # ì—ëŸ¬ í•¸ë“¤ë§ ì„¤ì •
        self.max_page_errors = 5  # ì—°ì† í˜ì´ì§€ ì—ëŸ¬ í—ˆìš© íšŸìˆ˜
        self.session_retry_limit = 3  # ì„¸ì…˜ ì¬ì‹œë„ íšŸìˆ˜
        self.request_timeout = 30  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        self.page_delay = 1  # í˜ì´ì§€ ê°„ ì§€ì—° (ì´ˆ)

        # ìƒíƒœ ì¶”ì 
        self.current_session = None
        self.page_error_count = 0
        self.total_collected = 0
        self.cookie_auth_mode = False  # ì¿ í‚¤ ì¸ì¦ ëª¨ë“œ

        # Helper ê°ì²´ë“¤ ì´ˆê¸°í™”
        self.request_utils = RegtechRequestUtils(self.base_url, self.request_timeout)
        self.data_transform = RegtechDataTransform()
        self.validation_utils = RegtechValidationUtils()
        self.validation_utils.set_ip_utils(IPUtils)

        # ì¿ í‚¤ê°€ ìˆìœ¼ë©´ ì¿ í‚¤ ëª¨ë“œ, ì—†ìœ¼ë©´ ìë™ ì¶”ì¶œ ì‹œë„
        if self.cookie_string:
            self.cookie_auth_mode = True
            self._parse_cookie_string()
            logger.info("REGTECH collector initialized in cookie mode")
        elif self.auto_extract_cookies and self.username and self.password:
            logger.info("No cookies found - will attempt automatic extraction")
            self.cookie_auth_mode = False  # ì²˜ìŒì—ëŠ” False, ì¶”ì¶œ ì„±ê³µ ì‹œ Trueë¡œ ë³€ê²½
        elif not self.username or not self.password:
            logger.warning("No REGTECH credentials or cookies provided - collector may not work")
        else:
            logger.info("REGTECH collector initialized in login mode")

    def _parse_cookie_string(self):
        """ì¿ í‚¤ ë¬¸ìì—´ íŒŒì‹±"""
        if not self.cookie_string:
            return
        
        for cookie in self.cookie_string.split(';'):
            cookie = cookie.strip()
            if '=' in cookie:
                name, value = cookie.split('=', 1)
                self.session_cookies[name.strip()] = value.strip()
        
        logger.info(f"Parsed {len(self.session_cookies)} cookies")

    def _load_saved_cookies(self) -> str:
        """ì €ì¥ëœ ì¿ í‚¤ íŒŒì¼ì—ì„œ ë¡œë“œ"""
        try:
            if os.path.exists(self.cookie_file_path):
                with open(self.cookie_file_path, 'r') as f:
                    data = json.load(f)
                    cookie_string = data.get('cookie_string', '')
                    if cookie_string:
                        # ì¿ í‚¤ ìœ íš¨ì„± ê°„ë‹¨ ì²´í¬ (ìƒì„± ì‹œê°„ í™•ì¸)
                        extracted_at = data.get('extracted_at', '')
                        if extracted_at:
                            logger.info(f"Loaded saved cookies from {extracted_at}")
                        return cookie_string
        except Exception as e:
            logger.debug(f"Failed to load saved cookies: {e}")
        return ''

    def _save_cookies(self, cookie_string: str, method: str = 'auto_extracted'):
        """ì¿ í‚¤ë¥¼ íŒŒì¼ì— ì €ì¥"""
        try:
            cookie_data = {
                'cookie_string': cookie_string,
                'cookies': self.session_cookies,
                'extracted_at': datetime.now().isoformat(),
                'method': method,
                'username': self.username
            }
            with open(self.cookie_file_path, 'w') as f:
                json.dump(cookie_data, f, indent=2)
            logger.info(f"Cookies saved to {self.cookie_file_path}")
        except Exception as e:
            logger.error(f"Failed to save cookies: {e}")

    def _extract_cookies_with_playwright(self) -> str:
        """Playwrightë¡œ ìë™ ì¿ í‚¤ ì¶”ì¶œ"""
        try:
            from playwright.sync_api import sync_playwright
            
            logger.info("ğŸª Extracting cookies with Playwright...")
            
            with sync_playwright() as p:
                # ë¸Œë¼ìš°ì € ì‹¤í–‰ (headless ëª¨ë“œ)
                browser = p.chromium.launch(headless=True)
                context = browser.new_context()
                page = context.new_page()
                
                # 1. ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì†
                logger.info("Accessing REGTECH login page...")
                page.goto(f'{self.base_url}/login/loginForm')
                page.wait_for_load_state('networkidle')
                
                # 2. ìë™ ë¡œê·¸ì¸
                logger.info("Attempting automatic login...")
                page.fill('input[name="loginID"]', self.username)
                page.fill('input[name="loginPW"]', self.password)
                
                # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
                page.click('button[type="submit"], input[type="submit"]')
                page.wait_for_load_state('networkidle')
                
                # 3. ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
                current_url = page.url
                if 'login' not in current_url.lower():
                    logger.info("âœ… Login successful!")
                    
                    # 4. ì¿ í‚¤ ì¶”ì¶œ
                    logger.info("Extracting cookies...")
                    cookies = context.cookies()
                    
                    important_cookies = {}
                    for cookie in cookies:
                        if cookie['name'] in ['JSESSIONID', 'regtech-front']:
                            important_cookies[cookie['name']] = cookie['value']
                    
                    if important_cookies:
                        cookie_string = '; '.join([f"{name}={value}" for name, value in important_cookies.items()])
                        
                        browser.close()
                        logger.info(f"âœ… Cookies extracted successfully: {len(important_cookies)} cookies")
                        return cookie_string
                    else:
                        logger.warning("âŒ No important cookies found")
                else:
                    logger.warning("âŒ Login failed - still on login page")
                
                browser.close()
                return ''
                
        except ImportError:
            logger.warning("Playwright not available - install with: pip install playwright && playwright install chromium")
            return ''
        except Exception as e:
            logger.error(f"Cookie extraction with Playwright failed: {e}")
            return ''

    def _extract_cookies_with_selenium(self) -> str:
        """Seleniumìœ¼ë¡œ ìë™ ì¿ í‚¤ ì¶”ì¶œ (fallback)"""
        try:
            from selenium import webdriver
            from selenium.webdriver.common.by import By
            from selenium.webdriver.chrome.options import Options
            
            logger.info("ğŸª Extracting cookies with Selenium...")
            
            # Chrome ì˜µì…˜ (headless)
            chrome_options = Options()
            chrome_options.add_argument('--headless')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            
            driver = webdriver.Chrome(options=chrome_options)
            
            try:
                # 1. ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì†
                logger.info("Accessing REGTECH login page...")
                driver.get(f'{self.base_url}/login/loginForm')
                time.sleep(2)
                
                # 2. ìë™ ë¡œê·¸ì¸
                logger.info("Attempting automatic login...")
                driver.find_element(By.NAME, 'loginID').send_keys(self.username)
                driver.find_element(By.NAME, 'loginPW').send_keys(self.password)
                
                # ë¡œê·¸ì¸ ë²„íŠ¼ í´ë¦­
                driver.find_element(By.CSS_SELECTOR, 'button[type="submit"], input[type="submit"]').click()
                time.sleep(3)
                
                # 3. ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸ ë° ì¿ í‚¤ ì¶”ì¶œ
                current_url = driver.current_url
                if 'login' not in current_url.lower():
                    logger.info("âœ… Login successful!")
                    
                    logger.info("Extracting cookies...")
                    cookies = driver.get_cookies()
                    
                    important_cookies = {}
                    for cookie in cookies:
                        if cookie['name'] in ['JSESSIONID', 'regtech-front']:
                            important_cookies[cookie['name']] = cookie['value']
                    
                    if important_cookies:
                        cookie_string = '; '.join([f"{name}={value}" for name, value in important_cookies.items()])
                        logger.info(f"âœ… Cookies extracted successfully: {len(important_cookies)} cookies")
                        return cookie_string
                else:
                    logger.warning("âŒ Login failed - still on login page")
                    
            finally:
                driver.quit()
                
        except ImportError:
            logger.warning("Selenium not available - install with: pip install selenium")
            return ''
        except Exception as e:
            logger.error(f"Cookie extraction with Selenium failed: {e}")
            return ''
        
        return ''

    def _auto_extract_cookies(self) -> bool:
        """ìë™ ì¿ í‚¤ ì¶”ì¶œ (Playwright ìš°ì„ , Selenium fallback)"""
        if not self.auto_extract_cookies or not self.username or not self.password:
            return False
        
        logger.info("ğŸ”„ Attempting automatic cookie extraction...")
        
        # 1. Playwright ì‹œë„
        cookie_string = self._extract_cookies_with_playwright()
        
        # 2. Playwright ì‹¤íŒ¨ ì‹œ Selenium ì‹œë„
        if not cookie_string:
            logger.info("Playwright failed, trying Selenium...")
            cookie_string = self._extract_cookies_with_selenium()
        
        # 3. ì¶”ì¶œ ì„±ê³µ ì‹œ ì„¤ì •
        if cookie_string:
            self.cookie_string = cookie_string
            self.cookie_auth_mode = True
            self._parse_cookie_string()
            self._save_cookies(cookie_string, 'auto_extracted')
            logger.info("âœ… Automatic cookie extraction successful - switched to cookie mode")
            return True
        else:
            logger.error("âŒ All cookie extraction methods failed")
            return False

    def _is_cookie_expired(self, response) -> bool:
        """ì¿ í‚¤ ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        if not response:
            return True
            
        # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ í™•ì¸
        if response.status_code == 302:
            location = response.headers.get('Location', '')
            if 'login' in location.lower():
                return True
        
        # í˜„ì¬ URLì´ ë¡œê·¸ì¸ í˜ì´ì§€ì¸ì§€ í™•ì¸
        if 'login' in response.url.lower():
            return True
            
        # ì¸ì¦ ì˜¤ë¥˜ ì‘ë‹µ í™•ì¸
        if response.status_code in [401, 403]:
            return True
            
        return False

    def set_cookie_string(self, cookie_string: str):
        """ì™¸ë¶€ì—ì„œ ì¿ í‚¤ ë¬¸ìì—´ ì„¤ì •"""
        self.cookie_string = cookie_string
        self.cookie_auth_mode = True
        self._parse_cookie_string()
        self._save_cookies(cookie_string, 'manual')
        logger.info("Cookie string updated - switched to cookie mode")

    @property
    def source_type(self) -> str:
        return "REGTECH"

    async def _collect_data(self) -> List[Any]:
        """
        ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ ë©”ì„œë“œ - ìë™ ì¿ í‚¤ ê´€ë¦¬ í¬í•¨
        """
        # 1. ì¿ í‚¤ê°€ ì—†ìœ¼ë©´ ìë™ ì¶”ì¶œ ì‹œë„
        if not self.cookie_auth_mode and self.auto_extract_cookies:
            logger.info("ğŸ”„ No cookies available - attempting automatic extraction...")
            if self._auto_extract_cookies():
                logger.info("âœ… Automatic cookie extraction successful")
            else:
                logger.warning("âŒ Automatic cookie extraction failed - falling back to login mode")
                return await self._collect_with_login()
        
        # 2. ì¿ í‚¤ ê¸°ë°˜ ìˆ˜ì§‘ ì‹œë„
        if self.cookie_auth_mode:
            collected_data = await self._collect_with_cookies()
            
            # 3. ìˆ˜ì§‘ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì¿ í‚¤ ë§Œë£Œ ì˜ì‹¬ ì‹œ ì¬ì¶”ì¶œ ì‹œë„
            if not collected_data and self.auto_extract_cookies:
                logger.warning("ğŸ”„ No data collected - cookies might be expired, attempting re-extraction...")
                if self._auto_extract_cookies():
                    logger.info("âœ… Cookie re-extraction successful - retrying collection...")
                    collected_data = await self._collect_with_cookies()
                else:
                    logger.error("âŒ Cookie re-extraction failed - falling back to login mode")
                    return await self._collect_with_login()
            
            return collected_data
        else:
            return await self._collect_with_login()

    async def _collect_with_cookies(self) -> List[Any]:
        """ì¿ í‚¤ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘"""
        collected_ips = []
        
        try:
            # ì„¸ì…˜ ìƒì„± ë° ì¿ í‚¤ ì„¤ì •
            session = requests.Session()
            
            # ì¿ í‚¤ ì„¤ì •
            for name, value in self.session_cookies.items():
                session.cookies.set(name, value)
            
            # í—¤ë” ì„¤ì •
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Referer': f'{self.base_url}/',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8'
            })
            
            logger.info("Starting cookie-based data collection")
            
            # ë¸”ë™ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë“¤ ì‹œë„
            blacklist_urls = [
                '/board/boardList?menuCode=HPHB0620101',  # ì•…ì„±IPì°¨ë‹¨
                '/board/excelDownload?menuCode=HPHB0620101',  # Excel ë‹¤ìš´ë¡œë“œ
                '/threat/blacklist/list',
                '/api/blacklist/search'
            ]
            
            for path in blacklist_urls:
                try:
                    url = f"{self.base_url}{path}"
                    logger.info(f"Trying URL: {url}")
                    
                    response = session.get(url, verify=False, timeout=self.request_timeout)
                    
                    # ì¿ í‚¤ ë§Œë£Œ í™•ì¸
                    if self._is_cookie_expired(response):
                        logger.warning(f"Cookies expired at {url} - will trigger re-extraction")
                        return []  # ë¹ˆ ê²°ê³¼ ë°˜í™˜í•˜ì—¬ ìƒìœ„ì—ì„œ ì¬ì¶”ì¶œ íŠ¸ë¦¬ê±°
                    
                    if response.status_code == 200:
                        content_type = response.headers.get('content-type', '').lower()
                        
                        # Excel íŒŒì¼ ì²˜ë¦¬
                        if 'excel' in content_type or 'spreadsheet' in content_type:
                            ips = await self._process_excel_response(response)
                            if ips:
                                collected_ips.extend(ips)
                                logger.info(f"Collected {len(ips)} IPs from Excel download")
                                break
                        
                        # HTML í˜ì´ì§€ ì²˜ë¦¬
                        elif 'text/html' in content_type:
                            ips = await self._process_html_response(response)
                            if ips:
                                collected_ips.extend(ips)
                                logger.info(f"Collected {len(ips)} IPs from HTML page")
                                if len(ips) > 10:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¤‘ë‹¨
                                    break
                        
                        # JSON ì‘ë‹µ ì²˜ë¦¬
                        elif 'application/json' in content_type:
                            ips = await self._process_json_response(response)
                            if ips:
                                collected_ips.extend(ips)
                                logger.info(f"Collected {len(ips)} IPs from JSON API")
                                break
                    
                    elif response.status_code == 302 and 'login' in response.headers.get('Location', ''):
                        logger.warning("Redirected to login - cookies may be expired")
                        break
                    
                except Exception as e:
                    logger.error(f"Error accessing {path}: {e}")
                    continue
            
            # ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦ ë° ë³€í™˜
            if collected_ips:
                validated_ips = []
                for ip_data in collected_ips:
                    if self.validation_utils.validate_ip_data(ip_data):
                        validated_ips.append(ip_data)
                
                logger.info(f"Validated {len(validated_ips)} out of {len(collected_ips)} collected IPs")
                return validated_ips
            else:
                logger.warning("No IPs collected - check cookies or access permissions")
                return []
                
        except Exception as e:
            logger.error(f"Cookie-based collection failed: {e}")
            return []

    async def _process_excel_response(self, response) -> List[Dict[str, Any]]:
        """Excel ì‘ë‹µ ì²˜ë¦¬"""
        try:
            import pandas as pd
            from io import BytesIO
            
            # Excel íŒŒì¼ ì €ì¥
            filename = f"regtech_blacklist_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            with open(filename, 'wb') as f:
                f.write(response.content)
            
            # Excel íŒŒì¼ ì½ê¸°
            df = pd.read_excel(BytesIO(response.content))
            
            # IP ì»¬ëŸ¼ ì°¾ê¸°
            ip_columns = [col for col in df.columns if 'ip' in str(col).lower() or 'IP' in str(col)]
            
            if not ip_columns:
                # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì´ IPì¼ ê°€ëŠ¥ì„±
                ip_columns = [df.columns[0]]
            
            ips = []
            for _, row in df.iterrows():
                for ip_col in ip_columns:
                    ip_value = str(row[ip_col]).strip()
                    if self.validation_utils.is_valid_ip(ip_value):
                        ips.append({
                            'ip': ip_value,
                            'source': 'REGTECH',
                            'threat_level': 'medium',
                            'detection_date': datetime.now().strftime('%Y-%m-%d'),
                            'method': 'excel_download',
                            'description': f'Blacklisted IP from REGTECH Excel export'
                        })
                        break
            
            logger.info(f"Processed Excel file: {filename}, extracted {len(ips)} IPs")
            return ips
            
        except ImportError:
            logger.warning("pandas not available - cannot process Excel files")
            return []
        except Exception as e:
            logger.error(f"Error processing Excel response: {e}")
            return []

    async def _process_html_response(self, response) -> List[Dict[str, Any]]:
        """HTML ì‘ë‹µ ì²˜ë¦¬"""
        try:
            import re
            
            # IP íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œ
            ip_pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}\b'
            ips_found = re.findall(ip_pattern, response.text)
            
            ips = []
            for ip in set(ips_found):  # ì¤‘ë³µ ì œê±°
                if self.validation_utils.is_valid_ip(ip):
                    ips.append({
                        'ip': ip,
                        'source': 'REGTECH',
                        'threat_level': 'medium',
                        'detection_date': datetime.now().strftime('%Y-%m-%d'),
                        'method': 'html_parsing',
                        'description': f'Blacklisted IP from REGTECH web page'
                    })
            
            # BeautifulSoupë¡œ í…Œì´ë¸” íŒŒì‹± ì‹œë„
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # í…Œì´ë¸”ì—ì„œ IP ì¶”ì¶œ
                tables = soup.find_all('table')
                for table in tables:
                    rows = table.find_all('tr')
                    for row in rows[1:]:  # í—¤ë” ì œì™¸
                        cells = row.find_all(['td', 'th'])
                        for cell in cells:
                            text = cell.get_text().strip()
                            if re.match(ip_pattern, text) and self.validation_utils.is_valid_ip(text):
                                # ì¤‘ë³µ í™•ì¸
                                if not any(ip_data['ip'] == text for ip_data in ips):
                                    ips.append({
                                        'ip': text,
                                        'source': 'REGTECH',
                                        'threat_level': 'medium',
                                        'detection_date': datetime.now().strftime('%Y-%m-%d'),
                                        'method': 'table_parsing',
                                        'description': f'Blacklisted IP from REGTECH table'
                                    })
            except:
                pass  # BeautifulSoup íŒŒì‹± ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ regex ê²°ê³¼ ì‚¬ìš©
            
            return ips[:100]  # ìµœëŒ€ 100ê°œë¡œ ì œí•œ
            
        except Exception as e:
            logger.error(f"Error processing HTML response: {e}")
            return []

    async def _process_json_response(self, response) -> List[Dict[str, Any]]:
        """JSON ì‘ë‹µ ì²˜ë¦¬"""
        try:
            data = response.json()
            ips = []
            
            # ë‹¤ì–‘í•œ JSON êµ¬ì¡° ì²˜ë¦¬
            if isinstance(data, dict):
                # ë°ì´í„° ë°°ì—´ ì°¾ê¸°
                items = None
                for key in ['data', 'items', 'list', 'blacklist', 'ips', 'results']:
                    if key in data and isinstance(data[key], list):
                        items = data[key]
                        break
                
                if items:
                    for item in items:
                        if isinstance(item, dict):
                            # IP í•„ë“œ ì°¾ê¸°
                            ip_value = None
                            for ip_key in ['ip', 'ipAddress', 'target_ip', 'source_ip', 'addr']:
                                if ip_key in item:
                                    ip_value = str(item[ip_key]).strip()
                                    break
                            
                            if ip_value and self.validation_utils.is_valid_ip(ip_value):
                                ips.append({
                                    'ip': ip_value,
                                    'source': 'REGTECH',
                                    'threat_level': item.get('threat_level', 'medium'),
                                    'detection_date': item.get('date', datetime.now().strftime('%Y-%m-%d')),
                                    'method': 'json_api',
                                    'description': item.get('description', 'Blacklisted IP from REGTECH API')
                                })
                        elif isinstance(item, str) and self.validation_utils.is_valid_ip(item):
                            ips.append({
                                'ip': item,
                                'source': 'REGTECH',
                                'threat_level': 'medium',
                                'detection_date': datetime.now().strftime('%Y-%m-%d'),
                                'method': 'json_api',
                                'description': 'Blacklisted IP from REGTECH API'
                            })
            
            elif isinstance(data, list):
                for item in data:
                    if isinstance(item, str) and self.validation_utils.is_valid_ip(item):
                        ips.append({
                            'ip': item,
                            'source': 'REGTECH',
                            'threat_level': 'medium',
                            'detection_date': datetime.now().strftime('%Y-%m-%d'),
                            'method': 'json_api',
                            'description': 'Blacklisted IP from REGTECH API'
                        })
            
            return ips
            
        except Exception as e:
            logger.error(f"Error processing JSON response: {e}")
            return []

    async def _collect_with_login(self) -> List[Any]:
        """ê¸°ì¡´ ë¡œê·¸ì¸ ê¸°ë°˜ ë°ì´í„° ìˆ˜ì§‘"""
        collected_ips = []
        session_retry_count = 0

        while session_retry_count < self.session_retry_limit:
            try:
                # ì„¸ì…˜ ì´ˆê¸°í™”
                session = self.request_utils.create_session()
                self.current_session = session

                # ë¡œê·¸ì¸ ì‹œë„
                if not await self._robust_login(session):
                    raise Exception("ë¡œê·¸ì¸ ì‹¤íŒ¨ í›„ ì¬ì‹œë„ í•œê³„ ë„ë‹¬")

                # ë°ì´í„° ìˆ˜ì§‘
                start_date, end_date = self.data_transform.get_date_range(self.config)
                collected_ips = await self._robust_collect_ips(
                    session, start_date, end_date
                )

                # ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘ ì™„ë£Œ
                self.logger.info(f"REGTECH ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_ips)}ê°œ IP")
                break

            except requests.exceptions.ConnectionError as e:
                session_retry_count += 1
                self.logger.warning(
                    f"ì—°ê²° ì˜¤ë¥˜ (ì¬ì‹œë„ {session_retry_count}/{self.session_retry_limit}): {e}"
                )
                if session_retry_count < self.session_retry_limit:
                    await asyncio.sleep(5 * session_retry_count)  # ì§€ìˆ˜ì  ë°±ì˜¤í”„

            except requests.exceptions.Timeout as e:
                session_retry_count += 1
                self.logger.warning(
                    f"íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ (ì¬ì‹œë„ {session_retry_count}/{self.session_retry_limit}): {e}"
                )
                if session_retry_count < self.session_retry_limit:
                    await asyncio.sleep(3 * session_retry_count)

            except Exception as e:
                self.logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                session_retry_count += 1
                if session_retry_count < self.session_retry_limit:
                    await asyncio.sleep(2 * session_retry_count)

            finally:
                if hasattr(self, "current_session") and self.current_session:
                    self.current_session.close()
                    self.current_session = None

        if session_retry_count >= self.session_retry_limit:
            raise Exception(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ({self.session_retry_limit}) ì´ˆê³¼")

        return collected_ips

    async def _robust_login(self, session: requests.Session) -> bool:
        """ê°•í™”ëœ ë¡œê·¸ì¸ ë¡œì§"""
        login_attempts = 0
        max_login_attempts = 3

        while login_attempts < max_login_attempts:
            try:
                self.logger.info(
                    f"ë¡œê·¸ì¸ ì‹œë„ {login_attempts + 1}/{max_login_attempts}"
                )

                # ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ê·¼
                login_page_url = f"{self.base_url}/login/loginForm"
                response = session.get(login_page_url)

                if response.status_code != 200:
                    raise Exception(f"ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨: {response.status_code}")

                # CSRF í† í°ì´ë‚˜ ìˆ¨ê²¨ì§„ í•„ë“œ ì¶”ì¶œ ì‹œë„
                soup = BeautifulSoup(response.text, "html.parser")
                hidden_inputs = soup.find_all("input", type="hidden")
                login_data = {
                    "username": self.username,
                    "password": self.password,
                    "login_error": "",
                    "txId": "",
                    "token": "",
                    "memberId": "",
                    "smsTimeExcess": "N",
                }

                # ìˆ¨ê²¨ì§„ í•„ë“œë“¤ ì¶”ê°€
                for hidden_input in hidden_inputs:
                    name = hidden_input.get("name")
                    value = hidden_input.get("value", "")
                    if name:
                        login_data[name] = value

                # ë¡œê·¸ì¸ ìš”ì²­
                login_url = f"{self.base_url}/login/addLogin"
                login_response = session.post(
                    login_url,
                    data=login_data,
                    headers={"Content-Type": "application/x-www-form-urlencoded"},
                    allow_redirects=True,
                )

                # ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
                if self.validation_utils.verify_login_success(login_response):
                    self.logger.info("ë¡œê·¸ì¸ ì„±ê³µ")
                    return True
                else:
                    login_attempts += 1
                    if login_attempts < max_login_attempts:
                        self.logger.warning(f"ë¡œê·¸ì¸ ì‹¤íŒ¨, {2} ì´ˆ í›„ ì¬ì‹œë„")
                        await asyncio.sleep(2)

            except Exception as e:
                login_attempts += 1
                self.logger.error(f"ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                if login_attempts < max_login_attempts:
                    await asyncio.sleep(3)

        self.logger.error("ë¡œê·¸ì¸ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
        return False

    async def _robust_collect_ips(
        self, session: requests.Session, start_date: str, end_date: str
    ) -> List[Dict[str, Any]]:
        """ê°•í™”ëœ IP ìˆ˜ì§‘ ë¡œì§"""
        all_ips = []
        page = 0
        consecutive_errors = 0
        max_pages = 100  # ì•ˆì „ì¥ì¹˜

        self.logger.info(f"IP ìˆ˜ì§‘ ì‹œì‘: {start_date} ~ {end_date}")

        while page < max_pages and consecutive_errors < self.max_page_errors:
            try:
                # ì·¨ì†Œ ìš”ì²­ í™•ì¸
                if self.validation_utils.should_cancel(
                    getattr(self, "_cancel_event", None)
                ):
                    self.logger.info("ì‚¬ìš©ì ì·¨ì†Œ ìš”ì²­ìœ¼ë¡œ ìˆ˜ì§‘ ì¤‘ë‹¨")
                    break

                # í˜ì´ì§€ ì§€ì—°
                if page > 0:
                    await asyncio.sleep(self.page_delay)

                # í˜ì´ì§€ ë°ì´í„° ìˆ˜ì§‘
                page_ips = await self.request_utils.collect_single_page(
                    session, page, start_date, end_date
                )

                # IP ìœ íš¨ì„± ê²€ì‚¬ ì ìš©
                valid_page_ips = []
                for ip_data in page_ips:
                    if self.validation_utils.is_valid_ip(ip_data.get("ip", "")):
                        valid_page_ips.append(ip_data)
                page_ips = valid_page_ips

                if not page_ips:
                    self.logger.info(
                        f"í˜ì´ì§€ {page + 1}ì—ì„œ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ, ìˆ˜ì§‘ ì¢…ë£Œ"
                    )
                    break

                all_ips.extend(page_ips)
                consecutive_errors = 0  # ì„±ê³µ ì‹œ ì—ëŸ¬ ì¹´ìš´íŠ¸ ë¦¬ì…‹

                self.logger.info(
                    f"í˜ì´ì§€ {page + 1}: {len(page_ips)}ê°œ ìˆ˜ì§‘ (ì´ {len(all_ips)}ê°œ)"
                )
                page += 1

            except requests.exceptions.RequestException as e:
                consecutive_errors += 1
                self.logger.warning(
                    f"í˜ì´ì§€ {page + 1} ìˆ˜ì§‘ ì‹¤íŒ¨ (ì—°ì† ì—ëŸ¬: {consecutive_errors}/{self.max_page_errors}): {e}"
                )

                if consecutive_errors < self.max_page_errors:
                    await asyncio.sleep(2 * consecutive_errors)  # ì ì§„ì  ì§€ì—°

            except Exception as e:
                consecutive_errors += 1
                self.logger.error(f"í˜ì´ì§€ {page + 1} ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")

                if consecutive_errors < self.max_page_errors:
                    await asyncio.sleep(1)

        if consecutive_errors >= self.max_page_errors:
            self.logger.error(f"ì—°ì† í˜ì´ì§€ ì—ëŸ¬ í•œê³„ ë„ë‹¬ ({self.max_page_errors})")

        # ì¤‘ë³µ ì œê±°
        unique_ips = self.data_transform.remove_duplicates(all_ips)
        self.logger.info(f"ì¤‘ë³µ ì œê±° í›„ ìµœì¢… ìˆ˜ì§‘: {len(unique_ips)}ê°œ IP")

        return unique_ips

    def _transform_data(self, raw_data: dict) -> dict:
        """ë°ì´í„° ë³€í™˜ - í—¬í¼ ëª¨ë“ˆ ìœ„ì„"""
        return self.data_transform.transform_data(raw_data)

    def collect_from_web(self, start_date: str = None, end_date: str = None) -> Dict[str, Any]:
        """
        ì›¹ ìˆ˜ì§‘ ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œ (ë™ê¸° ë˜í¼)
        collection_service.pyì—ì„œ í˜¸ì¶œí•˜ëŠ” ì¸í„°í˜ì´ìŠ¤
        """
        import asyncio
        
        try:
            # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
            if not start_date or not end_date:
                from datetime import datetime, timedelta
                end_date = datetime.now().strftime('%Y-%m-%d')
                start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
            
            # ë¹„ë™ê¸° ìˆ˜ì§‘ ì‹¤í–‰
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            
            try:
                collected_data = loop.run_until_complete(self._collect_data())
                return {
                    "success": True,
                    "data": collected_data,
                    "count": len(collected_data),
                    "message": f"REGTECHì—ì„œ {len(collected_data)}ê°œ IP ìˆ˜ì§‘ ì™„ë£Œ"
                }
            finally:
                loop.close()
                
        except Exception as e:
            self.logger.error(f"REGTECH ì›¹ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "data": [],
                "count": 0,
                "error": str(e),
                "message": f"REGTECH ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}"
            }
