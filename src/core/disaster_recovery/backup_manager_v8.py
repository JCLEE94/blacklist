"""
ğŸš€ AI ìë™í™” í”Œë«í¼ v8.3.0 - Enterprise Backup & Disaster Recovery System
Automated backup and disaster recovery procedures with Korean reporting

Features:
- Automated daily/hourly backup scheduling
- Multi-tier backup strategy (Database, Files, Config, Git)
- Point-in-time recovery capability
- Automated integrity verification
- Korean language disaster recovery reporting
- Zero-downtime backup operations
- Automated retention policies
"""

import asyncio
import gzip
import hashlib
import json
import logging
import os
import shutil
import sqlite3
import subprocess
import tarfile
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import psutil

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BackupType(Enum):
    FULL = "ì „ì²´ë°±ì—…"
    INCREMENTAL = "ì¦ë¶„ë°±ì—…"
    DIFFERENTIAL = "ì°¨ë“±ë°±ì—…"
    SNAPSHOT = "ìŠ¤ëƒ…ìƒ·"


class BackupStatus(Enum):
    PENDING = "ëŒ€ê¸°ì¤‘"
    IN_PROGRESS = "ì§„í–‰ì¤‘"
    COMPLETED = "ì™„ë£Œ"
    FAILED = "ì‹¤íŒ¨"
    VERIFIED = "ê²€ì¦ì™„ë£Œ"


class RecoveryLevel(Enum):
    MINIMAL = "ìµœì†Œë³µêµ¬"  # í•µì‹¬ ì„œë¹„ìŠ¤ë§Œ
    STANDARD = "í‘œì¤€ë³µêµ¬"  # ì¼ë°˜ì  ë³µêµ¬
    FULL = "ì™„ì „ë³µêµ¬"  # ëª¨ë“  ë°ì´í„° ë³µêµ¬
    DISASTER = "ì¬í•´ë³µêµ¬"  # ì™„ì „í•œ ì¬í•´ë³µêµ¬


@dataclass
class BackupMetadata:
    """ë°±ì—… ë©”íƒ€ë°ì´í„°"""

    backup_id: str
    backup_type: BackupType
    created_at: datetime
    size_bytes: int
    checksum: str
    status: BackupStatus
    retention_days: int
    components: List[str]
    korean_description: str
    recovery_point: datetime


@dataclass
class RecoveryPlan:
    """ì¬í•´ë³µêµ¬ ê³„íš"""

    plan_id: str
    recovery_level: RecoveryLevel
    estimated_rto: int  # Recovery Time Objective (minutes)
    estimated_rpo: int  # Recovery Point Objective (minutes)
    steps: List[str]
    korean_description: str
    prerequisites: List[str]
    validation_steps: List[str]


class EnterpriseBackupManager:
    """ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë°±ì—… ë° ì¬í•´ë³µêµ¬ ê´€ë¦¬ì"""

    def __init__(self):
        self.backup_root = Path("backups")
        self.db_path = "disaster_recovery/backup_registry.db"
        self.config_file = "config/backup_config.json"

        # ë°±ì—… ì„¤ì •
        self.retention_policies = {
            BackupType.FULL: 30,  # 30ì¼ ë³´ê´€
            BackupType.INCREMENTAL: 7,  # 7ì¼ ë³´ê´€
            BackupType.DIFFERENTIAL: 14,  # 14ì¼ ë³´ê´€
            BackupType.SNAPSHOT: 3,  # 3ì¼ ë³´ê´€
        }

        # ë°±ì—… ì»´í¬ë„ŒíŠ¸
        self.backup_components = {
            "database": {
                "path": "instance/blacklist.db",
                "type": "sqlite",
                "critical": True,
                "korean_name": "ë°ì´í„°ë² ì´ìŠ¤",
            },
            "config": {
                "path": "config/",
                "type": "directory",
                "critical": True,
                "korean_name": "ì„¤ì •íŒŒì¼",
            },
            "logs": {
                "path": "logs/",
                "type": "directory",
                "critical": False,
                "korean_name": "ë¡œê·¸íŒŒì¼",
            },
            "source": {
                "path": "src/",
                "type": "directory",
                "critical": True,
                "korean_name": "ì†ŒìŠ¤ì½”ë“œ",
            },
            "templates": {
                "path": "templates/",
                "type": "directory",
                "critical": True,
                "korean_name": "í…œí”Œë¦¿",
            },
        }

        self.backup_history: List[BackupMetadata] = []
        self.recovery_plans: Dict[RecoveryLevel, RecoveryPlan] = {}

        # ì´ˆê¸°í™”
        self._init_backup_system()

    def _init_backup_system(self):
        """ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        self.backup_root.mkdir(exist_ok=True)
        Path("disaster_recovery").mkdir(exist_ok=True)

        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self._init_database()

        # ë³µêµ¬ ê³„íš ì´ˆê¸°í™”
        self._init_recovery_plans()

        logger.info("âœ… Enterprise ë°±ì—… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _init_database(self):
        """ë°±ì—… ë ˆì§€ìŠ¤íŠ¸ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ë°±ì—… ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS backup_metadata (
                    backup_id TEXT PRIMARY KEY,
                    backup_type TEXT NOT NULL,
                    created_at DATETIME NOT NULL,
                    size_bytes INTEGER NOT NULL,
                    checksum TEXT NOT NULL,
                    status TEXT NOT NULL,
                    retention_days INTEGER NOT NULL,
                    components TEXT NOT NULL,
                    korean_description TEXT NOT NULL,
                    recovery_point DATETIME NOT NULL,
                    file_path TEXT NOT NULL
                )
            """
            )

            # ë³µêµ¬ ì´ë ¥ í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS recovery_history (
                    recovery_id TEXT PRIMARY KEY,
                    backup_id TEXT NOT NULL,
                    recovery_level TEXT NOT NULL,
                    started_at DATETIME NOT NULL,
                    completed_at DATETIME,
                    status TEXT NOT NULL,
                    korean_description TEXT NOT NULL,
                    details TEXT,
                    FOREIGN KEY (backup_id) REFERENCES backup_metadata (backup_id)
                )
            """
            )

            # ë°±ì—… ìŠ¤ì¼€ì¤„ í…Œì´ë¸”
            cursor.execute(
                """
                CREATE TABLE IF NOT EXISTS backup_schedules (
                    schedule_id TEXT PRIMARY KEY,
                    backup_type TEXT NOT NULL,
                    schedule_cron TEXT NOT NULL,
                    enabled BOOLEAN NOT NULL,
                    last_run DATETIME,
                    next_run DATETIME,
                    korean_description TEXT NOT NULL
                )
            """
            )

            conn.commit()

    def _init_recovery_plans(self):
        """ì¬í•´ë³µêµ¬ ê³„íš ì´ˆê¸°í™”"""
        self.recovery_plans = {
            RecoveryLevel.MINIMAL: RecoveryPlan(
                plan_id="minimal-recovery-v8-3-0",
                recovery_level=RecoveryLevel.MINIMAL,
                estimated_rto=15,  # 15ë¶„
                estimated_rpo=60,  # 1ì‹œê°„
                steps=[
                    "1. í•µì‹¬ ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬",
                    "2. ê¸°ë³¸ ì„¤ì •íŒŒì¼ ë³µì›",
                    "3. API ì„œë¹„ìŠ¤ ì¬ì‹œì‘",
                    "4. í—¬ìŠ¤ì²´í¬ ê²€ì¦",
                ],
                korean_description="í•µì‹¬ ì„œë¹„ìŠ¤ë§Œ ë¹ ë¥´ê²Œ ë³µêµ¬í•˜ì—¬ ê¸°ë³¸ ìš´ì˜ ì¬ê°œ",
                prerequisites=["ìµœì‹  ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… í™•ì¸", "ì„¤ì • ë°±ì—… ë¬´ê²°ì„± ê²€ì¦"],
                validation_steps=[
                    "API í—¬ìŠ¤ì²´í¬ ì„±ê³µ í™•ì¸",
                    "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒíƒœ í™•ì¸",
                    "ê¸°ë³¸ ê¸°ëŠ¥ ë™ì‘ í…ŒìŠ¤íŠ¸",
                ],
            ),
            RecoveryLevel.STANDARD: RecoveryPlan(
                plan_id="standard-recovery-v8-3-0",
                recovery_level=RecoveryLevel.STANDARD,
                estimated_rto=45,  # 45ë¶„
                estimated_rpo=30,  # 30ë¶„
                steps=[
                    "1. ì „ì²´ ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬",
                    "2. ì„¤ì • ë° í…œí”Œë¦¿ ë³µì›",
                    "3. ë¡œê·¸ ë°ì´í„° ë³µêµ¬ (ì„ íƒì )",
                    "4. ì„œë¹„ìŠ¤ ì™„ì „ ì¬ì‹œì‘",
                    "5. ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ë³µêµ¬",
                    "6. ê¸°ëŠ¥ ì „ì²´ ê²€ì¦",
                ],
                korean_description="í‘œì¤€ì ì¸ ì „ì²´ ì‹œìŠ¤í…œ ë³µêµ¬ë¡œ ì •ìƒ ìš´ì˜ ìƒíƒœ ë³µì›",
                prerequisites=[
                    "ëª¨ë“  ë°±ì—… íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦",
                    "ë³µêµ¬ ëŒ€ìƒ ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ",
                    "ë„¤íŠ¸ì›Œí¬ ë° ì¸í”„ë¼ ì •ìƒ ìƒíƒœ",
                ],
                validation_steps=[
                    "ì „ì²´ API ì—”ë“œí¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸",
                    "ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ë™ì‘ í™•ì¸",
                    "ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ì •ìƒ ë™ì‘",
                    "ì„±ëŠ¥ ì§€í‘œ ì •ìƒ ë²”ìœ„ í™•ì¸",
                ],
            ),
            RecoveryLevel.FULL: RecoveryPlan(
                plan_id="full-recovery-v8-3-0",
                recovery_level=RecoveryLevel.FULL,
                estimated_rto=90,  # 90ë¶„
                estimated_rpo=15,  # 15ë¶„
                steps=[
                    "1. ì‹œìŠ¤í…œ ì™„ì „ ì´ˆê¸°í™”",
                    "2. ëª¨ë“  ë°±ì—… ë°ì´í„° ì™„ì „ ë³µì›",
                    "3. ì†ŒìŠ¤ì½”ë“œ ë° ì„¤ì • ì „ì²´ ë³µêµ¬",
                    "4. ë¡œê·¸ íˆìŠ¤í† ë¦¬ ì™„ì „ ë³µì›",
                    "5. ëª¨ë“  ì„œë¹„ìŠ¤ ë‹¨ê³„ë³„ ì¬ì‹œì‘",
                    "6. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ ë³µêµ¬",
                    "7. ì„±ëŠ¥ ìµœì í™” ë° íŠœë‹",
                    "8. ì „ì²´ ì‹œìŠ¤í…œ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸",
                ],
                korean_description="ëª¨ë“  ë°ì´í„°ì™€ ì„¤ì •ì„ ì™„ë²½í•˜ê²Œ ë³µì›í•˜ëŠ” ì™„ì „ ë³µêµ¬",
                prerequisites=[
                    "ëª¨ë“  ë°±ì—… ì„¸íŠ¸ ì™„ì „ ê²€ì¦",
                    "ì¶©ë¶„í•œ ë³µêµ¬ ì‹œê°„ í™•ë³´",
                    "ì „ì²´ ì¸í”„ë¼ ì¬êµ¬ì¶• ì¤€ë¹„",
                    "ë³µêµ¬ ê²€ì¦ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ í™˜ê²½",
                ],
                validation_steps=[
                    "ëª¨ë“  ê¸°ëŠ¥ ì™„ì „ ë™ì‘ í™•ì¸",
                    "íˆìŠ¤í† ë¦¬ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦",
                    "ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ì¶©ì¡±",
                    "ë³´ì•ˆ ìŠ¤ìº” í†µê³¼ í™•ì¸",
                    "ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ì •ìƒ ìˆ˜ì§‘",
                ],
            ),
            RecoveryLevel.DISASTER: RecoveryPlan(
                plan_id="disaster-recovery-v8-3-0",
                recovery_level=RecoveryLevel.DISASTER,
                estimated_rto=180,  # 3ì‹œê°„
                estimated_rpo=5,  # 5ë¶„
                steps=[
                    "1. ì¬í•´ ìƒí™© í‰ê°€ ë° ì˜í–¥ ë¶„ì„",
                    "2. ëŒ€ì²´ ì¸í”„ë¼ ê¸´ê¸‰ êµ¬ì¶•",
                    "3. ì˜¤í”„ì‚¬ì´íŠ¸ ë°±ì—…ì—ì„œ ë°ì´í„° ë³µêµ¬",
                    "4. ë„¤íŠ¸ì›Œí¬ ë° ë³´ì•ˆ ì¬ì„¤ì •",
                    "5. ì„œë¹„ìŠ¤ ë‹¨ê³„ì  ë³µêµ¬ ì‹œì‘",
                    "6. ë°ì´í„° ë™ê¸°í™” ë° ì¼ê´€ì„± ê²€ì¦",
                    "7. Blue-Green ë°°í¬ë¡œ ì•ˆì „í•œ ì „í™˜",
                    "8. ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ ê²€ì¦",
                    "9. ì‚¬ìš©ì ì„œë¹„ìŠ¤ ì¬ê°œ ê³µì§€",
                    "10. ì¬í•´ ë³µêµ¬ ì™„ë£Œ ë³´ê³ ",
                ],
                korean_description="ì™„ì „í•œ ì¬í•´ ìƒí™©ì—ì„œ ìƒˆë¡œìš´ í™˜ê²½ìœ¼ë¡œ ì „ì²´ ë³µêµ¬",
                prerequisites=[
                    "ì˜¤í”„ì‚¬ì´íŠ¸ ë°±ì—… ì ‘ê·¼ ê°€ëŠ¥ í™•ì¸",
                    "ëŒ€ì²´ ì¸í”„ë¼ ë˜ëŠ” í´ë¼ìš°ë“œ í™˜ê²½ ì¤€ë¹„",
                    "DNS ë° ë„¤íŠ¸ì›Œí¬ ë¼ìš°íŒ… ë³€ê²½ ê¶Œí•œ",
                    "ì¬í•´ë³µêµ¬íŒ€ ë¹„ìƒ ì†Œì§‘",
                    "ê³ ê°/ì‚¬ìš©ì ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì±„ë„ ì¤€ë¹„",
                ],
                validation_steps=[
                    "ì „ì²´ ì„œë¹„ìŠ¤ ê°€ìš©ì„± 100% í™•ì¸",
                    "ë°ì´í„° ë¬´ì†ì‹¤ ë³µêµ¬ ê²€ì¦",
                    "ì„±ëŠ¥ SLA ê¸°ì¤€ ì¶©ì¡± í™•ì¸",
                    "ë³´ì•ˆ ë° ì»´í”Œë¼ì´ì–¸ìŠ¤ ê²€ì¦",
                    "ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì™„ì „ ë³µêµ¬",
                    "ë°±ì—… ì‹œìŠ¤í…œ ì •ìƒ ë™ì‘ í™•ì¸",
                    "ì¬í•´ ë³µêµ¬ ì ˆì°¨ ê°œì„ ì‚¬í•­ ë„ì¶œ",
                ],
            ),
        }

    async def create_backup(
        self, backup_type: BackupType, components: List[str] = None
    ) -> BackupMetadata:
        """ë°±ì—… ìƒì„±"""
        backup_id = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{backup_type.name.lower()}"

        logger.info(f"ğŸ”„ {backup_type.value} ì‹œì‘: {backup_id}")

        # ë°±ì—…í•  ì»´í¬ë„ŒíŠ¸ ê²°ì •
        if components is None:
            components = list(self.backup_components.keys())

        # ë°±ì—… ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = BackupMetadata(
            backup_id=backup_id,
            backup_type=backup_type,
            created_at=datetime.now(),
            size_bytes=0,
            checksum="",
            status=BackupStatus.IN_PROGRESS,
            retention_days=self.retention_policies[backup_type],
            components=components,
            korean_description=self._generate_backup_description(
                backup_type, components
            ),
            recovery_point=datetime.now(),
        )

        try:
            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            backup_dir = self.backup_root / backup_id
            backup_dir.mkdir(exist_ok=True)

            # ì»´í¬ë„ŒíŠ¸ë³„ ë°±ì—… ìˆ˜í–‰
            total_size = 0
            checksums = []

            for component in components:
                component_size, component_checksum = await self._backup_component(
                    component, backup_dir
                )
                total_size += component_size
                checksums.append(component_checksum)

            # ë°±ì—… ì••ì¶•
            compressed_file = await self._compress_backup(backup_dir)

            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            metadata.size_bytes = compressed_file.stat().st_size
            metadata.checksum = self._calculate_file_checksum(compressed_file)
            metadata.status = BackupStatus.COMPLETED

            # ë°±ì—… ê²€ì¦
            if await self._verify_backup(metadata, compressed_file):
                metadata.status = BackupStatus.VERIFIED
                logger.info(
                    f"âœ… {backup_type.value} ì™„ë£Œ: {backup_id} ({self._format_size(metadata.size_bytes)})"
                )
            else:
                metadata.status = BackupStatus.FAILED
                logger.error(f"âŒ {backup_type.value} ê²€ì¦ ì‹¤íŒ¨: {backup_id}")

            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            await self._store_backup_metadata(metadata, str(compressed_file))

            # ì •ë¦¬ ì‘ì—…
            shutil.rmtree(backup_dir)  # ì••ì¶• í›„ ì›ë³¸ ì‚­ì œ

            return metadata

        except Exception as e:
            metadata.status = BackupStatus.FAILED
            logger.error(f"âŒ {backup_type.value} ì‹¤íŒ¨: {backup_id} - {e}")
            return metadata

    async def _backup_component(
        self, component: str, backup_dir: Path
    ) -> Tuple[int, str]:
        """ê°œë³„ ì»´í¬ë„ŒíŠ¸ ë°±ì—…"""
        component_config = self.backup_components[component]
        source_path = Path(component_config["path"])

        if not source_path.exists():
            logger.warning(f"âš ï¸ ë°±ì—… ì†ŒìŠ¤ ì—†ìŒ: {source_path}")
            return 0, ""

        dest_path = backup_dir / component
        size = 0

        if component_config["type"] == "sqlite":
            # SQLite ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
            dest_path.parent.mkdir(parents=True, exist_ok=True)

            # Online backup using SQLite backup API
            with sqlite3.connect(source_path) as source_conn:
                with sqlite3.connect(dest_path) as backup_conn:
                    source_conn.backup(backup_conn)

            size = dest_path.stat().st_size

        elif component_config["type"] == "directory":
            # ë””ë ‰í† ë¦¬ ë°±ì—…
            shutil.copytree(source_path, dest_path, dirs_exist_ok=True)
            size = self._calculate_directory_size(dest_path)

        # ì²´í¬ì„¬ ê³„ì‚°
        checksum = self._calculate_component_checksum(dest_path)

        logger.info(f"ğŸ“¦ ì»´í¬ë„ŒíŠ¸ ë°±ì—… ì™„ë£Œ: {component} ({self._format_size(size)})")
        return size, checksum

    async def _compress_backup(self, backup_dir: Path) -> Path:
        """ë°±ì—… ì••ì¶•"""
        compressed_file = backup_dir.with_suffix(".tar.gz")

        with tarfile.open(compressed_file, "w:gz") as tar:
            tar.add(backup_dir, arcname=backup_dir.name)

        return compressed_file

    async def _verify_backup(self, metadata: BackupMetadata, backup_file: Path) -> bool:
        """ë°±ì—… ë¬´ê²°ì„± ê²€ì¦"""
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not backup_file.exists():
                return False

            # ì••ì¶• íŒŒì¼ ë¬´ê²°ì„± í™•ì¸
            with tarfile.open(backup_file, "r:gz") as tar:
                # ëª¨ë“  íŒŒì¼ì´ ì •ìƒì ìœ¼ë¡œ ì••ì¶•ë˜ì—ˆëŠ”ì§€ í™•ì¸
                tar.getmembers()

            # ì²´í¬ì„¬ ê²€ì¦
            calculated_checksum = self._calculate_file_checksum(backup_file)
            if calculated_checksum != metadata.checksum:
                logger.error(
                    f"âŒ ì²´í¬ì„¬ ë¶ˆì¼ì¹˜: {calculated_checksum} != {metadata.checksum}"
                )
                return False

            logger.info(f"âœ… ë°±ì—… ê²€ì¦ ì„±ê³µ: {metadata.backup_id}")
            return True

        except Exception as e:
            logger.error(f"âŒ ë°±ì—… ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False

    async def _store_backup_metadata(self, metadata: BackupMetadata, file_path: str):
        """ë°±ì—… ë©”íƒ€ë°ì´í„° ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                INSERT INTO backup_metadata 
                (backup_id, backup_type, created_at, size_bytes, checksum, 
                 status, retention_days, components, korean_description, 
                 recovery_point, file_path)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    metadata.backup_id,
                    metadata.backup_type.name,
                    metadata.created_at,
                    metadata.size_bytes,
                    metadata.checksum,
                    metadata.status.name,
                    metadata.retention_days,
                    json.dumps(metadata.components),
                    metadata.korean_description,
                    metadata.recovery_point,
                    file_path,
                ),
            )
            conn.commit()

    async def restore_from_backup(
        self,
        backup_id: str,
        recovery_level: RecoveryLevel = RecoveryLevel.STANDARD,
        components: List[str] = None,
    ) -> bool:
        """ë°±ì—…ì—ì„œ ë³µêµ¬"""
        recovery_id = f"recovery_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        logger.info(f"ğŸ”§ ë³µêµ¬ ì‹œì‘: {backup_id} -> {recovery_level.value}")

        try:
            # ë°±ì—… ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            metadata = await self._get_backup_metadata(backup_id)
            if not metadata:
                logger.error(f"âŒ ë°±ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {backup_id}")
                return False

            # ë³µêµ¬ ê³„íš ê°€ì ¸ì˜¤ê¸°
            recovery_plan = self.recovery_plans.get(recovery_level)
            if not recovery_plan:
                logger.error(f"âŒ ë³µêµ¬ ê³„íš ì—†ìŒ: {recovery_level}")
                return False

            # ë³µêµ¬ ì´ë ¥ ê¸°ë¡ ì‹œì‘
            await self._start_recovery_record(recovery_id, backup_id, recovery_level)

            # ë³µêµ¬ ì „ ê²€ì¦
            if not await self._pre_recovery_validation(metadata, recovery_plan):
                logger.error("âŒ ë³µêµ¬ ì „ ê²€ì¦ ì‹¤íŒ¨")
                return False

            # ë°±ì—… íŒŒì¼ ì••ì¶• í•´ì œ
            backup_file = Path(await self._get_backup_file_path(backup_id))
            temp_restore_dir = Path("temp_restore") / recovery_id
            temp_restore_dir.mkdir(parents=True, exist_ok=True)

            with tarfile.open(backup_file, "r:gz") as tar:
                tar.extractall(temp_restore_dir)

            extracted_backup_dir = temp_restore_dir / backup_id

            # ì»´í¬ë„ŒíŠ¸ë³„ ë³µêµ¬
            if components is None:
                components = metadata.components

            success_count = 0
            for component in components:
                if await self._restore_component(component, extracted_backup_dir):
                    success_count += 1
                    logger.info(f"âœ… ì»´í¬ë„ŒíŠ¸ ë³µêµ¬ ì„±ê³µ: {component}")
                else:
                    logger.error(f"âŒ ì»´í¬ë„ŒíŠ¸ ë³µêµ¬ ì‹¤íŒ¨: {component}")

            # ë³µêµ¬ í›„ ê²€ì¦
            recovery_success = await self._post_recovery_validation(recovery_plan)

            # ì •ë¦¬ ì‘ì—…
            shutil.rmtree(temp_restore_dir)

            # ë³µêµ¬ ì´ë ¥ ì™„ë£Œ
            await self._complete_recovery_record(recovery_id, recovery_success)

            if recovery_success:
                logger.info(f"âœ… {recovery_level.value} ì™„ë£Œ: {recovery_id}")
                await self._send_recovery_notification(
                    recovery_id, True, recovery_level
                )
            else:
                logger.error(f"âŒ {recovery_level.value} ì‹¤íŒ¨: {recovery_id}")
                await self._send_recovery_notification(
                    recovery_id, False, recovery_level
                )

            return recovery_success

        except Exception as e:
            logger.error(f"âŒ ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            await self._complete_recovery_record(recovery_id, False, str(e))
            return False

    async def _restore_component(self, component: str, backup_dir: Path) -> bool:
        """ê°œë³„ ì»´í¬ë„ŒíŠ¸ ë³µêµ¬"""
        component_config = self.backup_components[component]
        source_path = backup_dir / component
        dest_path = Path(component_config["path"])

        if not source_path.exists():
            logger.warning(f"âš ï¸ ë°±ì—… ì»´í¬ë„ŒíŠ¸ ì—†ìŒ: {source_path}")
            return False

        try:
            # ê¸°ì¡´ íŒŒì¼/ë””ë ‰í† ë¦¬ ë°±ì—… (ì•ˆì „ ì¡°ì¹˜)
            if dest_path.exists():
                backup_old_path = dest_path.with_suffix(
                    f'.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
                )
                if dest_path.is_file():
                    shutil.copy2(dest_path, backup_old_path)
                else:
                    shutil.copytree(dest_path, backup_old_path)

            # ë³µêµ¬ ìˆ˜í–‰
            if component_config["type"] == "sqlite":
                # SQLite ë°ì´í„°ë² ì´ìŠ¤ ë³µêµ¬
                dest_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(source_path, dest_path)

            elif component_config["type"] == "directory":
                # ë””ë ‰í† ë¦¬ ë³µêµ¬
                if dest_path.exists():
                    shutil.rmtree(dest_path)
                shutil.copytree(source_path, dest_path)

            logger.info(f"ğŸ“¦ ì»´í¬ë„ŒíŠ¸ ë³µêµ¬ ì™„ë£Œ: {component}")
            return True

        except Exception as e:
            logger.error(f"âŒ ì»´í¬ë„ŒíŠ¸ ë³µêµ¬ ì‹¤íŒ¨ {component}: {e}")
            return False

    async def _get_backup_metadata(self, backup_id: str) -> Optional[BackupMetadata]:
        """ë°±ì—… ë©”íƒ€ë°ì´í„° ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            cursor.execute(
                """
                SELECT * FROM backup_metadata WHERE backup_id = ?
            """,
                (backup_id,),
            )

            row = cursor.fetchone()
            if not row:
                return None

            return BackupMetadata(
                backup_id=row[0],
                backup_type=BackupType[row[1]],
                created_at=datetime.fromisoformat(row[2]),
                size_bytes=row[3],
                checksum=row[4],
                status=BackupStatus[row[5]],
                retention_days=row[6],
                components=json.loads(row[7]),
                korean_description=row[8],
                recovery_point=datetime.fromisoformat(row[9]),
            )

    def _generate_backup_description(
        self, backup_type: BackupType, components: List[str]
    ) -> str:
        """ë°±ì—… ì„¤ëª… ìƒì„±"""
        korean_components = [
            self.backup_components[comp]["korean_name"]
            for comp in components
            if comp in self.backup_components
        ]

        component_str = ", ".join(korean_components)
        return f"{backup_type.value}: {component_str} ë°±ì—… ìƒì„±"

    def _calculate_file_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚°"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

    def _calculate_component_checksum(self, path: Path) -> str:
        """ì»´í¬ë„ŒíŠ¸ ì²´í¬ì„¬ ê³„ì‚°"""
        if path.is_file():
            return self._calculate_file_checksum(path)
        else:
            # ë””ë ‰í† ë¦¬ì˜ ê²½ìš° ëª¨ë“  íŒŒì¼ì˜ ì²´í¬ì„¬ì„ í•©í•œ ì²´í¬ì„¬
            checksums = []
            for file_path in path.rglob("*"):
                if file_path.is_file():
                    checksums.append(self._calculate_file_checksum(file_path))

            combined = "".join(sorted(checksums))
            return hashlib.sha256(combined.encode()).hexdigest()

    def _calculate_directory_size(self, dir_path: Path) -> int:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°"""
        total_size = 0
        for file_path in dir_path.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size

    def _format_size(self, size_bytes: int) -> str:
        """ë°”ì´íŠ¸ í¬ê¸°ë¥¼ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ ë³€í™˜"""
        for unit in ["B", "KB", "MB", "GB", "TB"]:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} PB"

    async def cleanup_old_backups(self):
        """ë³´ê´€ ì •ì±…ì— ë”°ë¥¸ ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬"""
        logger.info("ğŸ§¹ ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ ì‹œì‘")

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ë³´ê´€ ê¸°ê°„ì´ ì§€ë‚œ ë°±ì—… ì¡°íšŒ
            cursor.execute(
                """
                SELECT backup_id, file_path, backup_type, created_at, retention_days
                FROM backup_metadata
                WHERE datetime(created_at, '+' || retention_days || ' days') < datetime('now')
                AND status != 'FAILED'
            """
            )

            expired_backups = cursor.fetchall()

            for backup in expired_backups:
                backup_id, file_path, backup_type, created_at, retention_days = backup

                try:
                    # ë°±ì—… íŒŒì¼ ì‚­ì œ
                    if os.path.exists(file_path):
                        os.remove(file_path)

                    # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œê±°
                    cursor.execute(
                        "DELETE FROM backup_metadata WHERE backup_id = ?", (backup_id,)
                    )

                    logger.info(f"ğŸ—‘ï¸ ë§Œë£Œëœ ë°±ì—… ì‚­ì œ: {backup_id}")

                except Exception as e:
                    logger.error(f"âŒ ë°±ì—… ì‚­ì œ ì‹¤íŒ¨ {backup_id}: {e}")

            conn.commit()
            logger.info(f"âœ… ë°±ì—… ì •ë¦¬ ì™„ë£Œ: {len(expired_backups)}ê°œ íŒŒì¼ ì‚­ì œ")

    async def get_backup_status_report(self) -> Dict[str, Any]:
        """ë°±ì—… ìƒíƒœ ë¦¬í¬íŠ¸ (í•œêµ­ì–´)"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # ë°±ì—… í†µê³„
            cursor.execute(
                """
                SELECT 
                    COUNT(*) as total,
                    SUM(size_bytes) as total_size,
                    COUNT(CASE WHEN status = 'VERIFIED' THEN 1 END) as verified,
                    COUNT(CASE WHEN status = 'FAILED' THEN 1 END) as failed
                FROM backup_metadata
                WHERE datetime(created_at) >= datetime('now', '-7 days')
            """
            )

            stats = cursor.fetchone()

            # ìµœê·¼ ë°±ì—…
            cursor.execute(
                """
                SELECT backup_id, backup_type, created_at, size_bytes, status, korean_description
                FROM backup_metadata
                ORDER BY created_at DESC
                LIMIT 10
            """
            )

            recent_backups = cursor.fetchall()

            return {
                "report_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "period": "ìµœê·¼ 7ì¼",
                "statistics": {
                    "total_backups": stats[0] or 0,
                    "total_size": self._format_size(stats[1] or 0),
                    "success_rate": f"{((stats[2] or 0) / (stats[0] or 1)) * 100:.1f}%",
                    "verified_backups": stats[2] or 0,
                    "failed_backups": stats[3] or 0,
                },
                "recent_backups": [
                    {
                        "backup_id": backup[0],
                        "type": BackupType[backup[1]].value,
                        "created_at": backup[2],
                        "size": self._format_size(backup[3]),
                        "status": BackupStatus[backup[4]].value,
                        "description": backup[5],
                    }
                    for backup in recent_backups
                ],
                "korean_summary": f"ì´ {stats[0] or 0}ê°œ ë°±ì—… ì¤‘ {stats[2] or 0}ê°œ ê²€ì¦ ì™„ë£Œ, ì„±ê³µë¥  {((stats[2] or 0) / (stats[0] or 1)) * 100:.1f}%",
            }

    async def schedule_automated_backups(self):
        """ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ë§"""
        logger.info("ğŸ“… ìë™ ë°±ì—… ìŠ¤ì¼€ì¤„ ì‹œì‘")

        # ë§¤ì¼ ìì • ì „ì²´ ë°±ì—…
        asyncio.create_task(self._schedule_daily_backup())

        # ë§¤ 6ì‹œê°„ ì¦ë¶„ ë°±ì—…
        asyncio.create_task(self._schedule_incremental_backup())

        # ë§¤ì£¼ ì¼ìš”ì¼ ì‹œìŠ¤í…œ ìŠ¤ëƒ…ìƒ·
        asyncio.create_task(self._schedule_weekly_snapshot())

    async def _schedule_daily_backup(self):
        """ì¼ì¼ ì „ì²´ ë°±ì—… ìŠ¤ì¼€ì¤„"""
        while True:
            try:
                # ìì •ê¹Œì§€ì˜ ì‹œê°„ ê³„ì‚°
                now = datetime.now()
                tomorrow = now.replace(
                    hour=0, minute=0, second=0, microsecond=0
                ) + timedelta(days=1)
                sleep_seconds = (tomorrow - now).total_seconds()

                await asyncio.sleep(sleep_seconds)

                # ì „ì²´ ë°±ì—… ìˆ˜í–‰
                logger.info("ğŸŒ™ ì¼ì¼ ì „ì²´ ë°±ì—… ì‹œì‘")
                await self.create_backup(BackupType.FULL)

                # ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬
                await self.cleanup_old_backups()

            except Exception as e:
                logger.error(f"âŒ ì¼ì¼ ë°±ì—… ìŠ¤ì¼€ì¤„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)  # 1ì‹œê°„ í›„ ì¬ì‹œë„

    async def _schedule_incremental_backup(self):
        """ì¦ë¶„ ë°±ì—… ìŠ¤ì¼€ì¤„ (6ì‹œê°„ë§ˆë‹¤)"""
        while True:
            try:
                await asyncio.sleep(6 * 3600)  # 6ì‹œê°„ ëŒ€ê¸°

                logger.info("â° ì¦ë¶„ ë°±ì—… ì‹œì‘")
                await self.create_backup(BackupType.INCREMENTAL, ["database", "config"])

            except Exception as e:
                logger.error(f"âŒ ì¦ë¶„ ë°±ì—… ìŠ¤ì¼€ì¤„ ì˜¤ë¥˜: {e}")

    async def _schedule_weekly_snapshot(self):
        """ì£¼ê°„ ìŠ¤ëƒ…ìƒ· ìŠ¤ì¼€ì¤„"""
        while True:
            try:
                # ë‹¤ìŒ ì¼ìš”ì¼ ìì •ê¹Œì§€ ëŒ€ê¸°
                now = datetime.now()
                days_until_sunday = (6 - now.weekday()) % 7
                if days_until_sunday == 0 and now.hour == 0:
                    days_until_sunday = 7

                next_sunday = now.replace(
                    hour=0, minute=0, second=0, microsecond=0
                ) + timedelta(days=days_until_sunday)
                sleep_seconds = (next_sunday - now).total_seconds()

                await asyncio.sleep(sleep_seconds)

                logger.info("ğŸ“¸ ì£¼ê°„ ìŠ¤ëƒ…ìƒ· ë°±ì—… ì‹œì‘")
                await self.create_backup(BackupType.SNAPSHOT)

            except Exception as e:
                logger.error(f"âŒ ì£¼ê°„ ìŠ¤ëƒ…ìƒ· ìŠ¤ì¼€ì¤„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(3600)

    async def _send_recovery_notification(
        self, recovery_id: str, success: bool, recovery_level: RecoveryLevel
    ):
        """ë³µêµ¬ ì™„ë£Œ ì•Œë¦¼ (í•œêµ­ì–´)"""
        status = "ì„±ê³µ" if success else "ì‹¤íŒ¨"
        icon = "âœ…" if success else "âŒ"

        message = f"{icon} {recovery_level.value} {status}: {recovery_id}"

        logger.info(f"ğŸ“§ ë³µêµ¬ ì•Œë¦¼: {message}")

        # ì—¬ê¸°ì— ì‹¤ì œ ì•Œë¦¼ ì‹œìŠ¤í…œ ì—°ë™ (ì´ë©”ì¼, ìŠ¬ë™, ë“±) êµ¬í˜„


# ê¸€ë¡œë²Œ ë°±ì—… ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤
_backup_manager_instance = None


def get_backup_manager() -> EnterpriseBackupManager:
    """ë°±ì—… ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _backup_manager_instance
    if _backup_manager_instance is None:
        _backup_manager_instance = EnterpriseBackupManager()
    return _backup_manager_instance


if __name__ == "__main__":

    async def main():
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        backup_manager = get_backup_manager()

        # ì „ì²´ ë°±ì—… ìƒì„±
        print("ğŸ”„ ì „ì²´ ë°±ì—… ìƒì„± ì¤‘...")
        backup_metadata = await backup_manager.create_backup(BackupType.FULL)
        print(f"âœ… ë°±ì—… ì™„ë£Œ: {backup_metadata.backup_id}")

        # ë°±ì—… ìƒíƒœ ë¦¬í¬íŠ¸
        report = await backup_manager.get_backup_status_report()
        print(json.dumps(report, indent=2, ensure_ascii=False))

        # ë³µêµ¬ í…ŒìŠ¤íŠ¸ (ì£¼ì˜: ì‹¤ì œ ë°ì´í„° ì˜í–¥)
        # await backup_manager.restore_from_backup(backup_metadata.backup_id, RecoveryLevel.MINIMAL)

    asyncio.run(main())
