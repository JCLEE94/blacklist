#!/usr/bin/env python3
"""
REGTECH ìë™ ìˆ˜ì§‘ ì‹œìŠ¤í…œ - HAR ë¶„ì„ ê¸°ë°˜ ì •ë¦¬ëœ ë²„ì „
"""

import os
import json
import logging
import time
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from pathlib import Path
import threading
from dataclasses import dataclass
import re
from bs4 import BeautifulSoup
import io
import tempfile

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    logging.warning("pandas not available - Excel download will not work")

from src.core.models import BlacklistEntry
from src.config.settings import settings

logger = logging.getLogger(__name__)


@dataclass
class RegtechCollectionStats:
    """REGTECH ìˆ˜ì§‘ í†µê³„"""
    start_time: datetime
    end_time: Optional[datetime] = None
    total_collected: int = 0
    successful_collections: int = 0
    failed_collections: int = 0
    pages_processed: int = 0
    duplicate_count: int = 0
    error_count: int = 0
    source_method: str = "unknown"


class RegtechCollector:
    """
    REGTECH ìë™ ìˆ˜ì§‘ ì‹œìŠ¤í…œ - HAR ë¶„ì„ ê¸°ë°˜ êµ¬í˜„
    """
    
    def __init__(self, data_dir: str, cache_backend=None):
        self.data_dir = data_dir
        self.regtech_dir = os.path.join(data_dir, 'regtech')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.regtech_dir, exist_ok=True)
        
        # REGTECH API ì„¤ì • (ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦)
        self.base_url = "https://regtech.fsec.or.kr"
        self.advisory_endpoint = "/fcti/securityAdvisory/advisoryList"
        
        # ì¿ í‚¤ ì„¤ì • (e.ps1ì—ì„œ ì¶”ì¶œí•œ ê°’ë“¤)
        self.cookies = {
            '_ga': 'GA1.1.1689204774.1752555033',
            'regtech-front': '2F3B7CE1B26084FCD546BDB56CE9ABAC',
            'regtech-va': 'BearereyJ0eXAiOiJKV1QiLCJhbGciOiJIUzUxMiJ9.eyJzdWIiOiJuZXh0cmFkZSIsIm9yZ2FubmFtZSI6IuuEpeyKpO2KuOugiOydtOuTnCIsImlkIjoibmV4dHJhZGUiLCJleHAiOjE3NTI4Mjk2NDUsInVzZXJuYW1lIjoi7J6l7ZmN7KSAIn0.ha36VHXTf1AnziAChasI68mh9nrDawyrKRXyXKV6liPCOA1MFnoR5kTg3pSw3RNM_zkDD2NnfX5PcbdzwPET1w',
            '_ga_7WRDYHF66J': 'GS2.1.s1752743223$o3$g1$t1752746099$j38$l0$h0'
        }
        
        # ìˆ˜ì§‘ í†µê³„
        self.stats = RegtechCollectionStats(start_time=datetime.now())
        
        logger.info(f"REGTECH ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ (ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦): {self.regtech_dir}")
    
    def collect_from_web(self, max_pages: int = 5, page_size: int = 100, 
                        parallel_workers: int = 1, start_date: str = None, 
                        end_date: str = None) -> List[BlacklistEntry]:
        """
        REGTECH Excel ë‹¤ìš´ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
        """
        logger.info(f"ğŸ”„ REGTECH Excel ë‹¤ìš´ë¡œë“œ ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"ğŸ“ ìˆ˜ì§‘ ì§„í–‰ ìƒí™©ì„ ìƒì„¸íˆ ë¡œê¹…í•©ë‹ˆë‹¤")
        
        # ì¼ì¼ ìˆ˜ì§‘ ì—¬ë¶€ í™•ì¸
        is_daily_collection = False
        
        # ê¸°ë³¸ ë‚ ì§œ ì„¤ì • (íŒŒë¼ë¯¸í„°ë¡œ ì œê³µë˜ì§€ ì•Šì€ ê²½ìš°)
        if not start_date or not end_date:
            end_dt = datetime.now()
            start_dt = end_dt - timedelta(days=90)  # 90ì¼ë¡œ í™•ëŒ€
            start_date = start_dt.strftime('%Y%m%d')
            end_date = end_dt.strftime('%Y%m%d')
        else:
            # ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ì´ ê°™ìœ¼ë©´ ì¼ì¼ ìˆ˜ì§‘
            if start_date == end_date:
                is_daily_collection = True
                logger.info(f"ğŸ“… ì¼ì¼ ìë™ ìˆ˜ì§‘ ëª¨ë“œ: {start_date} í•˜ë£¨ ë°ì´í„°ë§Œ ìˆ˜ì§‘")
        
        logger.info(f"ğŸ“† REGTECH ìˆ˜ì§‘ ë‚ ì§œ ë²”ìœ„: {start_date} ~ {end_date}")
        
        if is_daily_collection:
            logger.info(f"ğŸ”” ì¼ì¼ ìˆ˜ì§‘ ì‹¤í–‰ ì¤‘ - ê¸ˆì¼({start_date}) ì‹ ê·œ íƒì§€ IPë§Œ ìˆ˜ì§‘í•©ë‹ˆë‹¤")
        
        self.stats = RegtechCollectionStats(
            start_time=datetime.now(),
            source_method="excel_download"
        )
        
        try:
            # ì„¸ì…˜ ìƒì„± ë° ë¡œê·¸ì¸
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            })
            
            # ë¡œê·¸ì¸ ìˆ˜í–‰
            logger.info("ğŸ” REGTECH ë¡œê·¸ì¸ ì‹œë„ ì¤‘...")
            if not self._perform_login(session):
                logger.error("âŒ REGTECH ë¡œê·¸ì¸ ì‹¤íŒ¨")
                return []
            logger.info("âœ… REGTECH ë¡œê·¸ì¸ ì„±ê³µ")
            
            # Excel ë‹¤ìš´ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘
            logger.info("ğŸ“Š Excel ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œì‘...")
            collected_ips = self._download_excel_data(session, start_date, end_date)
            logger.info(f"ğŸ“‹ Excel ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {len(collected_ips)}ê°œ IP ìˆ˜ì§‘")
            
            if collected_ips:
                self.stats.total_collected = len(collected_ips)
                self.stats.successful_collections = len(collected_ips)
                self.stats.end_time = datetime.now()
                
                # ì¼ì¼ ìˆ˜ì§‘ ì—¬ë¶€ í™•ì¸
                if start_date == end_date:
                    logger.info(f"âœ… REGTECH ì¼ì¼ ìˆ˜ì§‘ ì™„ë£Œ ({start_date}): {len(collected_ips)}ê°œ ì‹ ê·œ IP ì¶”ê°€")
                    logger.info(f"ğŸ“Š ê¸ˆì¼ íƒì§€ í†µê³„:")
                    logger.info(f"   - ì‹ ê·œ íƒì§€ IP: {len(collected_ips)}ê°œ")
                    logger.info(f"   - ìˆ˜ì§‘ ì‹œê°„: {self.stats.end_time - self.stats.start_time}")
                else:
                    logger.info(f"âœ… REGTECH Excel ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_ips)}ê°œ IP")
                return collected_ips
            else:
                # Excel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ì‹œ ê¸°ì¡´ HTML íŒŒì‹± ë°©ì‹ ì‹œë„
                logger.warning("Excel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨, HTML íŒŒì‹± ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„")
                return self._collect_html_fallback(session, start_date, end_date, max_pages, page_size)
            
        except Exception as e:
            logger.error(f"REGTECH ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            self.stats.error_count += 1
            return []
    
    def _parse_html_response(self, html_content: str) -> List[BlacklistEntry]:
        """
        HAR ë¶„ì„ ê¸°ë°˜ HTML ì‘ë‹µ íŒŒì‹± - ì‹¤ì œ REGTECH í…Œì´ë¸” êµ¬ì¡° ê¸°ë°˜
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            ip_entries = []
            
            # REGTECH ìš”ì£¼ì˜ IP í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')
            
            for table in tables:
                # captionì´ "ìš”ì£¼ì˜ IP ëª©ë¡"ì¸ í…Œì´ë¸” ì°¾ê¸°
                caption = table.find('caption')
                if caption and 'ìš”ì£¼ì˜ IP' in caption.text:
                    logger.info("ğŸ“‹ ìš”ì£¼ì˜ IP í…Œì´ë¸” ë°œê²¬")
                    
                    # tbodyì˜ ëª¨ë“  tr ì°¾ê¸°
                    tbody = table.find('tbody')
                    if tbody:
                        rows = tbody.find_all('tr')
                        logger.info(f"ğŸ“Š í…Œì´ë¸”ì—ì„œ {len(rows)}ê°œì˜ í–‰ ë°œê²¬")
                        
                        for row in rows:
                            cells = row.find_all('td')
                            
                            # REGTECH í…Œì´ë¸” êµ¬ì¡°: IP, êµ­ê°€, ë“±ë¡ì‚¬ìœ , ë“±ë¡ì¼, í•´ì œì¼, ì¡°íšŒìˆ˜
                            if len(cells) >= 6:
                                try:
                                    ip_text = cells[0].get_text(strip=True)
                                    country = cells[1].get_text(strip=True)
                                    
                                    # ë“±ë¡ì‚¬ìœ ì—ì„œ attack_type ì¶”ì¶œ
                                    reason_cell = cells[2]
                                    attack_type = reason_cell.get_text(strip=True)
                                    
                                    # ë‚ ì§œ ì •ë³´
                                    detection_date = cells[3].get_text(strip=True)
                                    release_date = cells[4].get_text(strip=True) if len(cells) > 4 else ''
                                    views = cells[5].get_text(strip=True) if len(cells) > 5 else '0'
                                    
                                    # IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì¦
                                    if self._is_valid_ip(ip_text):
                                        # extra_dataì— ì¶”ê°€ ì •ë³´ ì €ì¥
                                        extra_data = {
                                            'release_date': release_date,
                                            'views': views
                                        }
                                        
                                        ip_entry = BlacklistEntry(
                                            ip_address=ip_text,
                                            country=country,
                                            reason=attack_type,
                                            source='REGTECH',
                                            reg_date=detection_date,
                                            exp_date=release_date,
                                            is_active=True,
                                            threat_level='high',
                                            source_details={'type': 'REGTECH', 'attack': attack_type}
                                        )
                                        ip_entries.append(ip_entry)
                                        logger.debug(f"IP ìˆ˜ì§‘: {ip_text} ({country}) - {attack_type}")
                                
                                except Exception as e:
                                    logger.debug(f"í–‰ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                                    continue
                    else:
                        # tbodyê°€ ì—†ëŠ” ê²½ìš° ëª¨ë“  tr ê²€ìƒ‰
                        rows = table.find_all('tr')
                        for row in rows[1:]:  # í—¤ë” ì œì™¸
                            cells = row.find_all('td')
                            if len(cells) >= 4:
                                try:
                                    ip_text = cells[0].get_text(strip=True)
                                    if self._is_valid_ip(ip_text):
                                        country = cells[1].get_text(strip=True) if len(cells) > 1 else 'Unknown'
                                        attack_type = cells[2].get_text(strip=True) if len(cells) > 2 else 'REGTECH'
                                        detection_date = cells[3].get_text(strip=True) if len(cells) > 3 else datetime.now().strftime('%Y-%m-%d')
                                        
                                        ip_entry = BlacklistEntry(
                                            ip_address=ip_text,
                                            country=country,
                                            reason=attack_type,
                                            source='REGTECH',
                                            reg_date=detection_date,
                                            is_active=True,
                                            threat_level='high'
                                        )
                                        ip_entries.append(ip_entry)
                                except Exception as e:
                                    logger.debug(f"í–‰ íŒŒì‹± ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
                                    continue
            
            # í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° ê²½ê³ 
            if not ip_entries:
                logger.warning("ìš”ì£¼ì˜ IP í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆê±°ë‚˜ ë°ì´í„°ê°€ ì—†ìŒ")
                
                # ë””ë²„ê¹…ì„ ìœ„í•´ ì´ ê±´ìˆ˜ í™•ì¸
                total_elem = soup.find('em')
                if total_elem:
                    total_text = total_elem.get_text(strip=True)
                    logger.info(f"í˜ì´ì§€ì— í‘œì‹œëœ ì´ ê±´ìˆ˜: {total_text}")
            
            return ip_entries
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _perform_login(self, session: requests.Session) -> bool:
        """REGTECH ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦ ì„¤ì • (e.ps1 ë°©ì‹)"""
        try:
            logger.info("REGTECH ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦ ì„¤ì • ì‹œì‘")
            
            # ê¸°ì¡´ ì¿ í‚¤ ì„¤ì •
            for name, value in self.cookies.items():
                session.cookies.set(name, value, domain='.regtech.fsec.or.kr')
            
            # ì¶”ê°€ í—¤ë” ì„¤ì •
            session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin'
            })
            
            # ì¿ í‚¤ ìœ íš¨ì„± ê°„ë‹¨ í…ŒìŠ¤íŠ¸
            test_resp = session.get(f"{self.base_url}/fcti/securityAdvisory/advisoryList", timeout=30)
            if test_resp.status_code == 200:
                logger.info("REGTECH ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦ ì„±ê³µ")
                return True
            else:
                logger.error(f"REGTECH ì¿ í‚¤ ì¸ì¦ ì‹¤íŒ¨: {test_resp.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"REGTECH ì¿ í‚¤ ì¸ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
            
            logger.info(f"REGTECH ë¡œê·¸ì¸ ì‹œì‘: {username}")
            
            # 1. ë©”ì¸ í˜ì´ì§€ ì ‘ì† (ì„¸ì…˜ ì´ˆê¸°í™”)
            main_resp = session.get(f"{self.base_url}/main/main", timeout=30)
            if main_resp.status_code != 200:
                logger.error(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {main_resp.status_code}")
                return False
            time.sleep(1)
            
            # 2. ë¡œê·¸ì¸ í¼ ì ‘ì†
            form_resp = session.get(f"{self.base_url}/login/loginForm", timeout=30)
            if form_resp.status_code != 200:
                logger.error(f"ë¡œê·¸ì¸ í¼ ì ‘ì† ì‹¤íŒ¨: {form_resp.status_code}")
                return False
            time.sleep(1)
            
            # 3. ì‹¤ì œ ë¡œê·¸ì¸ ìˆ˜í–‰
            login_data = {
                'login_error': '',
                'txId': '',
                'token': '',
                'memberId': '',
                'smsTimeExcess': 'N',
                'username': username,
                'password': password
            }
            
            login_resp = session.post(
                f"{self.base_url}/login/addLogin",
                data=login_data,
                headers={
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Origin': self.base_url,
                    'Referer': f"{self.base_url}/login/loginForm"
                },
                allow_redirects=True,
                timeout=30
            )
            
            if login_resp.status_code != 200:
                logger.error(f"ë¡œê·¸ì¸ ìš”ì²­ ì‹¤íŒ¨: {login_resp.status_code}")
                return False
            
            # Bearer Token í™•ì¸ ë° Authorization í—¤ë” ì„¤ì •
            bearer_token = None
            for cookie in session.cookies:
                if cookie.name == 'regtech-va' and cookie.value.startswith('Bearer'):
                    bearer_token = cookie.value
                    session.headers['Authorization'] = bearer_token
                    logger.info("Bearer Token íšë“ ë° í—¤ë” ì„¤ì • ì™„ë£Œ")
                    break
            
            if not bearer_token:
                logger.error("Bearer Tokenì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return False
            
            # ìë™ ì¸ì¦ ëª¨ë“ˆì— í† í° ì €ì¥
            try:
                from .regtech_auto_login import get_regtech_auth
                auth = get_regtech_auth()
                auth._current_token = bearer_token
                auth._save_token_to_file(bearer_token)
            except:
                pass
            
            logger.info("REGTECH ë¡œê·¸ì¸ ì„±ê³µ")
            return True
                
        except Exception as e:
            logger.error(f"REGTECH ë¡œê·¸ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def _download_excel_data(self, session: requests.Session, start_date: str, end_date: str) -> List[BlacklistEntry]:
        """Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„° ìˆ˜ì§‘"""
        if not PANDAS_AVAILABLE:
            logger.error("pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ Excel ë‹¤ìš´ë¡œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        try:
            # Excel ë‹¤ìš´ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸
            excel_url = f"{self.base_url}/fcti/securityAdvisory/advisoryListDownloadXlsx"
            
            # POST ë°ì´í„°
            excel_data = {
                'page': '0',
                'tabSort': 'blacklist',
                'excelDownload': 'blacklist,',
                'cveId': '',
                'ipId': '',
                'estId': '',
                'startDate': start_date,
                'endDate': end_date,
                'findCondition': 'all',
                'findKeyword': '',
                'excelDown': 'blacklist',
                'size': '10'
            }
            
            # ì¶”ê°€ í—¤ë”
            headers = {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
                'Origin': self.base_url,
                'Referer': f"{self.base_url}/fcti/securityAdvisory/advisoryList",
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'same-origin'
            }
            
            # ì¼ì¼ ìˆ˜ì§‘ ì—¬ë¶€ í™•ì¸
            is_daily = (start_date == end_date)
            if is_daily:
                logger.info(f"ğŸ“… ì¼ì¼ ìˆ˜ì§‘ ëª¨ë“œ - {start_date} í•˜ë£¨ ë°ì´í„°ë§Œ ë‹¤ìš´ë¡œë“œ")
            
            logger.info(f"ğŸ“¥ Excel íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘... (ê¸°ê°„: {start_date} ~ {end_date})")
            response = session.post(
                excel_url,
                data=excel_data,
                headers=headers,
                timeout=60,
                stream=True
            )
            
            if response.status_code == 200:
                # Excel íŒŒì¼ì¸ì§€ í™•ì¸
                content_type = response.headers.get('Content-Type', '')
                if 'excel' in content_type or 'spreadsheet' in content_type or 'octet-stream' in content_type or not content_type:
                    # ë©”ëª¨ë¦¬ì—ì„œ Excel íŒŒì¼ ì½ê¸°
                    excel_content = io.BytesIO(response.content)
                    
                    try:
                        df = pd.read_excel(excel_content)
                        logger.info(f"âœ… Excel ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(df)} í–‰")
                        
                        if is_daily:
                            logger.info(f"ğŸ“Š ì¼ì¼ ìˆ˜ì§‘ ê²°ê³¼: {start_date}ì— íƒì§€ëœ {len(df)}ê°œ IP ë°œê²¬")
                        
                        # IP ì»¬ëŸ¼ ì°¾ê¸°
                        ip_column = None
                        logger.info(f"Excel ì»¬ëŸ¼ ëª©ë¡: {list(df.columns)}")
                        for col in df.columns:
                            if 'IP' in col.upper() or 'ip' in col:
                                ip_column = col
                                logger.info(f"IP ì»¬ëŸ¼ ë°œê²¬: '{col}'")
                                break
                        
                        if not ip_column:
                            logger.error("Excel íŒŒì¼ì—ì„œ IP ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                            return []
                        
                        # BlacklistEntry ê°ì²´ë¡œ ë³€í™˜
                        ip_entries = []
                        invalid_count = 0
                        logger.info(f"IP ì²˜ë¦¬ ì‹œì‘ (ì´ {len(df)}ê°œ)")
                        
                        for idx, row in df.iterrows():
                            try:
                                ip = str(row[ip_column]).strip()
                                
                                # IP ìœ íš¨ì„± ê²€ì¦
                                if not self._is_valid_ip(ip):
                                    invalid_count += 1
                                    if invalid_count <= 5:
                                        logger.debug(f"ë¬´íš¨í•œ IP: '{ip}' (í–‰ {idx})")
                                    continue
                                
                                # ë‹¤ë¥¸ ì»¬ëŸ¼ ë°ì´í„° ì¶”ì¶œ
                                country = str(row.get('êµ­ê°€', 'Unknown')).strip()
                                attack_type = str(row.get('ë“±ë¡ì‚¬ìœ ', 'REGTECH')).strip()
                                
                                # ì›ë³¸ ë“±ë¡ì¼ íŒŒì‹± (Excelì—ì„œ ì‹¤ì œ ë‚ ì§œ ì»¬ëŸ¼ ì‚¬ìš©)
                                detection_date_raw = row.get('ë“±ë¡ì¼')
                                if pd.notna(detection_date_raw):
                                    if isinstance(detection_date_raw, pd.Timestamp):
                                        detection_date = detection_date_raw.strftime('%Y-%m-%d')
                                    else:
                                        # ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
                                        try:
                                            parsed_date = pd.to_datetime(str(detection_date_raw))
                                            detection_date = parsed_date.strftime('%Y-%m-%d')
                                        except:
                                            detection_date = datetime.now().strftime('%Y-%m-%d')
                                else:
                                    detection_date = datetime.now().strftime('%Y-%m-%d')
                                
                                release_date_raw = row.get('í•´ì œì¼')
                                if pd.notna(release_date_raw):
                                    if isinstance(release_date_raw, pd.Timestamp):
                                        release_date = release_date_raw.strftime('%Y-%m-%d')
                                    else:
                                        release_date = str(release_date_raw).strip()
                                else:
                                    release_date = ''
                                
                                # extra_data
                                extra_data = {
                                    'release_date': release_date,
                                    'excel_row': idx + 1
                                }
                                
                                entry = BlacklistEntry(
                                    ip_address=ip,
                                    country=country,
                                    reason=attack_type,
                                    source='REGTECH',
                                    reg_date=detection_date,
                                    exp_date=release_date,
                                    is_active=True,
                                    threat_level='high',
                                    source_details={'type': 'REGTECH', 'attack': attack_type}
                                )
                                
                                ip_entries.append(entry)
                                if len(ip_entries) <= 5:
                                    logger.info(f"IP ì¶”ê°€ë¨: {ip} ({country}) - {attack_type}")
                                
                            except Exception as e:
                                logger.warning(f"í–‰ {idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                if idx < 5:  # ì²˜ìŒ 5ê°œë§Œ ìì„¸íˆ ê¸°ë¡
                                    import traceback
                                    logger.warning(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
                                continue
                        
                        # ì¤‘ë³µ ì œê±°
                        unique_ips = []
                        seen_ips = set()
                        
                        for entry in ip_entries:
                            if entry.ip_address not in seen_ips:
                                unique_ips.append(entry)
                                seen_ips.add(entry.ip_address)
                            else:
                                self.stats.duplicate_count += 1
                        
                        # ì¼ì¼ ìˆ˜ì§‘ í†µê³„ ë¡œê·¸
                        if is_daily:
                            logger.info(f"ğŸ“Š {start_date} ì¼ì¼ ìˆ˜ì§‘ ìƒì„¸ í†µê³„:")
                            logger.info(f"   - ì „ì²´ í–‰ ìˆ˜: {len(df)}ê°œ")
                            logger.info(f"   - ìœ íš¨í•œ IP: {len(unique_ips)}ê°œ")
                            logger.info(f"   - ì¤‘ë³µ ì œê±°: {self.stats.duplicate_count}ê°œ")
                            logger.info(f"   - ë¬´íš¨í•œ IP: {invalid_count}ê°œ")
                            
                            # êµ­ê°€ë³„ í†µê³„
                            if unique_ips:
                                country_stats = {}
                                for entry in unique_ips:
                                    country = entry.country or 'Unknown'
                                    country_stats[country] = country_stats.get(country, 0) + 1
                                
                                logger.info(f"   - êµ­ê°€ë³„ ë¶„í¬:")
                                for country, count in sorted(country_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                                    logger.info(f"     â€¢ {country}: {count}ê°œ")
                        else:
                            logger.info(f"Excelì—ì„œ {len(unique_ips)}ê°œ ê³ ìœ  IP ì¶”ì¶œ (ì¤‘ë³µ {self.stats.duplicate_count}ê°œ ì œê±°)")
                            logger.info(f"ë¬´íš¨í•œ IP ìˆ˜: {invalid_count}")
                        
                        if len(unique_ips) == 0:
                            logger.warning(f"âš ï¸ IPê°€ í•˜ë‚˜ë„ ì¶”ì¶œë˜ì§€ ì•ŠìŒ. ì „ì²´ {len(df)}í–‰ ì¤‘ ë¬´íš¨ {invalid_count}ê°œ")
                        return unique_ips
                        
                    except Exception as e:
                        logger.error(f"Excel íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
                        return []
                else:
                    logger.error(f"Excelì´ ì•„ë‹Œ ì‘ë‹µ íƒ€ì…: {content_type}")
                    return []
            else:
                logger.error(f"Excel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            logger.error(f"Excel ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _collect_html_fallback(self, session: requests.Session, start_date: str, end_date: str, 
                               max_pages: int, page_size: int) -> List[BlacklistEntry]:
        """HTML íŒŒì‹± ë°©ì‹ í´ë°±"""
        logger.info("HTML íŒŒì‹± ë°©ì‹ìœ¼ë¡œ ìˆ˜ì§‘ ì‹œë„")
        collected_ips = []
        
        for page in range(max_pages):
            logger.info(f"REGTECH í˜ì´ì§€ {page + 1}/{max_pages} ìˆ˜ì§‘ ì¤‘...")
            
            collection_data = {
                'page': str(page),
                'tabSort': 'blacklist',
                'excelDownload': '',
                'cveId': '',
                'ipId': '',
                'estId': '',
                'startDate': start_date,
                'endDate': end_date,
                'findCondition': 'all',
                'findKeyword': '',
                'size': str(page_size)
            }
            
            response = session.post(
                f"{self.base_url}{self.advisory_endpoint}",
                data=collection_data,
                headers={
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'Referer': f"{self.base_url}{self.advisory_endpoint}"
                },
                timeout=30
            )
            
            if response.status_code == 200:
                page_ips = self._parse_html_response(response.text)
                
                if page_ips:
                    collected_ips.extend(page_ips)
                    self.stats.pages_processed += 1
                    logger.info(f"í˜ì´ì§€ {page + 1}: {len(page_ips)}ê°œ IP ìˆ˜ì§‘")
                else:
                    logger.warning(f"í˜ì´ì§€ {page + 1}: IP ë°ì´í„° ì—†ìŒ")
                    break
            else:
                logger.error(f"í˜ì´ì§€ {page + 1} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                break
        
        # ì¤‘ë³µ ì œê±°
        unique_ips = []
        seen_ips = set()
        
        for ip_entry in collected_ips:
            if ip_entry.ip_address not in seen_ips:
                unique_ips.append(ip_entry)
                seen_ips.add(ip_entry.ip_address)
            else:
                self.stats.duplicate_count += 1
        
        return unique_ips
    
    def _is_valid_ip(self, ip: str) -> bool:
        """IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì¦"""
        try:
            if not ip or not isinstance(ip, str):
                return False
            
            # IP íŒ¨í„´ ê²€ì¦
            import re
            ip_pattern = re.compile(r'^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$')
            if not ip_pattern.match(ip):
                return False
            
            # ê° ì˜¥í…Ÿ ë²”ìœ„ ê²€ì¦
            parts = ip.split('.')
            for part in parts:
                if not 0 <= int(part) <= 255:
                    return False
            
            # ì‚¬ì„¤ IP ë° íŠ¹ìˆ˜ IP ì œì™¸
            if parts[0] == '192' and parts[1] == '168':
                return False
            if parts[0] == '10':
                return False
            if parts[0] == '172' and 16 <= int(parts[1]) <= 31:
                return False
            if parts[0] in ['0', '127', '255']:
                return False
            
            return True
            
        except:
            return False


def create_regtech_collector(data_dir: str, cache_backend=None) -> RegtechCollector:
    """REGTECH ìˆ˜ì§‘ê¸° íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return RegtechCollector(data_dir, cache_backend)