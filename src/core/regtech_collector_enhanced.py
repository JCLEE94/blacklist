#!/usr/bin/env python3
"""
Enhanced REGTECH ìˆ˜ì§‘ê¸° - ì•ˆì •ì„± ë° ì˜¤ë¥˜ ì²˜ë¦¬ ê°•í™” ë²„ì „
"""

import os
import json
import time
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Tuple
from pathlib import Path
import threading
from dataclasses import dataclass
import re
from bs4 import BeautifulSoup
import io
import tempfile
from urllib.parse import urljoin
import traceback
from requests.exceptions import RequestException, Timeout, ConnectionError

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False

from src.core.models import BlacklistEntry
from src.config.settings import settings
from src.utils.error_handler import (
    CollectionError, ExternalServiceError, retry_on_error,
    handle_api_errors, safe_execute
)
from src.utils.structured_logging import get_logger

logger = get_logger(__name__)

if not PANDAS_AVAILABLE:
    logger.warning("pandas not available - Excel download will not work")


@dataclass
class RegtechCollectionStats:
    """REGTECH ìˆ˜ì§‘ í†µê³„"""
    start_time: datetime
    end_time: Optional[datetime] = None
    total_collected: int = 0
    successful_collections: int = 0
    failed_collections: int = 0
    pages_processed: int = 0
    duplicate_count: int = 0
    error_count: int = 0
    source_method: str = "unknown"
    last_error: Optional[str] = None
    auth_attempts: int = 0
    excel_attempts: int = 0
    html_attempts: int = 0


class EnhancedRegtechCollector:
    """
    Enhanced REGTECH ìˆ˜ì§‘ê¸° - ë” ê°•ë ¥í•œ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
    """
    
    def __init__(self, data_dir: str, cache_backend=None):
        self.data_dir = data_dir
        self.regtech_dir = os.path.join(data_dir, 'regtech')
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.regtech_dir, exist_ok=True)
        
        # REGTECH API ì„¤ì •
        self.base_url = settings.regtech_base_url.rstrip('/')
        self.advisory_endpoint = "/fcti/securityAdvisory/advisoryList"
        
        # ìˆ˜ì§‘ í†µê³„
        self.stats = RegtechCollectionStats(start_time=datetime.now())
        
        # ì¬ì‹œë„ ì„¤ì •
        self.max_retries = 3
        self.retry_delay = 5
        
        # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        self.session_timeout = 60
        
        logger.info(f"Enhanced REGTECH ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ: {self.regtech_dir}")
    
    def collect_from_web(self, max_pages: int = 25, page_size: int = 100, 
                        parallel_workers: int = 1, start_date: str = None, 
                        end_date: str = None) -> List[BlacklistEntry]:
        """
        REGTECH ë°ì´í„° ìˆ˜ì§‘ - ê°•í™”ëœ ì˜¤ë¥˜ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
        """
        logger.info(f"ğŸ”„ Enhanced REGTECH ìˆ˜ì§‘ ì‹œì‘")
        
        # ë‚ ì§œ ì„¤ì •
        if not start_date or not end_date:
            end_dt = datetime.now()
            start_dt = end_dt - timedelta(days=90)
            start_date = start_dt.strftime('%Y%m%d')
            end_date = end_dt.strftime('%Y%m%d')
        
        is_daily = (start_date == end_date)
        logger.info(f"ğŸ“† ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date} {'(ì¼ì¼ ìˆ˜ì§‘)' if is_daily else ''}")
        
        self.stats = RegtechCollectionStats(
            start_time=datetime.now(),
            source_method="enhanced_collection"
        )
        
        collected_ips = []
        
        # ì—¬ëŸ¬ ìˆ˜ì§‘ ë°©ë²•ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        collection_methods = [
            ("Excel Download", self._try_excel_collection),
            ("HTML Parsing", self._try_html_collection),
            ("HAR-based Collection", self._try_har_based_collection)
        ]
        
        for method_name, method_func in collection_methods:
            try:
                logger.info(f"ğŸ”§ ì‹œë„ ì¤‘: {method_name}")
                result = method_func(start_date, end_date, max_pages, page_size)
                
                if result and len(result) > 0:
                    logger.info(f"âœ… {method_name} ì„±ê³µ: {len(result)}ê°œ IP ìˆ˜ì§‘")
                    collected_ips = result
                    self.stats.source_method = method_name.lower().replace(' ', '_')
                    break
                else:
                    logger.warning(f"{method_name} ì‹¤íŒ¨ ë˜ëŠ” ë°ì´í„° ì—†ìŒ")
                    
            except CollectionError as e:
                logger.error(f"{method_name} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                self.stats.last_error = str(e)
                self.stats.error_count += 1
                continue
            except Exception as e:
                logger.error(f"{method_name} ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                self.stats.last_error = str(e)
                self.stats.error_count += 1
                continue
        
        # ìµœì¢… ê²°ê³¼ ì²˜ë¦¬
        self.stats.end_time = datetime.now()
        self.stats.total_collected = len(collected_ips)
        
        if collected_ips:
            self.stats.successful_collections = len(collected_ips)
            logger.info(f"âœ… ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œ: {len(collected_ips)}ê°œ IP")
            self._log_collection_summary()
        else:
            self.stats.failed_collections += 1
            logger.error(f"âŒ ëª¨ë“  ìˆ˜ì§‘ ë°©ë²• ì‹¤íŒ¨")
            self._log_failure_details()
        
        return collected_ips
    
    def _try_excel_collection(self, start_date: str, end_date: str, 
                             max_pages: int, page_size: int) -> List[BlacklistEntry]:
        """Excel ë‹¤ìš´ë¡œë“œ ë°©ì‹ ì‹œë„"""
        if not PANDAS_AVAILABLE:
            logger.warning("pandas ë¯¸ì„¤ì¹˜ë¡œ Excel ìˆ˜ì§‘ ë¶ˆê°€")
            return []
        
        for attempt in range(self.max_retries):
            self.stats.excel_attempts += 1
            
            try:
                session = self._create_session()
                
                # ë¡œê·¸ì¸ ì‹œë„
                if not self._perform_enhanced_login(session):
                    logger.error(f"Excel ìˆ˜ì§‘ ë¡œê·¸ì¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries})")
                    time.sleep(self.retry_delay)
                    continue
                
                # Excel ë‹¤ìš´ë¡œë“œ
                result = self._download_excel_data_enhanced(session, start_date, end_date)
                
                if result:
                    return result
                    
            except RequestException as e:
                logger.error(f"Excel ìˆ˜ì§‘ HTTP ì˜¤ë¥˜", 
                           exception=e, attempt=attempt + 1, max_attempts=self.max_retries)
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
            except Exception as e:
                logger.error(f"Excel ìˆ˜ì§‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜", 
                           exception=e, attempt=attempt + 1)
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                    
        return []
    
    def _try_html_collection(self, start_date: str, end_date: str,
                            max_pages: int, page_size: int) -> List[BlacklistEntry]:
        """HTML íŒŒì‹± ë°©ì‹ ì‹œë„"""
        for attempt in range(self.max_retries):
            self.stats.html_attempts += 1
            
            try:
                session = self._create_session()
                
                # ë¡œê·¸ì¸ ì‹œë„
                if not self._perform_enhanced_login(session):
                    logger.error(f"HTML ìˆ˜ì§‘ ë¡œê·¸ì¸ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.max_retries})")
                    time.sleep(self.retry_delay)
                    continue
                
                # HTML ìˆ˜ì§‘
                result = self._collect_html_enhanced(session, start_date, end_date, max_pages, page_size)
                
                if result:
                    return result
                    
            except Exception as e:
                logger.error(f"HTML ìˆ˜ì§‘ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay)
                    
        return []
    
    def _try_har_based_collection(self, start_date: str, end_date: str,
                                 max_pages: int, page_size: int) -> List[BlacklistEntry]:
        """HAR ê¸°ë°˜ ìˆ˜ì§‘ ë°©ì‹ ì‹œë„"""
        try:
            # HAR ê¸°ë°˜ ìˆ˜ì§‘ê¸° ì„í¬íŠ¸ ì‹œë„
            from .har_based_regtech_collector import HARBasedRegtechCollector
            
            logger.info("HAR ê¸°ë°˜ ìˆ˜ì§‘ê¸° ì‚¬ìš© ì‹œë„")
            har_collector = HARBasedRegtechCollector(self.data_dir)
            
            result = har_collector.collect_from_web(
                start_date=start_date,
                end_date=end_date,
                max_pages=max_pages
            )
            
            if result:
                logger.info(f"HAR ìˆ˜ì§‘ ì„±ê³µ: {len(result)}ê°œ")
                return result
                
        except ImportError:
            logger.warning("HAR ê¸°ë°˜ ìˆ˜ì§‘ê¸° ì‚¬ìš© ë¶ˆê°€")
        except Exception as e:
            logger.error(f"HAR ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
            
        return []
    
    def _create_session(self) -> requests.Session:
        """ê°•í™”ëœ ì„¸ì…˜ ìƒì„±"""
        session = requests.Session()
        
        # ê¸°ë³¸ í—¤ë” ì„¤ì •
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0'
        })
        
        # íƒ€ì„ì•„ì›ƒ ì„¤ì •
        session.request = lambda *args, **kwargs: requests.Session.request(
            session, 
            *args, 
            timeout=kwargs.pop('timeout', self.session_timeout),
            **kwargs
        )
        
        return session
    
    @retry_on_error(max_attempts=3, delay=2.0, exceptions=(RequestException,))
    def _perform_enhanced_login(self, session: requests.Session) -> bool:
        """ê°•í™”ëœ ë¡œê·¸ì¸ ì²˜ë¦¬"""
        try:
            self.stats.auth_attempts += 1
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¸ì¦ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìš°ì„ ìˆœìœ„: DB > í™˜ê²½ë³€ìˆ˜)
            try:
                from ..models.settings import get_settings_manager
                settings_manager = get_settings_manager()
                username = settings_manager.get_setting('regtech_username', settings.regtech_username)
                password = settings_manager.get_setting('regtech_password', settings.regtech_password)
                
                logger.info(f"REGTECH ì¸ì¦ ì •ë³´ ë¡œë“œ - username: {username[:3] + '***' if username else 'ì—†ìŒ'}, password: {'***' if password else 'ì—†ìŒ'}")
            except Exception as e:
                logger.warning(f"ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì½ê¸° ì‹¤íŒ¨, í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©: {e}")
                username = settings.regtech_username
                password = settings.regtech_password
            
            if not username or not password:
                logger.error("REGTECH ì¸ì¦ ì •ë³´ ì—†ìŒ")
                raise CollectionError("REGTECH", "ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
            
            logger.info(f"ğŸ” REGTECH ë¡œê·¸ì¸ ì‹œì‘ (ì‚¬ìš©ì: {username})")
            
            # 1. ì„¸ì…˜ ì´ˆê¸°í™” - ë©”ì¸ í˜ì´ì§€ ë°©ë¬¸
            main_url = f"{self.base_url}/main/main"
            logger.debug(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì†: {main_url}")
            
            main_resp = session.get(main_url)
            if main_resp.status_code != 200:
                logger.error(f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨", 
                           status_code=main_resp.status_code, url=main_url)
                raise ExternalServiceError("REGTECH", 
                                         f"ë©”ì¸ í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: HTTP {main_resp.status_code}")
            
            time.sleep(1)
            
            # 2. ë¡œê·¸ì¸ í¼ ì ‘ì†
            login_form_url = f"{self.base_url}/login/loginForm"
            logger.debug(f"ë¡œê·¸ì¸ í¼ ì ‘ì†: {login_form_url}")
            
            form_resp = session.get(login_form_url)
            if form_resp.status_code != 200:
                logger.error(f"ë¡œê·¸ì¸ í¼ ì ‘ì† ì‹¤íŒ¨: {form_resp.status_code}")
                return False
            
            time.sleep(1)
            
            # 3. ë¡œê·¸ì¸ ìˆ˜í–‰
            login_url = f"{self.base_url}/login/addLogin"
            login_data = {
                'login_error': '',
                'txId': '',
                'token': '',
                'memberId': '',
                'smsTimeExcess': 'N',
                'username': username,
                'password': password
            }
            
            login_headers = {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Origin': self.base_url,
                'Referer': login_form_url
            }
            
            logger.debug(f"ë¡œê·¸ì¸ ìš”ì²­: {login_url}")
            login_resp = session.post(
                login_url,
                data=login_data,
                headers=login_headers,
                allow_redirects=True
            )
            
            # ë¡œê·¸ì¸ ì‘ë‹µ ë¶„ì„
            if login_resp.status_code != 200:
                logger.error(f"ë¡œê·¸ì¸ ìš”ì²­ ì‹¤íŒ¨: {login_resp.status_code}")
                return False
            
            # ë¡œê·¸ì¸ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            if 'login/loginForm' in login_resp.url:
                # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ = ì‹¤íŒ¨
                logger.error("ë¡œê·¸ì¸ ì‹¤íŒ¨: ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨")
                
                # ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
                if 'error=true' in login_resp.url:
                    logger.error("ë¡œê·¸ì¸ ì˜¤ë¥˜: ì¸ì¦ ì‹¤íŒ¨ (ì˜ëª»ëœ ìê²©ì¦ëª… ë˜ëŠ” ì •ì±… ë³€ê²½)")
                
                return False
            
            # Bearer Token í™•ì¸
            bearer_token = None
            for cookie in session.cookies:
                if cookie.name == 'regtech-va' and cookie.value.startswith('Bearer'):
                    bearer_token = cookie.value
                    session.headers['Authorization'] = bearer_token
                    logger.info(f"âœ… Bearer Token íšë“ ì„±ê³µ (ê¸¸ì´: {len(bearer_token)})")
                    break
            
            if not bearer_token:
                logger.warning("Bearer Tokenì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ê³„ì† ì§„í–‰")
            
            # 4. ë¡œê·¸ì¸ í™•ì¸ - ë³´ì•ˆ ê¶Œê³  í˜ì´ì§€ ì ‘ê·¼
            verify_url = f"{self.base_url}/fcti/securityAdvisory/advisoryList"
            verify_resp = session.get(verify_url)
            
            if verify_resp.status_code == 200:
                logger.info("âœ… REGTECH ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸")
                return True
            else:
                logger.error(f"ë¡œê·¸ì¸ í™•ì¸ ì‹¤íŒ¨: {verify_resp.status_code}")
                return False
                
        except (Timeout, ConnectionError) as e:
            logger.error(f"REGTECH ì—°ê²° ì˜¤ë¥˜", exception=e)
            raise ExternalServiceError("REGTECH", f"ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        except RequestException as e:
            logger.error(f"REGTECH ìš”ì²­ ì˜¤ë¥˜", exception=e)
            raise ExternalServiceError("REGTECH", f"HTTP ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        except Exception as e:
            logger.error(f"REGTECH ë¡œê·¸ì¸ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜", exception=e)
            raise
    
    def _download_excel_data_enhanced(self, session: requests.Session, 
                                     start_date: str, end_date: str) -> List[BlacklistEntry]:
        """ê°•í™”ëœ Excel ë‹¤ìš´ë¡œë“œ"""
        try:
            excel_url = f"{self.base_url}/fcti/securityAdvisory/advisoryListDownloadXlsx"
            
            # POST ë°ì´í„°
            excel_data = {
                'page': '0',
                'tabSort': 'blacklist',
                'excelDownload': 'blacklist,',
                'cveId': '',
                'ipId': '',
                'estId': '',
                'startDate': start_date,
                'endDate': end_date,
                'findCondition': 'all',
                'findKeyword': '',
                'excelDown': 'blacklist',
                'size': '10'
            }
            
            # í—¤ë”
            headers = {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Origin': self.base_url,
                'Referer': f"{self.base_url}/fcti/securityAdvisory/advisoryList"
            }
            
            logger.info(f"ğŸ“¥ Excel ë‹¤ìš´ë¡œë“œ ì‹œë„... ({start_date} ~ {end_date})")
            
            response = session.post(
                excel_url,
                data=excel_data,
                headers=headers,
                stream=True
            )
            
            if response.status_code != 200:
                logger.error(f"Excel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
                return []
            
            # Content-Type í™•ì¸
            content_type = response.headers.get('Content-Type', '')
            content_length = response.headers.get('Content-Length', '0')
            
            logger.info(f"ì‘ë‹µ Content-Type: {content_type}, Length: {content_length}")
            
            # Excel íŒŒì¼ í™•ì¸
            if not any(x in content_type.lower() for x in ['excel', 'spreadsheet', 'octet-stream', 'download']):
                # HTML ì‘ë‹µì¸ ê²½ìš° ë‚´ìš© í™•ì¸
                if 'text/html' in content_type:
                    html_content = response.text[:500]
                    logger.error(f"Excel ëŒ€ì‹  HTML ì‘ë‹µ ë°›ìŒ: {html_content}")
                    
                    # ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ëœ ê²½ìš°
                    if 'login' in html_content.lower():
                        logger.error("ì„¸ì…˜ ë§Œë£Œ - ì¬ë¡œê·¸ì¸ í•„ìš”")
                        
                return []
            
            # Excel íŒŒì¼ íŒŒì‹±
            try:
                excel_content = io.BytesIO(response.content)
                df = pd.read_excel(excel_content)
                
                logger.info(f"âœ… Excel ë¡œë“œ ì„±ê³µ: {len(df)} í–‰")
                logger.info(f"ì»¬ëŸ¼: {list(df.columns)}")
                
                # IP ì»¬ëŸ¼ ì°¾ê¸°
                ip_column = None
                for col in df.columns:
                    if 'IP' in str(col).upper():
                        ip_column = col
                        break
                
                if not ip_column:
                    logger.error("IP ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(df.columns)}")
                    return []
                
                # ë°ì´í„° ë³€í™˜
                entries = []
                for idx, row in df.iterrows():
                    try:
                        ip = str(row[ip_column]).strip()
                        
                        if not self._is_valid_ip(ip):
                            continue
                        
                        # ë‹¤ë¥¸ í•„ë“œ ì¶”ì¶œ
                        country = str(row.get('êµ­ê°€', 'Unknown')).strip()
                        reason = str(row.get('ë“±ë¡ì‚¬ìœ ', 'REGTECH')).strip()
                        
                        # ë‚ ì§œ ì²˜ë¦¬
                        reg_date_raw = row.get('ë“±ë¡ì¼')
                        if pd.notna(reg_date_raw):
                            if isinstance(reg_date_raw, pd.Timestamp):
                                reg_date = reg_date_raw.strftime('%Y-%m-%d')
                            else:
                                try:
                                    reg_date = pd.to_datetime(str(reg_date_raw)).strftime('%Y-%m-%d')
                                except:
                                    reg_date = datetime.now().strftime('%Y-%m-%d')
                        else:
                            reg_date = datetime.now().strftime('%Y-%m-%d')
                        
                        entry = BlacklistEntry(
                            ip_address=ip,
                            country=country,
                            reason=reason,
                            source='REGTECH',
                            reg_date=reg_date,
                            is_active=True,
                            threat_level='high',
                            source_details={'type': 'REGTECH', 'attack': reason}
                        )
                        
                        entries.append(entry)
                        
                    except Exception as e:
                        logger.debug(f"í–‰ ì²˜ë¦¬ ì˜¤ë¥˜: {e}, row_index: {idx}, ip: {ip if 'ip' in locals() else 'N/A'}")
                        continue
                
                # ì¤‘ë³µ ì œê±°
                unique_entries = self._remove_duplicates(entries)
                
                logger.info(f"âœ… Excelì—ì„œ {len(unique_entries)}ê°œ ê³ ìœ  IP ì¶”ì¶œ")
                return unique_entries
                
            except pd.errors.ParserError as e:
                logger.error(f"Excel íŒŒì‹± ì˜¤ë¥˜", exception=e)
                raise CollectionError("REGTECH", "Excel íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨")
            except Exception as e:
                logger.error(f"Excel ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜", exception=e)
                raise CollectionError("REGTECH", f"Excel ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                
        except RequestException as e:
            logger.error(f"Excel ë‹¤ìš´ë¡œë“œ HTTP ì˜¤ë¥˜", exception=e)
            raise ExternalServiceError("REGTECH", f"Excel ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        except Exception as e:
            logger.error(f"Excel ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜", exception=e)
            raise
    
    def _collect_html_enhanced(self, session: requests.Session, start_date: str, 
                              end_date: str, max_pages: int, page_size: int) -> List[BlacklistEntry]:
        """ê°•í™”ëœ HTML ìˆ˜ì§‘"""
        collected_ips = []
        
        for page in range(max_pages):
            try:
                logger.info(f"ğŸ“„ HTML í˜ì´ì§€ {page + 1}/{max_pages} ìˆ˜ì§‘ ì¤‘...")
                
                # POST ë°ì´í„°
                post_data = {
                    'page': str(page),
                    'tabSort': 'blacklist',
                    'excelDownload': '',
                    'cveId': '',
                    'ipId': '',
                    'estId': '',
                    'startDate': start_date,
                    'endDate': end_date,
                    'findCondition': 'all',
                    'findKeyword': '',
                    'size': '1000'  # í˜ì´ì§€ë‹¹ 1000ê°œ ìš”ì²­
                }
                
                # í—¤ë”
                headers = {
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'X-Requested-With': 'XMLHttpRequest',
                    'Referer': f"{self.base_url}/fcti/securityAdvisory/advisoryList"
                }
                
                response = session.post(
                    f"{self.base_url}/fcti/securityAdvisory/advisoryList",
                    data=post_data,
                    headers=headers
                )
                
                if response.status_code != 200:
                    logger.error(f"í˜ì´ì§€ {page + 1} ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
                    break
                
                # HTML íŒŒì‹±
                page_entries = self._parse_html_enhanced(response.text)
                
                if page_entries:
                    collected_ips.extend(page_entries)
                    logger.info(f"í˜ì´ì§€ {page + 1}: {len(page_entries)}ê°œ IP ìˆ˜ì§‘")
                    
                    # ë§ˆì§€ë§‰ í˜ì´ì§€ í™•ì¸
                    if len(page_entries) < page_size:
                        logger.info("ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                        break
                else:
                    logger.warning(f"í˜ì´ì§€ {page + 1}: ë°ì´í„° ì—†ìŒ")
                    break
                    
                # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page + 1} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                self.stats.error_count += 1
                break
        
        # ì¤‘ë³µ ì œê±°
        unique_entries = self._remove_duplicates(collected_ips)
        
        logger.info(f"HTML ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_entries)}ê°œ ê³ ìœ  IP")
        return unique_entries
    
    def _parse_html_enhanced(self, html_content: str) -> List[BlacklistEntry]:
        """ê°•í™”ëœ HTML íŒŒì‹±"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            entries = []
            
            # ë‹¤ì–‘í•œ í…Œì´ë¸” êµ¬ì¡° ì‹œë„
            tables = soup.find_all('table')
            
            for table in tables:
                # ìš”ì£¼ì˜ IP í…Œì´ë¸” ì°¾ê¸°
                caption = table.find('caption')
                if caption and 'ìš”ì£¼ì˜' in caption.text:
                    logger.debug("ìš”ì£¼ì˜ IP í…Œì´ë¸” ë°œê²¬")
                    
                    # tbody ì°¾ê¸°
                    tbody = table.find('tbody')
                    rows = tbody.find_all('tr') if tbody else table.find_all('tr')[1:]
                    
                    for row in rows:
                        cells = row.find_all('td')
                        
                        if len(cells) >= 4:  # ìµœì†Œ í•„ìš” ì»¬ëŸ¼ ìˆ˜
                            try:
                                ip = cells[0].get_text(strip=True)
                                
                                if not self._is_valid_ip(ip):
                                    continue
                                
                                country = cells[1].get_text(strip=True) if len(cells) > 1 else 'Unknown'
                                reason = cells[2].get_text(strip=True) if len(cells) > 2 else 'REGTECH'
                                reg_date = cells[3].get_text(strip=True) if len(cells) > 3 else datetime.now().strftime('%Y-%m-%d')
                                
                                entry = BlacklistEntry(
                                    ip_address=ip,
                                    country=country,
                                    reason=reason,
                                    source='REGTECH',
                                    reg_date=reg_date,
                                    is_active=True,
                                    threat_level='high'
                                )
                                
                                entries.append(entry)
                                
                            except Exception as e:
                                logger.debug(f"HTML í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}, row_data: {str(cells)[:100]}")
                                continue
            
            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ êµ¬ì¡° ì‹œë„
            if not entries:
                # div ê¸°ë°˜ êµ¬ì¡° ë“± ëŒ€ì²´ íŒŒì‹± ì‹œë„
                logger.debug("í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì°¾ì§€ ëª»í•¨, ëŒ€ì²´ êµ¬ì¡° ê²€ìƒ‰")
                
                # IP íŒ¨í„´ìœ¼ë¡œ ì§ì ‘ ê²€ìƒ‰
                ip_pattern = re.compile(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b')
                potential_ips = ip_pattern.findall(html_content)
                
                for ip in potential_ips[:10]:  # ìµœëŒ€ 10ê°œë§Œ í™•ì¸
                    if self._is_valid_ip(ip):
                        logger.debug(f"íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ IP ë°œê²¬: {ip}")
            
            return entries
            
        except Exception as e:
            logger.error(f"HTML íŒŒì‹± ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜", exception=e)
            raise CollectionError("REGTECH", f"HTML íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
    
    def _is_valid_ip(self, ip: str) -> bool:
        """IP ìœ íš¨ì„± ê²€ì¦ (ê°•í™”)"""
        try:
            if not ip or not isinstance(ip, str):
                return False
            
            # ê¸°ë³¸ í˜•ì‹ ê²€ì¦
            parts = ip.split('.')
            if len(parts) != 4:
                return False
            
            # ê° ë¶€ë¶„ ê²€ì¦
            for part in parts:
                try:
                    num = int(part)
                    if not 0 <= num <= 255:
                        return False
                except ValueError:
                    return False
            
            # íŠ¹ìˆ˜ IP ì œì™¸
            if parts[0] in ['0', '127', '255']:
                return False
            
            # ì‚¬ì„¤ IP ì œì™¸
            if parts[0] == '10':
                return False
            if parts[0] == '192' and parts[1] == '168':
                return False
            if parts[0] == '172' and 16 <= int(parts[1]) <= 31:
                return False
            
            return True
            
        except Exception:
            return False
    
    def _remove_duplicates(self, entries: List[BlacklistEntry]) -> List[BlacklistEntry]:
        """ì¤‘ë³µ ì œê±°"""
        unique_entries = []
        seen_ips = set()
        
        for entry in entries:
            if entry.ip_address not in seen_ips:
                unique_entries.append(entry)
                seen_ips.add(entry.ip_address)
            else:
                self.stats.duplicate_count += 1
        
        if self.stats.duplicate_count > 0:
            logger.info(f"ì¤‘ë³µ ì œê±°: {self.stats.duplicate_count}ê°œ")
        
        return unique_entries
    
    def _log_collection_summary(self):
        """ìˆ˜ì§‘ ìš”ì•½ ë¡œê·¸"""
        duration = self.stats.end_time - self.stats.start_time
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š REGTECH ìˆ˜ì§‘ ìš”ì•½")
        logger.info(f"  - ìˆ˜ì§‘ ë°©ë²•: {self.stats.source_method}")
        logger.info(f"  - ì´ ìˆ˜ì§‘ IP: {self.stats.total_collected}ê°œ")
        logger.info(f"  - ì¤‘ë³µ ì œê±°: {self.stats.duplicate_count}ê°œ")
        logger.info(f"  - ì†Œìš” ì‹œê°„: {duration}")
        logger.info(f"  - ì¸ì¦ ì‹œë„: {self.stats.auth_attempts}íšŒ")
        logger.info(f"  - Excel ì‹œë„: {self.stats.excel_attempts}íšŒ")
        logger.info(f"  - HTML ì‹œë„: {self.stats.html_attempts}íšŒ")
        logger.info(f"  - ì˜¤ë¥˜ íšŸìˆ˜: {self.stats.error_count}íšŒ")
        logger.info("=" * 60)
    
    def _log_failure_details(self):
        """ì‹¤íŒ¨ ìƒì„¸ ë¡œê·¸"""
        logger.error("=" * 60)
        logger.error("âŒ REGTECH ìˆ˜ì§‘ ì‹¤íŒ¨ ìƒì„¸")
        logger.error(f"  - ì¸ì¦ ì‹œë„: {self.stats.auth_attempts}íšŒ")
        logger.error(f"  - Excel ì‹œë„: {self.stats.excel_attempts}íšŒ")
        logger.error(f"  - HTML ì‹œë„: {self.stats.html_attempts}íšŒ")
        logger.error(f"  - ë§ˆì§€ë§‰ ì˜¤ë¥˜: {self.stats.last_error}")
        logger.error("=" * 60)


def create_enhanced_regtech_collector(data_dir: str, cache_backend=None) -> EnhancedRegtechCollector:
    """Enhanced REGTECH ìˆ˜ì§‘ê¸° íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return EnhancedRegtechCollector(data_dir, cache_backend)