#!/usr/bin/env python3
"""
REGTECH ê°œì„ ëœ ìˆ˜ì§‘ê¸° - PowerShell ìŠ¤í¬ë¦½íŠ¸ ë¡œì§ ê¸°ë°˜
ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦ìœ¼ë¡œ ì•ˆì •ì ì¸ ë°ì´í„° ìˆ˜ì§‘
"""

import os
import json
import logging
import time
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
import re
from bs4 import BeautifulSoup

from src.core.models import BlacklistEntry

logger = logging.getLogger(__name__)

@dataclass
class RegtechCollectionStats:
    """REGTECH ìˆ˜ì§‘ í†µê³„"""
    start_time: datetime
    end_time: Optional[datetime] = None
    total_collected: int = 0
    pages_processed: int = 0
    error_count: int = 0
    duplicate_count: int = 0

class RegtechEnhancedCollector:
    """
    REGTECH ê°œì„ ëœ ìˆ˜ì§‘ê¸°
    - ì¿ í‚¤ ê¸°ë°˜ ì¸ì¦
    - í˜ì´ì§€ ìë™ ìˆœíšŒ
    - í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì •
    """
    
    def __init__(self):
        self.base_url = "https://regtech.fsec.or.kr"
        self.api_url = f"{self.base_url}/fcti/securityAdvisory/advisoryList"
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¿ í‚¤ ì„¤ì • ë¡œë“œ
        self.cookies = self._load_cookies_from_env()
        
        # ìš”ì²­ í—¤ë” ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Referer': f'{self.base_url}/fcti/securityAdvisory/advisoryList',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/png,image/svg+xml,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin',
        }
        
        # ì„¸ì…˜ ìƒì„±
        self.session = requests.Session()
        self.session.headers.update(self.headers)
        self.session.cookies.update(self.cookies)
        
        logger.info("âœ… REGTECH ê°œì„ ëœ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_cookies_from_env(self) -> Dict[str, str]:
        """ë°ì´í„°ë² ì´ìŠ¤ ìš°ì„ , í™˜ê²½ë³€ìˆ˜ ëŒ€ì²´ë¡œ ì¿ í‚¤ ì„¤ì • ë¡œë“œ"""
        cookies = {}
        
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¿ í‚¤ ì„¤ì • ë¡œë“œ ì‹œë„
        try:
            from src.models.settings import get_settings_manager
            settings_manager = get_settings_manager()
            
            # DBì—ì„œ REGTECH ì¿ í‚¤ ì„¤ì •ë“¤ ì¡°íšŒ
            db_cookie_mapping = {
                'regtech_cookie_ga': '_ga',
                'regtech_cookie_front': 'regtech-front',
                'regtech_cookie_va': 'regtech-va', 
                'regtech_cookie_ga_analytics': '_ga_7WRDYHF66J'
            }
            
            db_cookies_found = 0
            for db_key, cookie_name in db_cookie_mapping.items():
                cookie_value = settings_manager.get_setting(db_key)
                if cookie_value and cookie_value.strip():
                    cookies[cookie_name] = cookie_value.strip()
                    logger.info(f"âœ… DBì—ì„œ ì¿ í‚¤ ë¡œë“œë¨: {cookie_name}")
                    db_cookies_found += 1
            
            if db_cookies_found > 0:
                logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ {db_cookies_found}ê°œ ì¿ í‚¤ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
                return cookies
            else:
                logger.info("â„¹ï¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ëœ ì¿ í‚¤ ì„¤ì •ì´ ì—†ìŒ, í™˜ê²½ë³€ìˆ˜ í™•ì¸")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {e}, í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©")
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¿ í‚¤ ì„¤ì • ë¡œë“œ (DB ëŒ€ì²´)
        env_cookie_mapping = {
            'REGTECH_COOKIE_GA': '_ga',
            'REGTECH_COOKIE_FRONT': 'regtech-front', 
            'REGTECH_COOKIE_VA': 'regtech-va',
            'REGTECH_COOKIE_GA_ANALYTICS': '_ga_7WRDYHF66J'
        }
        
        env_cookies_found = 0
        for env_key, cookie_name in env_cookie_mapping.items():
            cookie_value = os.getenv(env_key)
            if cookie_value and cookie_value.strip():
                cookies[cookie_name] = cookie_value.strip()
                logger.info(f"âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¿ í‚¤ ë¡œë“œë¨: {cookie_name}")
                env_cookies_found += 1
        
        # ì¿ í‚¤ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
        if not cookies:
            logger.warning("âš ï¸ DBì™€ í™˜ê²½ë³€ìˆ˜ì—ì„œ ì¿ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            cookies = {
                '_ga': 'GA1.1.1689204774.1752555033',
                'regtech-front': '2F3B7CE1B26084FCD546BDB56CE9ABAC',
                'regtech-va': 'Bearer...',  # ì‹¤ì œ í† í°ìœ¼ë¡œ êµì²´ í•„ìš”
                '_ga_7WRDYHF66J': 'GS2.1.s1752743223$o3$g1$t1752746099$j38$l0$h0'
            }
        else:
            if env_cookies_found > 0:
                logger.info(f"âœ… í™˜ê²½ë³€ìˆ˜ì—ì„œ {env_cookies_found}ê°œ ì¿ í‚¤ ì„¤ì • ë¡œë“œ ì™„ë£Œ")
        
        return cookies
    
    def collect_from_web(self, start_date: str = None, end_date: str = None) -> List[BlacklistEntry]:
        """
        ì›¹ì—ì„œ ë°ì´í„° ìˆ˜ì§‘ - ëª¨ë“  í˜ì´ì§€ ìë™ ìˆœíšŒ
        
        Args:
            start_date: ì‹œì‘ì¼ (YYYYMMDD) - ê¸°ë³¸ê°’: 3ê°œì›” ì „
            end_date: ì¢…ë£Œì¼ (YYYYMMDD) - ê¸°ë³¸ê°’: ì˜¤ëŠ˜
            
        Returns:
            ìˆ˜ì§‘ëœ BlacklistEntry ë¦¬ìŠ¤íŠ¸
        """
        stats = RegtechCollectionStats(start_time=datetime.now())
        
        try:
            # ë‚ ì§œ ì„¤ì •
            if not start_date:
                start_date = (datetime.now() - timedelta(days=90)).strftime('%Y%m%d')
            if not end_date:
                end_date = datetime.now().strftime('%Y%m%d')
            
            logger.info(f"ğŸš€ REGTECH ìˆ˜ì§‘ ì‹œì‘ - ê¸°ê°„: {start_date} ~ {end_date}")
            
            all_entries = []
            page = 0
            
            while True:
                logger.info(f"ğŸ“„ í˜ì´ì§€ {page} ìš”ì²­ ì¤‘...")
                
                # POST ìš”ì²­ ë°ì´í„°
                post_data = {
                    'page': str(page),
                    'tabSort': 'blacklist',
                    'startDate': start_date,
                    'endDate': end_date,
                    'findCondition': 'all',
                    'findKeyword': '',
                    'size': '50'
                }
                
                try:
                    # ìš”ì²­ ì‹¤í–‰
                    response = self.session.post(
                        self.api_url,
                        data=post_data,
                        timeout=30,
                        verify=False  # SSL ê²€ì¦ ë¹„í™œì„±í™” (í•„ìš”ì‹œ)
                    )
                    
                    response.raise_for_status()
                    stats.pages_processed += 1
                    
                    # HTML íŒŒì‹±
                    page_entries = self._parse_html_response(response.text)
                    
                    if not page_entries:
                        logger.info(f"âœ… í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ì–´ ìˆ˜ì§‘ ì¢…ë£Œ")
                        break
                    
                    all_entries.extend(page_entries)
                    logger.info(f"   -> {len(page_entries)}ê°œ í•­ëª© ë°œê²¬ (ëˆ„ì : {len(all_entries)}ê°œ)")
                    
                    page += 1
                    
                    # ìš”ì²­ ê°„ê²© (ì„œë²„ ë¶€í•˜ ë°©ì§€)
                    time.sleep(1)
                    
                except requests.exceptions.RequestException as e:
                    logger.error(f"âŒ í˜ì´ì§€ {page} ìš”ì²­ ì‹¤íŒ¨: {e}")
                    stats.error_count += 1
                    break
                except Exception as e:
                    logger.error(f"âŒ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    stats.error_count += 1
                    continue
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            stats.end_time = datetime.now()
            stats.total_collected = len(all_entries)
            
            # ê²°ê³¼ ë¡œê¹…
            duration = (stats.end_time - stats.start_time).total_seconds()
            logger.info(f"ğŸ¯ REGTECH ìˆ˜ì§‘ ì™„ë£Œ:")
            logger.info(f"   - ìˆ˜ì§‘ ê¸°ê°„: {start_date} ~ {end_date}")
            logger.info(f"   - ì²˜ë¦¬ëœ í˜ì´ì§€: {stats.pages_processed}")
            logger.info(f"   - ì´ ìˆ˜ì§‘ IP: {stats.total_collected}")
            logger.info(f"   - ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")
            logger.info(f"   - ì˜¤ë¥˜ íšŸìˆ˜: {stats.error_count}")
            
            return all_entries
            
        except Exception as e:
            logger.error(f"âŒ REGTECH ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_html_response(self, html_content: str) -> List[BlacklistEntry]:
        """HTML ì‘ë‹µì—ì„œ IP ë°ì´í„° ì¶”ì¶œ"""
        entries = []
        
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # "ìš”ì£¼ì˜ IP ëª©ë¡" í…Œì´ë¸” ì°¾ê¸°
            table = None
            for caption in soup.find_all('caption'):
                if 'ìš”ì£¼ì˜ IP ëª©ë¡' in caption.get_text():
                    table = caption.find_parent('table')
                    break
            
            if not table:
                logger.warning("âš ï¸ ìš”ì£¼ì˜ IP ëª©ë¡ í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return entries
            
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì²´í¬
            if 'ì´ <em>0</em>' in html_content:
                logger.info("â„¹ï¸ í•´ë‹¹ í˜ì´ì§€ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                return entries
            
            # tbodyì—ì„œ í–‰ ì¶”ì¶œ
            tbody = table.find('tbody')
            if not tbody:
                logger.warning("âš ï¸ tbodyë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return entries
            
            rows = tbody.find_all('tr')
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 6:
                    try:
                        ip_address = cells[0].get_text().strip()
                        country = cells[1].get_text().strip()
                        reason = cells[2].get_text().strip()
                        registration_date = cells[3].get_text().strip()
                        release_date = cells[4].get_text().strip()
                        
                        # IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì¦
                        if self._is_valid_ip(ip_address):
                            # ë“±ë¡ì¼ íŒŒì‹±
                            detection_date = self._parse_date(registration_date)
                            
                            entry = BlacklistEntry(
                                ip_address=ip_address,
                                source='REGTECH',
                                detection_date=detection_date,
                                reason=f"{reason} ({country})",
                                threat_level='medium',
                                is_active=True
                            )
                            
                            entries.append(entry)
                        else:
                            logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ IP ì£¼ì†Œ: {ip_address}")
                            
                    except Exception as e:
                        logger.warning(f"âš ï¸ í–‰ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        continue
            
        except Exception as e:
            logger.error(f"âŒ HTML íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        return entries
    
    def _is_valid_ip(self, ip_address: str) -> bool:
        """IP ì£¼ì†Œ ìœ íš¨ì„± ê²€ì¦"""
        import ipaddress
        try:
            ipaddress.ip_address(ip_address)
            return True
        except ValueError:
            return False
    
    def _parse_date(self, date_str: str) -> datetime:
        """ë‚ ì§œ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
            date_formats = [
                '%Y-%m-%d',
                '%Y.%m.%d',
                '%Y/%m/%d',
                '%Y-%m-%d %H:%M:%S',
                '%Y.%m.%d %H:%M:%S'
            ]
            
            for date_format in date_formats:
                try:
                    return datetime.strptime(date_str.strip(), date_format)
                except ValueError:
                    continue
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
            logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str}, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
            return datetime.now()
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜: {e}, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
            return datetime.now()
    
    def test_connection(self) -> bool:
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            logger.info("ğŸ” REGTECH ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            response = self.session.get(
                f"{self.base_url}/fcti/securityAdvisory/advisoryList",
                timeout=10
            )
            
            success = response.status_code == 200
            
            if success:
                logger.info("âœ… REGTECH ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                
                # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
                if 'regtech-va' in self.session.cookies:
                    logger.info("âœ… ì¸ì¦ ìƒíƒœ í™•ì¸ë¨")
                else:
                    logger.warning("âš ï¸ ì¸ì¦ ì¿ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                    
            else:
                logger.error(f"âŒ REGTECH ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: HTTP {response.status_code}")
            
            return success
            
        except Exception as e:
            logger.error(f"âŒ REGTECH ì—°ê²° í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
            return False


def create_regtech_collector():
    """REGTECH ìˆ˜ì§‘ê¸° íŒ©í† ë¦¬ í•¨ìˆ˜"""
    return RegtechEnhancedCollector()


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    collector = RegtechEnhancedCollector()
    
    if collector.test_connection():
        entries = collector.collect_from_web()
        print(f"ìˆ˜ì§‘ëœ IP ê°œìˆ˜: {len(entries)}")
        
        if entries:
            print("ì²« ë²ˆì§¸ IP ì˜ˆì‹œ:")
            print(f"  IP: {entries[0].ip_address}")
            print(f"  ì¶œì²˜: {entries[0].source}")
            print(f"  íƒì§€ì¼: {entries[0].detection_date}")
            print(f"  ì´ìœ : {entries[0].reason}")
    else:
        print("ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")