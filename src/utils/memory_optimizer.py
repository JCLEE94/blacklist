"""
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ Blacklist ì‹œìŠ¤í…œì˜ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ
ëŒ€ëŸ‰ ë°ì´í„° ì²˜ë¦¬, ë©”ëª¨ë¦¬ í’€ë§, ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import gc
import sqlite3
import sys
import threading
import time
import weakref
from collections import defaultdict
from contextlib import contextmanager
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, Iterator, List, Optional, Union

import psutil

try:
    import numpy as np

    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False

from loguru import logger


@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„"""

    total_memory_mb: float
    available_memory_mb: float
    used_memory_mb: float
    memory_percent: float
    process_memory_mb: float
    gc_collections: Dict[int, int]
    timestamp: datetime


class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ê´€ë¦¬ì"""

    def __init__(
        self,
        max_memory_percent: float = 80.0,
        gc_threshold_mb: float = 100.0,
        monitoring_interval: float = 30.0,
    ):
        self.max_memory_percent = max_memory_percent
        self.gc_threshold_mb = gc_threshold_mb
        self.monitoring_interval = monitoring_interval

        # ë©”ëª¨ë¦¬ í’€
        self.object_pools = defaultdict(list)
        self.pool_locks = defaultdict(threading.Lock)

        # ëª¨ë‹ˆí„°ë§
        self.memory_history = []
        self.max_history_size = 100
        self.monitoring_active = False
        self.monitoring_thread = None

        # í†µê³„
        self.optimization_stats = {
            "pool_hits": 0,
            "pool_misses": 0,
            "gc_forced": 0,
            "memory_warnings": 0,
            "chunked_operations": 0,
        }

        logger.info(
            f"MemoryOptimizer initialized: max_memory={max_memory_percent}%, "
            f"gc_threshold={gc_threshold_mb}MB"
        )

    def get_memory_stats(self) -> MemoryStats:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„"""
        # ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
        memory = psutil.virtual_memory()

        # í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬
        process = psutil.Process()
        process_memory = process.memory_info().rss / 1024 / 1024  # MB

        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ í†µê³„
        gc_stats = {}
        for i in range(3):
            gc_stats[i] = gc.get_count()[i]

        return MemoryStats(
            total_memory_mb=memory.total / 1024 / 1024,
            available_memory_mb=memory.available / 1024 / 1024,
            used_memory_mb=memory.used / 1024 / 1024,
            memory_percent=memory.percent,
            process_memory_mb=process_memory,
            gc_collections=gc_stats,
            timestamp=datetime.now(),
        )

    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒí™© í™•ì¸"""
        stats = self.get_memory_stats()

        if stats.memory_percent > self.max_memory_percent:
            self.optimization_stats["memory_warnings"] += 1
            logger.warning(
                f"High memory usage: {stats.memory_percent:.1f}% "
                f"(process: {stats.process_memory_mb:.1f}MB)"
            )
            return True

        return False

    def force_gc_if_needed(self) -> bool:
        """í•„ìš”ì‹œ ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"""
        stats = self.get_memory_stats()

        if (
            stats.memory_percent > self.max_memory_percent * 0.8
            or stats.process_memory_mb > self.gc_threshold_mb
        ):
            logger.info("Forcing garbage collection...")
            gc.collect()
            self.optimization_stats["gc_forced"] += 1

            # í›„ í†µê³„
            new_stats = self.get_memory_stats()
            freed_mb = stats.process_memory_mb - new_stats.process_memory_mb
            logger.info(f"GC completed, freed {freed_mb:.1f}MB memory")

            return True

        return False

    @contextmanager
    def chunked_processing(self, data: List[Any], chunk_size: int = 1000):
        """ëŒ€ëŸ‰ ë°ì´í„°ì˜ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬"""
        self.optimization_stats["chunked_operations"] += 1

        try:
            total_chunks = (len(data) + chunk_size - 1) // chunk_size
            logger.info(
                f"Processing {len(data)} items in {total_chunks} chunks of {chunk_size}"
            )

            for i in range(0, len(data), chunk_size):
                chunk = data[i : i + chunk_size]
                chunk_num = i // chunk_size + 1

                # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸
                if chunk_num % 10 == 0:  # 10ì²­í¬ë§ˆë‹¤ í™•ì¸
                    if self.check_memory_pressure():
                        self.force_gc_if_needed()

                yield chunk_num, chunk, total_chunks

                # ì²­í¬ ì²˜ë¦¬ í›„ ëª…ì‹œì ìœ¼ë¡œ ì°¸ì¡° í•´ì œ
                del chunk

        except Exception as e:
            logger.error(f"Chunked processing error: {e}")
            raise

    def get_object_from_pool(self, object_type: str, factory: callable = None) -> Any:
        """ê°ì²´ í’€ì—ì„œ ê°ì²´ íšë“"""
        with self.pool_locks[object_type]:
            if self.object_pools[object_type]:
                obj = self.object_pools[object_type].pop()
                self.optimization_stats["pool_hits"] += 1
                return obj
            elif factory:
                self.optimization_stats["pool_misses"] += 1
                return factory()
            else:
                self.optimization_stats["pool_misses"] += 1
                return None

    def return_object_to_pool(
        self, object_type: str, obj: Any, reset_func: callable = None
    ):
        """ê°ì²´ë¥¼ í’€ì— ë°˜í™˜"""
        if reset_func:
            reset_func(obj)

        with self.pool_locks[object_type]:
            # í’€ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
            if len(self.object_pools[object_type]) < 50:
                self.object_pools[object_type].append(obj)

    def optimize_database_operations(
        self, db_path: str, operations: List[str]
    ) -> List[Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ì‚° ë©”ëª¨ë¦¬ ìµœì í™”"""
        results = []

        # ë‹¨ì¼ ì—°ê²°ë¡œ ëª¨ë“  ì‘ì—… ìˆ˜í–‰
        with sqlite3.connect(db_path) as conn:
            conn.execute("PRAGMA cache_size = -64000")  # 64MB ìºì‹œ
            conn.execute("PRAGMA temp_store = MEMORY")

            cursor = conn.cursor()

            try:
                for i, operation in enumerate(operations):
                    cursor.execute(operation)

                    # ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ fetchall
                    if operation.strip().upper().startswith("SELECT"):
                        result = cursor.fetchall()
                        results.append(result)
                    else:
                        results.append(cursor.rowcount)

                    # ì£¼ê¸°ì  ì»¤ë°‹ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                    if i % 100 == 0:
                        conn.commit()

                conn.commit()

            except Exception as e:
                conn.rollback()
                logger.error(f"Database operation failed: {e}")
                raise

        return results

    def efficient_ip_processing(self, ip_list: List[str]) -> List[str]:
        """ëŒ€ëŸ‰ IP ì²˜ë¦¬ ë©”ëª¨ë¦¬ ìµœì í™”"""
        if not ip_list:
            return []

        # numpy ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ë²¡í„°í™” ì²˜ë¦¬
        if HAS_NUMPY and len(ip_list) > 10000:
            logger.info(f"Using numpy for efficient processing of {len(ip_list)} IPs")

            # numpy ë°°ì—´ë¡œ ë³€í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            ip_array = np.array(ip_list, dtype="U15")  # ìµœëŒ€ 15ì IP ì£¼ì†Œ

            # ì¤‘ë³µ ì œê±° (numpyëŠ” ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            unique_ips = np.unique(ip_array)

            return unique_ips.tolist()

        else:
            # í‘œì¤€ Python ìµœì í™”
            logger.info(f"Using standard Python for processing {len(ip_list)} IPs")

            # ì§‘í•©ì„ ì‚¬ìš©í•œ ì¤‘ë³µ ì œê±° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            unique_ips = set()

            # ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            with self.chunked_processing(ip_list, chunk_size=5000) as chunks:
                for chunk_num, chunk, total_chunks in chunks:
                    unique_ips.update(chunk)

                    if chunk_num % 5 == 0:
                        logger.debug(f"Processed chunk {chunk_num}/{total_chunks}")

            return list(unique_ips)

    def start_memory_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring_active:
            return

        self.monitoring_active = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop, daemon=True, name="memory_monitor"
        )
        self.monitoring_thread.start()
        logger.info("Memory monitoring started")

    def stop_memory_monitoring(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)
        logger.info("Memory monitoring stopped")

    def _monitoring_loop(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                stats = self.get_memory_stats()

                # íˆìŠ¤í† ë¦¬ ì €ì¥
                self.memory_history.append(stats)
                if len(self.memory_history) > self.max_history_size:
                    self.memory_history.pop(0)

                # ë©”ëª¨ë¦¬ ì••ë°• í™•ì¸ ë° ëŒ€ì‘
                if self.check_memory_pressure():
                    self.force_gc_if_needed()

                time.sleep(self.monitoring_interval)

            except Exception as e:
                logger.error(f"Memory monitoring error: {e}")
                time.sleep(self.monitoring_interval)

    def get_optimization_report(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ë³´ê³ ì„œ"""
        current_stats = self.get_memory_stats()

        # í’€ íš¨ìœ¨ì„± ê³„ì‚°
        total_pool_requests = (
            self.optimization_stats["pool_hits"]
            + self.optimization_stats["pool_misses"]
        )
        pool_hit_rate = (
            self.optimization_stats["pool_hits"] / total_pool_requests * 100
            if total_pool_requests > 0
            else 0
        )

        # ë©”ëª¨ë¦¬ íˆìŠ¤í† ë¦¬ ë¶„ì„
        if len(self.memory_history) > 1:
            memory_trend = (
                self.memory_history[-1].memory_percent
                - self.memory_history[0].memory_percent
            )
        else:
            memory_trend = 0

        return {
            "current_memory": {
                "total_mb": current_stats.total_memory_mb,
                "used_mb": current_stats.used_memory_mb,
                "available_mb": current_stats.available_memory_mb,
                "usage_percent": current_stats.memory_percent,
                "process_mb": current_stats.process_memory_mb,
            },
            "optimization_stats": {
                **self.optimization_stats,
                "pool_hit_rate_percent": round(pool_hit_rate, 2),
                "memory_trend_percent": round(memory_trend, 2),
            },
            "pool_status": {
                pool_type: len(objects)
                for pool_type, objects in self.object_pools.items()
            },
            "recommendations": self._generate_recommendations(current_stats),
            "timestamp": current_stats.timestamp.isoformat(),
        }

    def _generate_recommendations(self, stats: MemoryStats) -> List[str]:
        """ë©”ëª¨ë¦¬ ìµœì í™” ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []

        if stats.memory_percent > 90:
            recommendations.append(
                "ğŸš¨ Critical: System memory usage > 90%. Immediate optimization needed."
            )
        elif stats.memory_percent > 80:
            recommendations.append(
                "âš ï¸ Warning: High memory usage. Consider enabling chunked processing."
            )

        if stats.process_memory_mb > 500:
            recommendations.append(
                "ğŸ’¡ Process memory > 500MB. Consider using object pools."
            )

        if self.optimization_stats["gc_forced"] > 10:
            recommendations.append(
                "ğŸ”„ Frequent GC detected. Review data processing patterns."
            )

        pool_efficiency = (
            self.optimization_stats["pool_hits"]
            / (
                self.optimization_stats["pool_hits"]
                + self.optimization_stats["pool_misses"]
            )
            if self.optimization_stats["pool_hits"]
            + self.optimization_stats["pool_misses"]
            > 0
            else 0
        )

        if pool_efficiency < 0.5:
            recommendations.append(
                "ğŸ“Š Low object pool efficiency. Review pool usage patterns."
            )

        if not recommendations:
            recommendations.append("âœ… Memory usage is optimal.")

        return recommendations


# ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ìµœì í™”ê¸°
_global_memory_optimizer = None


def get_global_memory_optimizer() -> MemoryOptimizer:
    """ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ìµœì í™”ê¸° ë°˜í™˜"""
    global _global_memory_optimizer

    if _global_memory_optimizer is None:
        _global_memory_optimizer = MemoryOptimizer()

    return _global_memory_optimizer


def memory_efficient(chunk_size: int = 1000):
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ ë°ì½”ë ˆì´í„°"""

    def decorator(func):
        def wrapper(data, *args, **kwargs):
            optimizer = get_global_memory_optimizer()

            if isinstance(data, list) and len(data) > chunk_size:
                results = []
                with optimizer.chunked_processing(data, chunk_size) as chunks:
                    for chunk_num, chunk, total_chunks in chunks:
                        chunk_result = func(chunk, *args, **kwargs)
                        results.extend(
                            chunk_result
                            if isinstance(chunk_result, list)
                            else [chunk_result]
                        )
                return results
            else:
                return func(data, *args, **kwargs)

        return wrapper

    return decorator


if __name__ == "__main__":
    """ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ê²€ì¦"""
    import sys

    optimizer = MemoryOptimizer()
    all_tests_passed = True

    try:
        # í…ŒìŠ¤íŠ¸ 1: ë©”ëª¨ë¦¬ í†µê³„
        stats = optimizer.get_memory_stats()
        if stats.total_memory_mb <= 0:
            print("âŒ ë©”ëª¨ë¦¬ í†µê³„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            all_tests_passed = False

        # í…ŒìŠ¤íŠ¸ 2: ì²­í¬ ì²˜ë¦¬
        test_data = list(range(10000))
        processed_chunks = 0

        with optimizer.chunked_processing(test_data, chunk_size=1000) as chunks:
            for chunk_num, chunk, total_chunks in chunks:
                processed_chunks += 1
                if len(chunk) > 1000:
                    print("âŒ ì²­í¬ í¬ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                    all_tests_passed = False

        if processed_chunks != 10:
            print(f"âŒ ì²­í¬ ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: expected 10, got {processed_chunks}")
            all_tests_passed = False

        # í…ŒìŠ¤íŠ¸ 3: IP ì²˜ë¦¬ ìµœì í™”
        test_ips = [f"192.168.1.{i}" for i in range(1000)] * 2  # ì¤‘ë³µ í¬í•¨
        unique_ips = optimizer.efficient_ip_processing(test_ips)

        if len(unique_ips) != 1000:
            print(f"âŒ IP ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: expected 1000 unique IPs, got {len(unique_ips)}")
            all_tests_passed = False

        # í…ŒìŠ¤íŠ¸ 4: ê°ì²´ í’€
        def list_factory():
            return []

        obj1 = optimizer.get_object_from_pool("test_list", list_factory)
        optimizer.return_object_to_pool("test_list", obj1, lambda x: x.clear())
        obj2 = optimizer.get_object_from_pool("test_list")

        if obj1 is not obj2:
            print("âŒ ê°ì²´ í’€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            all_tests_passed = False

        # ìµœì¢… ë³´ê³ ì„œ
        report = optimizer.get_optimization_report()

        if all_tests_passed:
            print("âœ… ë©”ëª¨ë¦¬ ìµœì í™” ì‹œìŠ¤í…œ ê²€ì¦ ì™„ë£Œ - ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼")
            print(f"ğŸ“Š ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {report['current_memory']['usage_percent']:.1f}%")
            print(f"ğŸ¯ ìµœì í™” í†µê³„: {report['optimization_stats']}")
            sys.exit(0)
        else:
            print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            sys.exit(1)

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
