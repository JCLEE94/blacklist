"""
ì„±ëŠ¥ ìµœì í™”ë¥¼ ìœ„í•œ ê³ ê¸‰ ìºì‹± ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ Blacklist ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ
ë‹¤ì¸µ ìºì‹± ì „ëµê³¼ ìºì‹œ ìµœì í™” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import functools
import hashlib
import pickle
import threading
import time
import weakref
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Callable, Dict, Optional, Union

try:
    import redis

    HAS_REDIS = True
except ImportError:
    HAS_REDIS = False

try:
    import orjson as json

    HAS_ORJSON = True
except ImportError:
    import json

    HAS_ORJSON = False

from loguru import logger


@dataclass
class CacheConfig:
    """ìºì‹œ ì„¤ì •"""

    default_ttl: int = 300  # 5ë¶„
    max_memory_items: int = 1000
    compression_threshold: int = 1024  # 1KB ì´ìƒ ì••ì¶•
    enable_metrics: bool = True
    redis_prefix: str = "blacklist:"


class PerformanceCache:
    """ê³ ì„±ëŠ¥ ë‹¤ì¸µ ìºì‹œ ì‹œìŠ¤í…œ"""

    def __init__(self, config: CacheConfig = None, redis_client=None):
        self.config = config or CacheConfig()
        self.redis_client = redis_client

        # ë©”ëª¨ë¦¬ ìºì‹œ (L1)
        self.memory_cache: Dict[str, Dict[str, Any]] = {}
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "memory_hits": 0,
            "redis_hits": 0,
            "compression_saves": 0,
        }
        self._lock = threading.RLock()

        # ìºì‹œ í¬ê¸° ì œí•œì„ ìœ„í•œ LRU ì¶”ì 
        self.access_times = {}

        logger.info(
            f"PerformanceCache initialized with Redis: {self.redis_client is not None}"
        )

    def _generate_key(self, key: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„± ë° ì •ê·œí™”"""
        if isinstance(key, str):
            return f"{self.config.redis_prefix}{key}"
        return f"{self.config.redis_prefix}{hashlib.md5(str(key).encode()).hexdigest()}"

    def _serialize_value(self, value: Any) -> bytes:
        """ê°’ ì§ë ¬í™” ë° ì••ì¶•"""
        try:
            if HAS_ORJSON:
                serialized = json.dumps(value)
                if isinstance(serialized, str):
                    serialized = serialized.encode("utf-8")
            else:
                serialized = json.dumps(value).encode("utf-8")

            # ì••ì¶• ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì••ì¶•
            if len(serialized) > self.config.compression_threshold:
                import gzip

                compressed = gzip.compress(serialized)
                if len(compressed) < len(serialized):
                    self.cache_stats["compression_saves"] += 1
                    return b"GZIP:" + compressed

            return serialized
        except Exception as e:
            logger.warning(f"Serialization failed, using pickle: {e}")
            return b"PICKLE:" + pickle.dumps(value)

    def _deserialize_value(self, data: bytes) -> Any:
        """ê°’ ì—­ì§ë ¬í™” ë° ì••ì¶• í•´ì œ"""
        try:
            if data.startswith(b"GZIP:"):
                import gzip

                decompressed = gzip.decompress(data[5:])
                if HAS_ORJSON:
                    return json.loads(decompressed)
                else:
                    return json.loads(decompressed.decode("utf-8"))
            elif data.startswith(b"PICKLE:"):
                return pickle.loads(data[7:])
            else:
                if HAS_ORJSON:
                    return json.loads(data)
                else:
                    return json.loads(data.decode("utf-8"))
        except Exception as e:
            logger.error(f"Deserialization failed: {e}")
            return None

    def get(self, key: str) -> Optional[Any]:
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        cache_key = self._generate_key(key)

        with self._lock:
            # L1: ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            if cache_key in self.memory_cache:
                item = self.memory_cache[cache_key]
                if item["expires_at"] > time.time():
                    self.cache_stats["hits"] += 1
                    self.cache_stats["memory_hits"] += 1
                    self.access_times[cache_key] = time.time()
                    return item["value"]
                else:
                    # ë§Œë£Œëœ í•­ëª© ì œê±°
                    del self.memory_cache[cache_key]
                    del self.access_times[cache_key]

            # L2: Redis ìºì‹œ í™•ì¸
            if self.redis_client:
                try:
                    data = self.redis_client.get(cache_key)
                    if data:
                        value = self._deserialize_value(data)
                        if value is not None:
                            self.cache_stats["hits"] += 1
                            self.cache_stats["redis_hits"] += 1

                            # ë©”ëª¨ë¦¬ ìºì‹œì— ë°±ì—… ì €ì¥
                            self._set_memory_cache(
                                cache_key, value, self.config.default_ttl
                            )
                            return value
                except Exception as e:
                    logger.warning(f"Redis get failed: {e}")

            self.cache_stats["misses"] += 1
            return None

    def set(self, key: str, value: Any, ttl: int = None) -> bool:
        """ë‹¤ì¸µ ìºì‹œì— ê°’ ì €ì¥"""
        if ttl is None:
            ttl = self.config.default_ttl

        cache_key = self._generate_key(key)

        try:
            with self._lock:
                # L1: ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥
                self._set_memory_cache(cache_key, value, ttl)

                # L2: Redis ìºì‹œì— ì €ì¥
                if self.redis_client:
                    try:
                        serialized = self._serialize_value(value)
                        self.redis_client.setex(cache_key, ttl, serialized)
                    except Exception as e:
                        logger.warning(f"Redis set failed: {e}")

                self.cache_stats["sets"] += 1
                return True

        except Exception as e:
            logger.error(f"Cache set failed: {e}")
            return False

    def _set_memory_cache(self, cache_key: str, value: Any, ttl: int):
        """ë©”ëª¨ë¦¬ ìºì‹œì— ê°’ ì €ì¥ (LRU ì ìš©)"""
        # ìºì‹œ í¬ê¸° ì œí•œ
        while len(self.memory_cache) >= self.config.max_memory_items:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            oldest_key = min(
                self.access_times.keys(), key=lambda k: self.access_times[k]
            )
            del self.memory_cache[oldest_key]
            del self.access_times[oldest_key]

        expires_at = time.time() + ttl
        self.memory_cache[cache_key] = {
            "value": value,
            "expires_at": expires_at,
            "created_at": time.time(),
        }
        self.access_times[cache_key] = time.time()

    def delete(self, key: str) -> bool:
        """ìºì‹œì—ì„œ ê°’ ì‚­ì œ"""
        cache_key = self._generate_key(key)

        with self._lock:
            deleted = False

            # ë©”ëª¨ë¦¬ ìºì‹œì—ì„œ ì‚­ì œ
            if cache_key in self.memory_cache:
                del self.memory_cache[cache_key]
                del self.access_times[cache_key]
                deleted = True

            # Redis ìºì‹œì—ì„œ ì‚­ì œ
            if self.redis_client:
                try:
                    if self.redis_client.delete(cache_key):
                        deleted = True
                except Exception as e:
                    logger.warning(f"Redis delete failed: {e}")

            if deleted:
                self.cache_stats["deletes"] += 1

            return deleted

    def clear(self) -> bool:
        """ëª¨ë“  ìºì‹œ í•­ëª© ì‚­ì œ"""
        with self._lock:
            self.memory_cache.clear()
            self.access_times.clear()

            if self.redis_client:
                try:
                    # í”„ë¦¬í”½ìŠ¤ì— í•´ë‹¹í•˜ëŠ” í‚¤ë“¤ë§Œ ì‚­ì œ
                    pattern = f"{self.config.redis_prefix}*"
                    keys = self.redis_client.keys(pattern)
                    if keys:
                        self.redis_client.delete(*keys)
                except Exception as e:
                    logger.warning(f"Redis clear failed: {e}")

            return True

    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_stats["hits"] + self.cache_stats["misses"]
        hit_rate = (
            (self.cache_stats["hits"] / total_requests * 100)
            if total_requests > 0
            else 0
        )

        return {
            **self.cache_stats,
            "hit_rate_percent": round(hit_rate, 2),
            "memory_cache_size": len(self.memory_cache),
            "redis_available": self.redis_client is not None,
            "compression_enabled": True,
            "orjson_enabled": HAS_ORJSON,
        }

    def cleanup_expired(self):
        """ë§Œë£Œëœ ë©”ëª¨ë¦¬ ìºì‹œ í•­ëª© ì •ë¦¬"""
        current_time = time.time()
        expired_keys = []

        with self._lock:
            for key, item in self.memory_cache.items():
                if item["expires_at"] <= current_time:
                    expired_keys.append(key)

            for key in expired_keys:
                del self.memory_cache[key]
                del self.access_times[key]

        if expired_keys:
            logger.debug(f"Cleaned up {len(expired_keys)} expired cache items")


def performance_cached(ttl: int = 300, key_func: Callable = None):
    """ê³ ì„±ëŠ¥ ìºì‹± ë°ì½”ë ˆì´í„°"""

    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                cache_key = f"{func.__module__}.{func.__name__}:{hashlib.md5(str(args + tuple(kwargs.items())).encode()).hexdigest()}"

            # ê¸€ë¡œë²Œ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
            cache = get_global_performance_cache()

            # ìºì‹œì—ì„œ ì¡°íšŒ
            result = cache.get(cache_key)
            if result is not None:
                return result

            # ìºì‹œ ë¯¸ìŠ¤ ì‹œ í•¨ìˆ˜ ì‹¤í–‰
            result = func(*args, **kwargs)

            # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            cache.set(cache_key, result, ttl)

            return result

        return wrapper

    return decorator


# ê¸€ë¡œë²Œ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤
_global_performance_cache = None


def get_global_performance_cache() -> PerformanceCache:
    """ê¸€ë¡œë²Œ ì„±ëŠ¥ ìºì‹œ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_performance_cache

    if _global_performance_cache is None:
        # Redis í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹œë„
        redis_client = None
        if HAS_REDIS:
            try:
                import os

                redis_url = os.getenv("REDIS_URL", "redis://localhost:6379/0")
                redis_client = redis.from_url(redis_url)
                redis_client.ping()  # ì—°ê²° í…ŒìŠ¤íŠ¸
                logger.info("Redis client connected for performance cache")
            except Exception as e:
                logger.warning(f"Redis connection failed, using memory-only cache: {e}")
                redis_client = None

        _global_performance_cache = PerformanceCache(redis_client=redis_client)

    return _global_performance_cache


if __name__ == "__main__":
    """ì„±ëŠ¥ ìºì‹œ ì‹œìŠ¤í…œ ê²€ì¦"""
    import sys

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    test_data = {
        "small_data": "test_value",
        "large_data": {"data": "x" * 2000, "numbers": list(range(1000))},
        "complex_data": {
            "nested": {"deep": {"structure": {"with": "multiple_levels"}}},
            "array": [{"item": i, "value": f"data_{i}"} for i in range(100)],
        },
    }

    cache = get_global_performance_cache()
    all_tests_passed = True

    try:
        # í…ŒìŠ¤íŠ¸ 1: ê¸°ë³¸ ìºì‹œ ë™ì‘
        for key, value in test_data.items():
            cache.set(key, value, ttl=60)
            retrieved = cache.get(key)
            if retrieved != value:
                print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {key} - ì €ì¥ëœ ê°’ê³¼ ì¡°íšŒëœ ê°’ì´ ë‹¤ë¦„")
                all_tests_passed = False

        # í…ŒìŠ¤íŠ¸ 2: TTL í™•ì¸
        cache.set("ttl_test", "ttl_value", ttl=1)
        time.sleep(2)
        if cache.get("ttl_test") is not None:
            print("âŒ TTL í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ë§Œë£Œëœ í•­ëª©ì´ ì—¬ì „íˆ ì¡´ì¬")
            all_tests_passed = False

        # í…ŒìŠ¤íŠ¸ 3: í†µê³„ í™•ì¸
        stats = cache.get_stats()
        if stats["sets"] == 0:
            print("âŒ í†µê³„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ì„¤ì • íšŸìˆ˜ê°€ 0")
            all_tests_passed = False

        # í…ŒìŠ¤íŠ¸ 4: ìºì‹œ ì •ë¦¬
        cache.clear()
        if len(cache.memory_cache) > 0:
            print("âŒ ì •ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ìºì‹œê°€ ì™„ì „íˆ ì •ë¦¬ë˜ì§€ ì•ŠìŒ")
            all_tests_passed = False

        if all_tests_passed:
            print("âœ… ì„±ëŠ¥ ìºì‹œ ì‹œìŠ¤í…œ ê²€ì¦ ì™„ë£Œ - ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼")
            print(f"ğŸ“Š ìµœì¢… í†µê³„: {cache.get_stats()}")
            sys.exit(0)
        else:
            print("âŒ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            sys.exit(1)

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)
