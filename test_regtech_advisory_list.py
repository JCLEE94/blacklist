#!/usr/bin/env python3
"""
REGTECH advisoryList HTML í˜ì´ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ
"""

import os
import re
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from loguru import logger
from dotenv import load_dotenv

load_dotenv()


def collect_from_advisory_list():
    """advisoryList í˜ì´ì§€ì—ì„œ ë°ì´í„° ìˆ˜ì§‘"""
    
    username = os.getenv('REGTECH_USERNAME', '')
    password = os.getenv('REGTECH_PASSWORD', '')
    base_url = "https://regtech.fsec.or.kr"
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
    })
    
    # ë¡œê·¸ì¸
    logger.info(f"ğŸ” ë¡œê·¸ì¸: {username}")
    login_resp = session.post(
        f"{base_url}/login/loginProcess",
        data={'loginId': username, 'loginPw': password}
    )
    
    if 'regtech-front' not in session.cookies:
        logger.error("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨")
        return []
    
    logger.info("âœ… ë¡œê·¸ì¸ ì„±ê³µ")
    
    # advisoryList í˜ì´ì§€ ì ‘ê·¼
    collected_ips = []
    page = 1
    max_pages = 10
    
    # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30)
    
    while page <= max_pages:
        logger.info(f"\nğŸ“„ í˜ì´ì§€ {page} ìˆ˜ì§‘ ì¤‘...")
        
        # POST ìš”ì²­ìœ¼ë¡œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        url = f"{base_url}/fcti/securityAdvisory/advisoryList"
        
        # íŒŒë¼ë¯¸í„° ì„¤ì •
        data = {
            "page": str(page),
            "tabSort": "blacklist",  # ë¸”ë™ë¦¬ìŠ¤íŠ¸ íƒ­
            "startDate": start_date.strftime('%Y%m%d'),
            "endDate": end_date.strftime('%Y%m%d'),
            "findCondition": "all",
            "findKeyword": "",
            "size": "100",
            "rows": "100"
        }
        
        response = session.post(
            url,
            data=data,
            headers={"Content-Type": "application/x-www-form-urlencoded"}
        )
        
        if response.status_code != 200:
            logger.warning(f"í˜ì´ì§€ {page} ì‹¤íŒ¨: {response.status_code}")
            break
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # í…Œì´ë¸” ì°¾ê¸°
        tables = soup.find_all('table')
        logger.info(f"í…Œì´ë¸” ìˆ˜: {len(tables)}")
        
        # tbody ì°¾ê¸°
        tbody = soup.find('tbody')
        if tbody:
            rows = tbody.find_all('tr')
            logger.info(f"í–‰ ìˆ˜: {len(rows)}")
            
            for row in rows:
                cells = row.find_all('td')
                for cell in cells:
                    text = cell.get_text(strip=True)
                    
                    # IP íŒ¨í„´ ë§¤ì¹­
                    ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
                    ips = ip_pattern.findall(text)
                    
                    for ip in ips:
                        # IP ìœ íš¨ì„± ê²€ì‚¬
                        octets = ip.split('.')
                        try:
                            if all(0 <= int(o) <= 255 for o in octets):
                                ip_data = {
                                    "ip": ip,
                                    "source": "REGTECH",
                                    "description": "Malicious IP from advisoryList",
                                    "detection_date": datetime.now().strftime('%Y-%m-%d'),
                                    "confidence": "high"
                                }
                                if ip_data not in collected_ips:
                                    collected_ips.append(ip_data)
                                    logger.info(f"  âœ… IP ë°œê²¬: {ip}")
                        except:
                            pass
        
        # divë‚˜ ul/li êµ¬ì¡°ì—ì„œë„ ì°¾ê¸°
        advisory_items = soup.find_all(['div', 'ul', 'li'], class_=re.compile('advisory|blacklist|threat|ip', re.I))
        
        for item in advisory_items:
            text = item.get_text(strip=True)
            ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
            ips = ip_pattern.findall(text)
            
            for ip in ips:
                octets = ip.split('.')
                try:
                    if all(0 <= int(o) <= 255 for o in octets):
                        ip_data = {
                            "ip": ip,
                            "source": "REGTECH",
                            "description": "Malicious IP from advisoryList",
                            "detection_date": datetime.now().strftime('%Y-%m-%d'),
                            "confidence": "high"
                        }
                        if ip_data not in collected_ips:
                            collected_ips.append(ip_data)
                            logger.info(f"  âœ… IP ë°œê²¬: {ip}")
                except:
                    pass
        
        # JavaScript ë°ì´í„°ì—ì„œë„ ì°¾ê¸°
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                # JSON í˜•íƒœì˜ ë°ì´í„° ì°¾ê¸°
                if 'ipList' in script.string or 'blacklist' in script.string.lower():
                    ip_pattern = re.compile(r'"ip"\s*:\s*"([^"]+)"')
                    matches = ip_pattern.findall(script.string)
                    for ip in matches:
                        if '.' in ip:
                            ip_data = {
                                "ip": ip.split('/')[0] if '/' in ip else ip,
                                "source": "REGTECH",
                                "description": "Malicious IP from advisoryList JS",
                                "detection_date": datetime.now().strftime('%Y-%m-%d'),
                                "confidence": "high"
                            }
                            if ip_data not in collected_ips:
                                collected_ips.append(ip_data)
                                logger.info(f"  âœ… IP ë°œê²¬ (JS): {ip}")
        
        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
        if page == 1 and not collected_ips:
            # ì²« í˜ì´ì§€ HTML ì €ì¥ (ë””ë²„ê¹…ìš©)
            with open('regtech_advisory_list.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            logger.info("ì²« í˜ì´ì§€ HTML ì €ì¥: regtech_advisory_list.html")
            
            # HTMLì—ì„œ íŠ¹ì • í…ìŠ¤íŠ¸ íŒ¨í„´ ì°¾ê¸°
            if 'ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤' in response.text or 'No data' in response.text:
                logger.warning("í˜ì´ì§€ì— ë°ì´í„°ê°€ ì—†ë‹¤ê³  í‘œì‹œë¨")
            
            break
        
        if not collected_ips:
            logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ë°ì´í„° ì—†ìŒ")
            break
        
        page += 1
    
    session.close()
    
    # ê²°ê³¼
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“Š ì´ {len(collected_ips)}ê°œ IP ìˆ˜ì§‘")
    
    if collected_ips:
        logger.info("\nì²˜ìŒ 10ê°œ IP:")
        for i, ip_data in enumerate(collected_ips[:10], 1):
            logger.info(f"  {i}. {ip_data['ip']}")
    
    return collected_ips


if __name__ == "__main__":
    ips = collect_from_advisory_list()
    
    if ips:
        print(f"\nâœ… ì„±ê³µ! {len(ips)}ê°œ ì‹¤ì œ IP ìˆ˜ì§‘")
        
        # PostgreSQLì— ì €ì¥
        from src.core.data_storage_fixed import FixedDataStorage
        storage = FixedDataStorage()
        result = storage.store_ips(ips, "REGTECH")
        
        if result.get("success"):
            print(f"âœ… PostgreSQL ì €ì¥ ì™„ë£Œ: {result.get('imported_count')}ê°œ")
        else:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {result.get('error')}")
    else:
        print("\nâŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. HTML íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")