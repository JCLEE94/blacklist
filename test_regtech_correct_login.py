#!/usr/bin/env python3
"""
REGTECH ì˜¬ë°”ë¥¸ ë¡œê·¸ì¸ í”„ë¡œì„¸ìŠ¤ë¡œ ë°ì´í„° ìˆ˜ì§‘
1. /member/findOneMemberë¡œ ì‚¬ìš©ì í™•ì¸
2. /login/addLoginìœ¼ë¡œ ì‹¤ì œ ë¡œê·¸ì¸
"""

import os
import re
import json
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from dotenv import load_dotenv

load_dotenv()

def collect_with_correct_login():
    """ì˜¬ë°”ë¥¸ ë¡œê·¸ì¸ í”„ë¡œì„¸ìŠ¤ë¡œ ë°ì´í„° ìˆ˜ì§‘"""
    
    username = os.getenv('REGTECH_USERNAME', '')
    password = os.getenv('REGTECH_PASSWORD', '')
    base_url = "https://regtech.fsec.or.kr"
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'X-Requested-With': 'XMLHttpRequest',
        'Origin': base_url,
        'Referer': f'{base_url}/login/loginForm'
    })
    
    collected_ips = []
    
    # 1. ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì† (ì„¸ì…˜ ì¿ í‚¤ íšë“)
    print(f"ğŸ” Step 1: Getting session cookie...")
    login_page = session.get(f'{base_url}/login/loginForm')
    print(f"  Initial cookies: {list(session.cookies.keys())}")
    
    # 2. ì‚¬ìš©ì í™•ì¸ API í˜¸ì¶œ
    print(f"\nğŸ” Step 2: Verifying user ({username})...")
    verify_data = {
        'memberId': username,
        'memberPw': password
    }
    
    verify_resp = session.post(
        f'{base_url}/member/findOneMember',
        data=verify_data
    )
    
    print(f"  Verify status: {verify_resp.status_code}")
    
    if verify_resp.status_code == 200:
        print("  âœ… User verified successfully")
        
        # 3. ì‹¤ì œ ë¡œê·¸ì¸
        print(f"\nğŸ” Step 3: Performing actual login...")
        
        # Form ë°ì´í„° ì¤€ë¹„
        login_form_data = {
            'username': username,
            'password': password,
            'login_error': '',
            'txId': '',
            'token': '',
            'memberId': '',
            'smsTimeExcess': 'N'
        }
        
        # addLogin ì—”ë“œí¬ì¸íŠ¸ë¡œ POST
        session.headers.update({
            'Content-Type': 'application/x-www-form-urlencoded'
        })
        
        login_resp = session.post(
            f'{base_url}/login/addLogin',
            data=login_form_data,
            allow_redirects=True
        )
        
        print(f"  Login status: {login_resp.status_code}")
        print(f"  Final URL: {login_resp.url}")
        print(f"  Cookies: {list(session.cookies.keys())}")
        
        # ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸
        if 'logout' in login_resp.text.lower() or 'ë¡œê·¸ì•„ì›ƒ' in login_resp.text:
            print("  âœ… Login successful!")
        else:
            print("  âš ï¸ Login status uncertain")
            
            # ë””ë²„ê¹…ì„ ìœ„í•´ ì‘ë‹µ ì¼ë¶€ ì €ì¥
            with open('login_response.html', 'w', encoding='utf-8') as f:
                f.write(login_resp.text[:5000])
            print("  Response saved to login_response.html for debugging")
        
        # 4. ë°ì´í„° ìˆ˜ì§‘
        print(f"\nğŸ” Step 4: Collecting blacklist data...")
        
        # 4-1. Advisory List ì‹œë„
        advisory_url = f"{base_url}/fcti/securityAdvisory/advisoryList"
        print(f"  Accessing: {advisory_url}")
        
        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)
        
        # POST íŒŒë¼ë¯¸í„°
        list_params = {
            'page': '1',
            'size': '100',
            'rows': '100',
            'startDate': start_date.strftime('%Y%m%d'),
            'endDate': end_date.strftime('%Y%m%d'),
            'tabSort': 'blacklist',
            'findCondition': 'all',
            'findKeyword': ''
        }
        
        # í—¤ë” ì—…ë°ì´íŠ¸
        session.headers.update({
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Content-Type': 'application/x-www-form-urlencoded'
        })
        
        list_resp = session.post(advisory_url, data=list_params)
        print(f"    Status: {list_resp.status_code}")
        
        if list_resp.status_code == 200:
            soup = BeautifulSoup(list_resp.text, 'html.parser')
            
            # í…Œì´ë¸” ì°¾ê¸°
            tables = soup.find_all('table')
            print(f"    Tables: {len(tables)}")
            
            # tbody í–‰ ì°¾ê¸°
            rows = soup.select('tbody tr')
            print(f"    Rows: {len(rows)}")
            
            # IP íŒ¨í„´ ì°¾ê¸°
            ip_pattern = re.compile(r'\b(?:\d{1,3}\.){3}\d{1,3}\b')
            
            for row in rows:
                cells = row.find_all('td')
                for cell in cells:
                    text = cell.get_text(strip=True)
                    ips = ip_pattern.findall(text)
                    
                    for ip in ips:
                        octets = ip.split('.')
                        try:
                            if all(0 <= int(o) <= 255 for o in octets):
                                if not ip.startswith(('127.', '192.168.', '10.', '172.', '0.')):
                                    if ip not in collected_ips:
                                        collected_ips.append(ip)
                                        print(f"      âœ… Found: {ip}")
                        except:
                            pass
            
            # í˜ì´ì§€ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œë„ IP ì°¾ê¸°
            page_text = soup.get_text()
            all_ips = ip_pattern.findall(page_text)
            
            for ip in all_ips:
                octets = ip.split('.')
                try:
                    if all(0 <= int(o) <= 255 for o in octets):
                        if not ip.startswith(('127.', '192.168.', '10.', '172.', '0.')):
                            if ip not in collected_ips:
                                collected_ips.append(ip)
                                print(f"      âœ… Found in page: {ip}")
                except:
                    pass
        
        # 4-2. Board List ì‹œë„
        if len(collected_ips) == 0:
            board_url = f"{base_url}/board/boardList"
            print(f"\n  Trying board: {board_url}")
            
            board_params = {
                'menuCode': 'HPHB0620101',
                'pageIndex': '1',
                'pageSize': '100'
            }
            
            board_resp = session.get(board_url, params=board_params)
            print(f"    Status: {board_resp.status_code}")
            
            if board_resp.status_code == 200:
                soup = BeautifulSoup(board_resp.text, 'html.parser')
                
                # ê²Œì‹œíŒ ë‚´ìš©ì—ì„œ IP ì°¾ê¸°
                all_text = soup.get_text()
                ips = ip_pattern.findall(all_text)
                
                for ip in ips:
                    octets = ip.split('.')
                    try:
                        if all(0 <= int(o) <= 255 for o in octets):
                            if not ip.startswith(('127.', '192.168.', '10.', '172.', '0.')):
                                if ip not in collected_ips:
                                    collected_ips.append(ip)
                                    print(f"      âœ… Found in board: {ip}")
                    except:
                        pass
        
        # 4-3. Excel ë‹¤ìš´ë¡œë“œ ì‹œë„
        if len(collected_ips) == 0:
            excel_url = f"{base_url}/board/excelDownload"
            print(f"\n  Trying Excel download: {excel_url}")
            
            excel_params = {
                'menuCode': 'HPHB0620101',
                'startDate': start_date.strftime('%Y%m%d'),
                'endDate': end_date.strftime('%Y%m%d')
            }
            
            excel_resp = session.post(excel_url, data=excel_params, stream=True)
            print(f"    Status: {excel_resp.status_code}")
            print(f"    Content-Type: {excel_resp.headers.get('Content-Type', '')}")
            
            # Excel íŒŒì¼ì¸ì§€ í™•ì¸
            content_type = excel_resp.headers.get('Content-Type', '').lower()
            if 'excel' in content_type or 'spreadsheet' in content_type:
                print("    âœ… Excel file received")
                
                # íŒŒì¼ ì €ì¥
                excel_path = f"/tmp/regtech_excel_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
                with open(excel_path, 'wb') as f:
                    for chunk in excel_resp.iter_content(chunk_size=8192):
                        f.write(chunk)
                print(f"    Saved to: {excel_path}")
                
                # Excel íŒŒì‹±
                try:
                    import pandas as pd
                    df = pd.read_excel(excel_path)
                    print(f"    Excel shape: {df.shape}")
                    print(f"    Columns: {df.columns.tolist()}")
                    
                    # IP ì»¬ëŸ¼ ì°¾ê¸°
                    for col in df.columns:
                        for idx, val in df[col].dropna().items():
                            val_str = str(val)
                            ips = ip_pattern.findall(val_str)
                            for ip in ips:
                                if ip not in collected_ips:
                                    collected_ips.append(ip)
                                    print(f"      âœ… Found in Excel: {ip}")
                except Exception as e:
                    print(f"    Excel parsing error: {e}")
    
    else:
        print(f"  âŒ User verification failed: {verify_resp.status_code}")
        if verify_resp.text:
            print(f"  Response: {verify_resp.text[:200]}")
    
    # 5. ê²°ê³¼
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì´ {len(collected_ips)}ê°œ IP ìˆ˜ì§‘")
    
    if collected_ips:
        # ì¤‘ë³µ ì œê±°
        unique_ips = list(set(collected_ips))
        print(f"   ì¤‘ë³µ ì œê±° í›„: {len(unique_ips)}ê°œ")
        
        print("\nì²˜ìŒ 10ê°œ IP:")
        for i, ip in enumerate(unique_ips[:10], 1):
            print(f"  {i}. {ip}")
        
        # IP ë°ì´í„° í˜•ì‹ ë³€í™˜
        ip_data_list = []
        for ip in unique_ips:
            ip_data_list.append({
                "ip": ip,
                "source": "REGTECH",
                "description": "Malicious IP from REGTECH",
                "detection_date": datetime.now().strftime('%Y-%m-%d'),
                "confidence": "high"
            })
        
        return ip_data_list
    
    return []


if __name__ == "__main__":
    print("ğŸš€ REGTECH Correct Login Process Collection")
    print("="*60)
    
    ips = collect_with_correct_login()
    
    if ips:
        print(f"\nâœ… ì„±ê³µ! {len(ips)}ê°œ ì‹¤ì œ IP ìˆ˜ì§‘")
        
        # PostgreSQLì— ì €ì¥
        try:
            from src.core.data_storage_fixed import FixedDataStorage
            storage = FixedDataStorage()
            result = storage.store_ips(ips, "REGTECH")
            
            if result.get("success"):
                print(f"âœ… PostgreSQL ì €ì¥ ì™„ë£Œ: {result.get('imported_count')}ê°œ")
            else:
                print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {result.get('error')}")
        except Exception as e:
            print(f"âŒ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
    else:
        print("\nâŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ë¡œê·¸ì¸í•˜ì—¬ ë°ì´í„° í™•ì¸")
        print("2. HAR íŒŒì¼ë¡œ ì •í™•í•œ ìš”ì²­ ì‹œí€€ìŠ¤ ìº¡ì²˜")
        print("3. ì¿ í‚¤ ë¬¸ìì—´ë¡œ ì§ì ‘ ì¸ì¦")